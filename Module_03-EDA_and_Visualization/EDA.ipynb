{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC5502/blob/main/Module_03-EDA_and_Visualization/EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "268ad749",
      "metadata": {
        "id": "268ad749"
      },
      "source": [
        "# Exploratory Data Analysis (EDA)\n",
        "\n",
        "Your Name"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "* Colab - get notebook from gitmystuff\n",
        "* Save a Copy in Drive\n",
        "* Remove Copy of\n",
        "* Edit name\n",
        "* Take attendance\n",
        "* Clean up Colab Notebooks folder\n",
        "* Submit shared link"
      ],
      "metadata": {
        "id": "ojyIDptwB7Iy"
      },
      "id": "ojyIDptwB7Iy"
    },
    {
      "cell_type": "markdown",
      "id": "c41cc0a6",
      "metadata": {
        "id": "c41cc0a6"
      },
      "source": [
        "## Functions, Methods, and Attributes\n",
        "\n",
        "* df.shape: attribute; values that are precomputed\n",
        "* df.head(): method; values are computed when called; belongs to a class, package, module, an object\n",
        "* my_func(): function; usually created by programmer; set of instructions that perform a task"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replit Fun"
      ],
      "metadata": {
        "id": "O6imfx-TBKt1"
      },
      "id": "O6imfx-TBKt1"
    },
    {
      "cell_type": "markdown",
      "id": "ec7572c3",
      "metadata": {
        "id": "ec7572c3"
      },
      "source": [
        "## Data Resources\n",
        "\n",
        "* https://www.statsmodels.org/devel/datasets/index.html\n",
        "* https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research\n",
        "* http://lib.stat.cmu.edu/datasets/\n",
        "* https://www.kaggle.com/datasets\n",
        "* https://registry.opendata.aws/\n",
        "* https://dataportals.org/\n",
        "* https://opendatamonitor.eu/frontend/web/index.php?r=dashboard%2Findex\n",
        "* https://data.nasdaq.com/\n",
        "* http://radar.oreilly.com/2011/01/journalist-data-tools.html\n",
        "* https://www.reddit.com/r/datasets/\n",
        "* https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html\n",
        "* https://github.com/vincentarelbundock/Rdatasets\n",
        "* https://data.world/\n",
        "* https://data.gov/\n",
        "* https://registry.opendata.aws/\n",
        "* https://cloud.google.com/datasets\n",
        "* https://cloud.google.com/bigquery/public-data\n",
        "* https://services.google.com/fh/files/misc/public_datasets_one_pager.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our Brain and Uncertainty\n",
        "\n",
        "**The Illusion of Certainty**\n",
        "\n",
        "Our brains are hardwired to seek certainty and order. We prefer clear-cut answers and logical conclusions, which is a powerful mechanism for survival and efficiency. However, this preference for certainty can lead to irrationality in decision-making. We often operate on mental shortcuts, or heuristics, that can sometimes lead to systematic errors in judgment. For example, the **Müller-Lyer illusion** demonstrates how our visual system, a key part of our brain's processing, can be fooled. Even when we know the lines are the same length, our brain continues to perceive them as different. This shows that our brain's default processing can be flawed, even in the face of contradictory facts.\n",
        "\n",
        "https://how-emotions-are-made.com/notes/M%C3%BCller-Lyer_illusion\n",
        "\n",
        "---\n",
        "\n",
        "**Decisions as Probabilistic Outcomes**\n",
        "\n",
        "The idea of a \"mistake\" is often tied to a binary view of right and wrong. The provided notes from the book *Thinking in Bets* suggest a different perspective: a decision's outcome is not a simple result of being \"right\" or \"wrong,\" but rather a reflection of a **probabilistic result**. A good decision can lead to a bad outcome due to a roll of the dice, and a bad decision can lead to a good outcome purely by luck. The key is to separate the quality of the decision-making process from the quality of the outcome. A truly good decision is one that maximizes the probability of a favorable outcome, regardless of the final result.\n",
        "\n",
        "This is famously illustrated in the \"Battle of Wits\" from *The Princess Bride*. Westley's decision to drink from a poisoned cup was based on his assessment of Vizzini's character and his own knowledge of psychology, not a guarantee of a favorable result. While the outcome was a win, the decision itself was a high-stakes bet, not an inevitable victory. Even a fan theory that suggests the entire ordeal was a sham further highlights the idea that the apparent certainty of the outcome was an illusion, created by Westley to gain a probabilistic edge.\n",
        "\n",
        "---\n",
        "\n",
        "**The Cost of Fast Decisions**\n",
        "\n",
        "In a world full of rapid-fire choices, our brains often default to quick, intuitive thinking (System 1 thinking, as described by Daniel Kahneman). This allows us to make decisions efficiently in the \"now,\" but it comes at a cost. When we rely on these fast decisions, we are more susceptible to cognitive biases and irrational thinking. We may overlook crucial information or fail to consider long-term consequences in favor of immediate gratification or perceived safety. This can result in decisions that do not serve our best interests in the long run. By acknowledging that many of our decisions are driven by this fast, intuitive process, we can learn to slow down and apply more deliberate, rational thinking (System 2 thinking) when the situation requires it, thus improving our chances of making a more favorable probabilistic bet.\n"
      ],
      "metadata": {
        "id": "l9c8XQVtqMEO"
      },
      "id": "l9c8XQVtqMEO"
    },
    {
      "cell_type": "markdown",
      "id": "0b50af6f",
      "metadata": {
        "id": "0b50af6f"
      },
      "source": [
        "## Errors and Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Errors"
      ],
      "metadata": {
        "id": "MstDd7QdsA4Q"
      },
      "id": "MstDd7QdsA4Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2078a7e2",
      "metadata": {
        "id": "2078a7e2"
      },
      "outputs": [],
      "source": [
        "# # practice getting data (from github) and dealing with errors\n",
        "# import pandas as pd\n",
        "\n",
        "# grades = pd.read_csv('class-grades2.csv')\n",
        "# # grades = pd.read_csv('class-grades2.csv', on_bad_lines='warn')\n",
        "# print(grades.shape)\n",
        "# print(grades.info())\n",
        "# grades.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "424776a2",
      "metadata": {
        "id": "424776a2"
      },
      "source": [
        "### Missing Values\n",
        "\n",
        "Missing data cause problems because most statistical procedures require a value for each variable. When a data set is incomplete, the data analyst has to decide how to deal with it.\n",
        "\n",
        "https://www.theanalysisfactor.com/causes-of-missing-data/<br />\n",
        "https://www.theanalysisfactor.com/when-listwise-deletion-works/\n",
        "\n",
        "Missing data is a common issue in data analysis, and the way it's handled depends on the underlying mechanism of missingness. There are three main types: Missing Completely at Random (MCAR), Missing at Random (MAR), and Missing Not at Random (MNAR).\n",
        "\n",
        "***\n",
        "\n",
        "**Missing Completely at Random (MCAR)**\n",
        "\n",
        "This occurs when the missingness of a data point is entirely **unrelated to any other variable in the dataset, both observed and unobserved**. The missing values are essentially a random subset of the data. This is the simplest and most ideal type of missingness to deal with, as it does not introduce any systematic bias.\n",
        "\n",
        "* **Example:** A hard drive crash causes a random loss of survey responses. The missing values are due to a technical failure, not because of anything the respondents answered. Another example is if some lab samples were randomly damaged during transit, leading to missing test results. The damage is not related to the actual values that would have been measured.\n",
        "\n",
        "***\n",
        "\n",
        "**Missing at Random (MAR)**\n",
        "\n",
        "This is when the missingness of a data point is **related to other observed variables** in the dataset, but not to the value of the missing data itself. This means you can predict whether a value is missing based on other information you have. The name is a bit misleading, as the data is not missing \"randomly\" in the intuitive sense.\n",
        "\n",
        "* **Example:** A study on depression may find that **male participants are less likely to complete a survey** about their depression severity than female participants. The missingness of the depression score is related to the participant's gender (an observed variable), but within the group of males, the missingness is not related to their actual depression score (the unobserved value). Another example could be a survey about income where younger people are less likely to report their income.\n",
        "\n",
        "***\n",
        "\n",
        "**Missing Not at Random (MNAR)**\n",
        "\n",
        "This is the most complex and problematic type of missing data. MNAR occurs when the missingness is **directly related to the value of the missing data itself**, even after accounting for other variables. The reason a value is missing is because of what that value would have been.\n",
        "\n",
        "* **Example:** A person with a very **high or very low income** is less likely to report their income on a survey. The missingness is directly caused by the value of the income itself. Another classic example is a drug trial where participants with severe side effects drop out of the study. The missing data (final health status) is directly related to the poor outcome they experienced, which is the very thing the study is trying to measure. This can severely bias the results.\n",
        "\n",
        "Here is an example of an explanation of these types of missing data.\n",
        "\n",
        "https://stefvanbuuren.name/fimd/sec-MCAR.html<br />\n",
        "https://www.iriseekhout.com/post/2022-06-28-missingdatamechanisms/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80e59473",
      "metadata": {
        "id": "80e59473"
      },
      "source": [
        "### Complete-Case Analysis (CCA)\n",
        "\n",
        "* Aka Listwise deletion\n",
        "* Reduces sample size\n",
        "* Can reduce the statistical efficiency of estimates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69119af1",
      "metadata": {
        "id": "69119af1"
      },
      "outputs": [],
      "source": [
        "# code along - delete rows with missing values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89aaa3dd",
      "metadata": {
        "id": "89aaa3dd"
      },
      "source": [
        "### Any vs All"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08a61d39",
      "metadata": {
        "id": "08a61d39"
      },
      "outputs": [],
      "source": [
        "# count nulls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d153c4e1",
      "metadata": {
        "id": "d153c4e1"
      },
      "outputs": [],
      "source": [
        "# drop columns with null values https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffce2927",
      "metadata": {
        "id": "ffce2927"
      },
      "outputs": [],
      "source": [
        "# drop rows with null values, information is lost when we drop, the default of dropna\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f8961e4",
      "metadata": {
        "id": "7f8961e4"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deceptive Statitics\n",
        "\n",
        "* Descriptive statistics: the numbers and calculations we use to summarize raw data\n",
        "* The use of statistics to describe complex phenomena is not exact\n",
        "* The mean is prone to distortion by outliers so we have the median\n",
        "* Elon Musk and Tiny Homes, 35000 till Elon joins and then average income jumps to $91 million\n",
        "* The median isn't the message either (mortality distribution - Jay Gould, evolutionary biologist, had cancer and was told by doctors he had 8 months to live, which was the median, what wasn't explained is that 50% lived longer and some much longer)\n",
        "* Absolute statistic vs relative statistic\n",
        "* Standard deviation: how dispersed the data from the mean, how spread out\n",
        "* Descriptive statistics are often used to compare two quantities\n",
        "* Descriptive vs Story Telling\n",
        "* Again precision vs accuracy\n",
        "* Precision can mask inaccuracy\n",
        "* In 1950, Joseph McCarthy waved a piece of paper in a speech and declared he had a list of 205 names known to the Secretary of State that were working in the State Department. The paper was blank and this was an outright lie, but the specificity gave the lie credibility\n",
        "* Measurements, or calculations, no matter how precise, need to be checked with common sense\n",
        "* Descriptive statistics may suffer from clarity over exactly what is being described\n",
        "* Be sure to present a range of statistics with a range of perspectives\n",
        "* Our schools are getting worse! 60% of our schools had lower test scores this year from last\n",
        "* Our schools are getting better! 80% of our students had higher test scores from last year\n",
        "* Not all schools/students are equal and it depends on the unit of analysis\n",
        "* One measured schools and the other measured students"
      ],
      "metadata": {
        "id": "jkkNs4VWXV90"
      },
      "id": "jkkNs4VWXV90"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Anscombes Quartet\n",
        "Anscombe's quartet comprises four data sets that have nearly identical simple descriptive statistics, yet have very different distributions and appear very different when graphed.\n",
        "https://en.wikipedia.org/wiki/Anscombe%27s_quartet"
      ],
      "metadata": {
        "id": "Hgrw2SdBqHwa"
      },
      "id": "Hgrw2SdBqHwa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Analysis vs Explanatory Analysis\n",
        "\n",
        "Exploratory analysis is the process of turning over 100 rocks to find perhaps 1 or 2 precious gemstones. Explanatory analysis is what happens when you have something specific you want to show an audience - like those 1 or 2 precious gemstones.\n",
        "\n",
        "https://www.storytellingwithdata.com/blog/2014/04/exploratory-vs-explanatory-analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "7HzADrxYYmF-"
      },
      "id": "7HzADrxYYmF-"
    },
    {
      "cell_type": "markdown",
      "id": "11d57cbb",
      "metadata": {
        "id": "11d57cbb"
      },
      "source": [
        "### Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5abb17ad",
      "metadata": {
        "id": "5abb17ad"
      },
      "outputs": [],
      "source": [
        "# # train test split\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(grades.drop('FinalGrade', axis=1), grades['FinalGrade'], test_size=.2, random_state=42)\n",
        "# print(X_train.shape)\n",
        "# print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1f3e831",
      "metadata": {
        "id": "c1f3e831"
      },
      "outputs": [],
      "source": [
        "# # info\n",
        "# X_train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "709bae64",
      "metadata": {
        "id": "709bae64"
      },
      "outputs": [],
      "source": [
        "# # numerical statistics\n",
        "# X_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train['Prefix'] = X_train['Prefix'].astype('object')\n",
        "# X_train.info()"
      ],
      "metadata": {
        "id": "QxXZ2eEyyfHb"
      },
      "id": "QxXZ2eEyyfHb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # all statistics\n",
        "# X_train.describe(include='all') # top is object mode and freq is how often it shows up"
      ],
      "metadata": {
        "id": "uRbxVtrfxUP5"
      },
      "id": "uRbxVtrfxUP5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # describe objects\n",
        "# X_train.describe(include=['object'])"
      ],
      "metadata": {
        "id": "ABi2JiB1xuSP"
      },
      "id": "ABi2JiB1xuSP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "126d9d4d",
      "metadata": {
        "id": "126d9d4d"
      },
      "outputs": [],
      "source": [
        "# # value counts\n",
        "# X_train['TakeHome'].str.lower().value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97707d53",
      "metadata": {
        "id": "97707d53"
      },
      "outputs": [],
      "source": [
        "# # pie chart\n",
        "# X_train['TakeHome'].str.lower().value_counts(dropna=False).plot.pie()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d5eb588",
      "metadata": {
        "id": "6d5eb588"
      },
      "source": [
        "## Measures of Center\n",
        "* Mean\n",
        "* Median\n",
        "* Mode\n",
        "\n",
        "And the Normal Distribution: https://www.mathsisfun.com/data/standard-normal-distribution.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f55c153",
      "metadata": {
        "id": "5f55c153"
      },
      "outputs": [],
      "source": [
        "# # mean, median, mode\n",
        "# import numpy as np\n",
        "\n",
        "# print('mean:', int(np.mean(X_train['Assignment1'])))\n",
        "# print('mean:', int(X_train['Assignment1'].mean()))\n",
        "# print('median:', int(X_train['Assignment1'].median()))\n",
        "# print('mode:', X_train['TakeHome'].mode())\n",
        "# print('mode:', X_train['TakeHome'].mode()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58dc35ef",
      "metadata": {
        "id": "58dc35ef"
      },
      "outputs": [],
      "source": [
        "# X_train['TakeHome'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # what happens to the mean and median with skewed data\n",
        "# import numpy as np\n",
        "# from scipy.stats import skewnorm, norm\n",
        "# from scipy import stats\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# fig, ax = plt.subplots(1, 3, figsize=(14, 3))\n",
        "# skew = 20\n",
        "# n = 100000\n",
        "\n",
        "# r = skewnorm.rvs(skew, loc=0, scale=10, size=n)\n",
        "# ax[0].hist(r, bins=50, density=True, alpha=0.4)\n",
        "# ax[0].axvline(x=np.median(r), color='green', label='Median')\n",
        "# ax[0].axvline(x=np.mean(r).round(2), color='red', label='Mean')\n",
        "# ax[0].legend()\n",
        "# print(f'right skew data mean: {np.mean(r).round(2)}, median: {np.median(r).round(2)}')\n",
        "\n",
        "# l = skewnorm.rvs(-skew, loc=0, scale=10, size=n)\n",
        "# ax[2].hist(l, bins=50, density=True, alpha=0.4);\n",
        "# ax[2].axvline(x=np.mean(l).round(2), color='red', label='Mean')\n",
        "# ax[2].axvline(x=np.median(l), color='green', label='Median')\n",
        "# ax[2].legend()\n",
        "# print(f'left skew data mean: {np.mean(l).round(2)}, median: {np.median(l).round(2)}')\n",
        "\n",
        "# n = norm.rvs(loc=0, scale=1, size=n)\n",
        "# ax[1].hist(n, bins=50, density=True, alpha=0.4);\n",
        "# ax[1].axvline(x=np.mean(n).round(2), color='red', linewidth=3, label='Mean')\n",
        "# ax[1].axvline(x=np.median(n), color='green', label='Median')\n",
        "# ax[1].legend()\n",
        "# print(f'normal data mean: {np.mean(n).round(2)}, median: {np.median(n).round(2)}')"
      ],
      "metadata": {
        "id": "SXPp9Oqbqg9-"
      },
      "id": "SXPp9Oqbqg9-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d6ce6a16",
      "metadata": {
        "id": "d6ce6a16"
      },
      "source": [
        "### The Wisdom of the Crowd, The Median"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ab67540",
      "metadata": {
        "id": "3ab67540"
      },
      "source": [
        "### Imputing Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03b7e654",
      "metadata": {
        "id": "03b7e654"
      },
      "outputs": [],
      "source": [
        "# X_train.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "541c8cbb",
      "metadata": {
        "id": "541c8cbb"
      },
      "outputs": [],
      "source": [
        "# # show histograms\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# X_train.hist()\n",
        "# plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab0a59a6",
      "metadata": {
        "id": "ab0a59a6"
      },
      "outputs": [],
      "source": [
        "# # replace missing values with mean\n",
        "# X_train['Assignment1'].fillna(X_train['Assignment1'].round(decimals=2).mean(), inplace=True)\n",
        "# X_train.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "837cf897",
      "metadata": {
        "id": "837cf897"
      },
      "outputs": [],
      "source": [
        "# replace missing values for Tutorial, Midterm, Quiz, and Final using the example above"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b0572cc",
      "metadata": {
        "id": "2b0572cc"
      },
      "source": [
        "## Measures of Spread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aa5b5b1",
      "metadata": {
        "id": "2aa5b5b1"
      },
      "outputs": [],
      "source": [
        "# # Assignment histogram\n",
        "# X_train['Final'].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2163a715",
      "metadata": {
        "id": "2163a715"
      },
      "outputs": [],
      "source": [
        "# # Assignment boxplot\n",
        "# X_train.boxplot(column=['Final']);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "304660b6",
      "metadata": {
        "id": "304660b6"
      },
      "outputs": [],
      "source": [
        "# # Assignment violinplot\n",
        "# import seaborn as sns\n",
        "\n",
        "# sns.violinplot(x=X_train['Final']);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6e05e23",
      "metadata": {
        "id": "b6e05e23"
      },
      "source": [
        "### Variance\n",
        "\n",
        "Variance is a measure of the spread or dispersion of a dataset. It tells you, on average, how far each data point is from the mean. The way it's calculated is slightly different depending on whether you're working with a whole population or just a sample.\n",
        "\n",
        "Equation for mean:<br />\n",
        "$\\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i$\n",
        "\n",
        "Equation for population variance:<br />\n",
        "$\\sigma^2 = \\frac{1}{N}\\sum({x}-\\mu)^2$\n",
        "\n",
        "Equation for sample variance:<br />\n",
        "$s^2 = \\frac{1}{n-1}\\sum({x}-\\bar{x})^2$\n",
        "\n",
        "Equation for population standard deviation:<br />\n",
        "$\\sigma = \\sqrt{\\frac{1}{N}\\sum(x-\\mu)^2}$\n",
        "\n",
        "Equation for sample standard deviation:<br />\n",
        "$s = \\sqrt{\\frac{1}{n-1}\\sum(x-\\bar{x})^2}$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Population Variance ($\\sigma^2$)\n",
        "\n",
        "This is the true, fixed variance of an entire population. It's a single, constant value that describes the spread of all the data points you care about. To calculate it, you find the average of the squared differences from the population mean.\n",
        "\n",
        "* **When to use:** When you have data for every single member of the group you're studying (e.g., the heights of all students in a specific classroom).\n",
        "* **Formula:**\n",
        "    $$\\sigma^2 = \\frac{\\sum (x_i - \\mu)^2}{N}$$\n",
        "    Where $\\mu$ is the population mean and $N$ is the total number of observations in the population.\n",
        "\n"
      ],
      "metadata": {
        "id": "qjcWZ2Jf12Jb"
      },
      "id": "qjcWZ2Jf12Jb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sample Variance ($s^2$)\n",
        "\n",
        "This is an **estimate** of the population variance, calculated from a subset of the population. It's used when it's impractical or impossible to collect data for the entire population. To make this estimate a more accurate representation of the true variance, a small correction is made to the formula.\n",
        "\n",
        "* **When to use:** When you have data from only a portion of the group you're studying (e.g., the heights of a random group of 30 students to estimate the variance of all students in a school).\n",
        "* **Formula:**\n",
        "    $$s^2 = \\frac{\\sum (x_i - \\bar{x})^2}{n-1}$$\n",
        "    Where $\\bar{x}$ is the sample mean and $n$ is the total number of observations in the sample.\n",
        "\n",
        "The use of **$n-1$** in the denominator is known as **Bessel's correction**. It makes the sample variance a more accurate, unbiased estimate of the population variance, as the sample mean tends to be closer to the sample data points than the true population mean would be."
      ],
      "metadata": {
        "id": "6roH96fXEyqq"
      },
      "id": "6roH96fXEyqq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bessel's Correction\n",
        "\n",
        "* See Notebook in DSChunks"
      ],
      "metadata": {
        "id": "pbuB3Sv4E3_V"
      },
      "id": "pbuB3Sv4E3_V"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "246a6e16",
      "metadata": {
        "id": "246a6e16"
      },
      "outputs": [],
      "source": [
        "# # Assignment variance and standard deviation\n",
        "# print('Population variance:', X_train['Final'].var(ddof=0))\n",
        "# print('Sample variance:', X_train['Final'].var(ddof=1))\n",
        "# print('Population std dev:', X_train['Final'].std(ddof=0))\n",
        "# print('Sample std dev:', X_train['Final'].std(ddof=1))\n",
        "# print('Square root of sample variance:', np.sqrt(X_train['Final'].var()))\n",
        "# print('Square root of sample variance:', X_train['Final'].var()**(1/2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "747d0858",
      "metadata": {
        "id": "747d0858"
      },
      "source": [
        "### Quartiles\n",
        "\n",
        "https://en.wikipedia.org/wiki/Interquartile_range\n",
        "\n",
        "Interquartile range<br />\n",
        "Whiskers<br />\n",
        "\n",
        "Outliers<br />\n",
        "Fence<br />\n",
        "https://www.statisticshowto.com/upper-and-lower-fences/\n",
        "\n",
        "Boxplots<br />\n",
        "Violin plots"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Percentages\n",
        "\n",
        "* Naked Statistics - the dress and 25% markdown\n",
        "* Sales tax is raised from 3% to 5%\n",
        "* One group says tax went up 2 points, 3 to 5, the absolute change\n",
        "* Another group says the tax went up 67% (5-3)/3 = 2/3 ~ 67, the relative change\n",
        "* Art of Statistics Chapter 2 - 5% mortality sounds much worse than 95% survival"
      ],
      "metadata": {
        "id": "cG_pLdrbQi7G"
      },
      "id": "cG_pLdrbQi7G"
    },
    {
      "cell_type": "markdown",
      "id": "094c027a",
      "metadata": {
        "id": "094c027a"
      },
      "source": [
        "## Measures of Shape\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2021/05/shape-of-data-skewness-and-kurtosis/\n",
        "\n",
        "**Skewness**\n",
        "* Skewed right\n",
        "* Skewed left\n",
        "\n",
        "**Kurtosis**\n",
        "* Mesokurtic\n",
        "* Leptokurtic\n",
        "* Platykurtic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59d3bd9c",
      "metadata": {
        "id": "59d3bd9c"
      },
      "outputs": [],
      "source": [
        "# # visualize outliers before and after\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from random import random\n",
        "# from random import randint\n",
        "# from sklearn.datasets import make_regression\n",
        "\n",
        "# X, y = make_regression(n_samples=100, n_features=1, noise=50, random_state=42)\n",
        "# print('mean before outliers:', np.mean(X))\n",
        "# print('var before outliers:', np.var(X))\n",
        "# fig, ([ax1, ax2], [ax3, ax4]) = plt.subplots(2, 2, figsize=(10, 10))\n",
        "# ax1.scatter(X, y)\n",
        "# ax2.hist(X)\n",
        "# ax3.boxplot(X)\n",
        "# sns.violinplot(ax=ax4, data=X);"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Introduce outliers\n",
        "# import random\n",
        "\n",
        "# outliers = np.array([[10], [12], [15]])\n",
        "# X_with_outliers = np.append(X, outliers)\n",
        "\n",
        "# # Generate 30 new random numbers between 4 and 10\n",
        "# new_numbers = np.random.randint(4, 11, 30) # High is exclusive, so use 11 to include 10\n",
        "# X_updated = np.append(X_with_outliers, new_numbers)\n",
        "\n",
        "# # Let's create some corresponding y-values for the scatter plot\n",
        "# y_updated = np.append(y, np.random.rand(len(outliers) + len(new_numbers)) * 100)\n",
        "\n",
        "# fig, ([ax1, ax2], [ax3, ax4]) = plt.subplots(2, 2, figsize=(10, 10))\n",
        "# ax1.scatter(X_updated, y_updated)\n",
        "# ax1.set_title('Updated Data with More Numbers')\n",
        "# ax2.hist(X_updated, bins=30)\n",
        "# ax2.set_title('Updated Histogram')\n",
        "# ax3.boxplot(X_updated)\n",
        "# ax3.set_title('Updated Boxplot')\n",
        "# sns.violinplot(ax=ax4, data=X_updated)\n",
        "# ax4.set_title('Updated Violin Plot')\n",
        "# plt.suptitle('Data Distribution After Adding More Numbers')"
      ],
      "metadata": {
        "id": "WRu0k0OFnesq"
      },
      "id": "WRu0k0OFnesq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f1cbf3d7",
      "metadata": {
        "id": "f1cbf3d7"
      },
      "source": [
        "### Skewed data\n",
        "\n",
        "https://www.itl.nist.gov/div898/handbook/eda/section3/eda33e6.htm\n",
        "\n",
        "Occur due to upper or lower bounds on the data<br />\n",
        "https://www.mathsisfun.com/definitions/upper-bound.html<br />\n",
        "Mean, Median, and Mode should be mentioned because there is no center in the usual sense<br />\n",
        "\n",
        "**Right Skewed**\n",
        "* Tail is on the right side\n",
        "* Mode Median Mean\n",
        "* Data have a lower bound\n",
        "\n",
        "**Left Skewed**\n",
        "* Tail is on the left\n",
        "* Mean Median Mode\n",
        "* Data have an upper bound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61543d9b",
      "metadata": {
        "id": "61543d9b"
      },
      "outputs": [],
      "source": [
        "# # creating skewed data\n",
        "# # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.skewnorm.html\n",
        "# from scipy.stats import skewnorm\n",
        "\n",
        "# a = 4 # skewness parameter: positive values are right skewed, negative values are left skewed\n",
        "# X = skewnorm.rvs(a, size=100)\n",
        "# fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 5))\n",
        "# ax1.hist(X)\n",
        "# ax2.boxplot(X)\n",
        "# sns.violinplot(ax=ax3, data=X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37d57f7f",
      "metadata": {
        "id": "37d57f7f"
      },
      "source": [
        "### Kurtosis\n",
        "\n",
        "https://www.investopedia.com/terms/p/platykurtic.asp\n",
        "\n",
        "* Mesokurtic: Extreme events are rare, resembles normal distribution\n",
        "* Platykurtic: Excess kurtosis is negative (< 3) and has thinner tails. Fewer extreme events. In finance, risk-averse investors might perfer platykurtic distributions\n",
        "* Leptokurtic: Excess kurtosis is greater than 3 and has fatter tails. Caused by extreme events or outliers.\n",
        "\n",
        "According to Investopedia (2022):\n",
        "\n",
        "> Risk-seeking investors can focus on investments whose returns follow a leptokurtic distribution, to maximize the chances of rare events—both positive and negative (para 3).\n",
        "\n",
        "Sources:\n",
        "* Leptokurtic Definition. (2022, February 1). In *Investopedia*. https://www.investopedia.com/terms/l/leptokurtic.asp.\n",
        "* https://medium.com/@filip.sekan/4-ways-how-to-shape-histograms-appearance-a87764df1417\n",
        "* https://miro.medium.com/v2/resize:fit:1100/format:webp/1*m2X-C-IMcYORq7G5mzvAzg.png"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# from scipy.stats import norm\n",
        "\n",
        "# # generate a normal distribution\n",
        "# normal_dist = norm(0, 1)\n",
        "# normal_samples = normal_dist.rvs(10000)\n",
        "\n",
        "# # generate a leptokurtic distribution\n",
        "# leptokurtic_dist = norm(loc=0, scale=0.5)\n",
        "# leptokurtic_samples = leptokurtic_dist.rvs(10000)\n",
        "\n",
        "# # generate a platykurtic distribution\n",
        "# platykurtic_samples = normal_samples + np.random.randn(10000)\n",
        "\n",
        "# # Plot the distributions\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.hist(normal_samples, bins=100, alpha=0.5, label='Mesokurtic distribution')\n",
        "# plt.hist(leptokurtic_samples, bins=100, alpha=0.5, label='Leptokurtic distribution')\n",
        "# plt.hist(platykurtic_samples, bins=100, alpha=0.5, label='Platykurtic distribution')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "UDj900fQxAj6"
      },
      "id": "UDj900fQxAj6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4c052ccd",
      "metadata": {
        "id": "4c052ccd"
      },
      "source": [
        "https://www.kaggle.com/getting-started/170781\n",
        "\n",
        "Skewness essentially measures the symmetry of the distribution, while kurtosis determines the heaviness of the distribution tails.\n",
        "\n",
        "The topic of Kurtosis has been controversial for decades now, the basis of kurtosis all these years has been linked with the peakedness but the ultimate verdict is that outliers (fatter tails) govern the kurtosis effect far more than the values near the mean (peak).\n",
        "\n",
        "https://towardsdatascience.com/skewness-kurtosis-simplified-1338e094fc85"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0327526",
      "metadata": {
        "id": "e0327526"
      },
      "source": [
        "## Moments\n",
        "\n",
        "Sometimes you'll see *mvsk*\n",
        "* Mean\n",
        "* Variance\n",
        "* Skewness\n",
        "* Kurtosis\n",
        "\n",
        "According to Wikipedia (2022):\n",
        "\n",
        ">  If the function is a probability distribution, then the first moment is the expected value, the second central moment is the variance, the third standardized moment is the skewness, and the fourth standardized moment is the kurtosis (para 1).\n",
        "\n",
        "Moment (mathematics). (2022, January 31). In *Wikipedia*. https://en.wikipedia.org/wiki/Moment_(mathematics).\n",
        "\n",
        "Mean:<br />\n",
        "$\\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i$v\n",
        "\n",
        "Variance:<br />\n",
        "$s^2 = \\frac{\\sum(x-\\bar{x})^2}{n-1}$\n",
        "\n",
        "Skewness:<br />\n",
        "$\\frac{\\frac{1}{n}\\sum(x - \\mu)^3}{\\sigma^3}$\n",
        "\n",
        "Kurtosis:<br />\n",
        "$\\frac{\\frac{1}{n}\\sum(x - \\mu)^4}{\\sigma^4}$\n",
        "\n",
        "More reading: https://gregorygundersen.com/blog/2020/04/11/moments/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distributions"
      ],
      "metadata": {
        "id": "gvd6Kua9GUAz"
      },
      "id": "gvd6Kua9GUAz"
    },
    {
      "cell_type": "code",
      "source": [
        "# # https://docs.scipy.org/doc/scipy/reference/stats.html#continuous-distributions\n",
        "# # https://towardsdatascience.com/probability-distributions-with-pythons-scipy-3da89bf60565\n",
        "# # https://www.itl.nist.gov/div898/handbook/eda/section3/eda364.htm\n",
        "# from scipy.stats import (\n",
        "#     norm, beta, expon, gamma, genextreme, logistic, lognorm, triang, uniform, fatiguelife,\n",
        "#     gengamma, gennorm, dweibull, dgamma, gumbel_r, powernorm, rayleigh, weibull_max, weibull_min,\n",
        "#     laplace, alpha, genexpon, bradford, betaprime, burr, fisk, genpareto, hypsecant,\n",
        "#     halfnorm, halflogistic, invgauss, invgamma, levy, loglaplace, loggamma, maxwell,\n",
        "#     mielke, ncx2, ncf, nct, nakagami, pareto, lomax, powerlognorm, powerlaw, rice,\n",
        "#     semicircular, trapezoid, rice, invweibull, foldnorm, foldcauchy, cosine, exponpow,\n",
        "#     exponweib, wald, wrapcauchy, truncexpon, truncnorm, t, rdist\n",
        "#     )\n",
        "\n",
        "# distributions = [\n",
        "#     norm, beta, expon, gamma, genextreme, logistic, lognorm, triang, uniform, fatiguelife,\n",
        "#     gengamma, gennorm, dweibull, dgamma, gumbel_r, powernorm, rayleigh, weibull_max, weibull_min,\n",
        "#     laplace, alpha, genexpon, bradford, betaprime, burr, fisk, genpareto, hypsecant,\n",
        "#     halfnorm, halflogistic, invgauss, invgamma, levy, loglaplace, loggamma, maxwell,\n",
        "#     mielke, ncx2, ncf, nct, nakagami, pareto, lomax, powerlognorm, powerlaw, rice,\n",
        "#     semicircular, trapezoid, rice, invweibull, foldnorm, foldcauchy, cosine, exponpow,\n",
        "#     exponweib, wald, wrapcauchy, truncexpon, truncnorm, t, rdist\n",
        "#     ]"
      ],
      "metadata": {
        "id": "hz-VUHq1A62y"
      },
      "id": "hz-VUHq1A62y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How Data Science Leverages Probability Distributions\n",
        "\n",
        "In data science, probability distributions are not just theoretical concepts; they are the primary tools for **modeling the real world**. The core idea is to assume that the messy, random data we collect is generated by some underlying, structured process. A probability distribution is our mathematical description of that process.\n",
        "\n",
        "Here are key distributions and their typical use in data science:\n",
        "\n",
        "1. Normal Distribution (Gaussian or \"Bell Curve\")\n",
        "\n",
        "The Normal Distribution is the cornerstone of statistics and is ubiquitous in data science. It describes phenomena where values cluster around a central mean.\n",
        "\n",
        "* **The Story it Tells:** Most values are average, and extreme values are rare.\n",
        "* **Data Science Application:**\n",
        "    * **Modeling Natural Phenomena:** Physical attributes like height, weight, and blood pressure often follow a normal distribution.\n",
        "    * **Financial Modeling:** Stock market returns are often modeled as being normally distributed.\n",
        "    * **Central Limit Theorem:** This is a critical concept. It states that if you take many large enough samples from *any* population, the distribution of the sample means will be approximately normal. This allows data scientists to make inferences about a whole population from a sample.\n",
        "    * **Assumption in Models:** Linear regression assumes that the errors (residuals) are normally distributed, which is used to validate the model's performance.\n",
        "\n",
        "    **Mathematical Notation:** $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$\n",
        "\n",
        "2. Binomial Distribution\n",
        "\n",
        "This is a discrete distribution that models the number of successes in a fixed number of independent trials.\n",
        "\n",
        "* **The Story it Tells:** \"Out of 'n' attempts, how many will succeed?\" Each attempt has only two outcomes (e.g., success/failure, click/no-click, fraud/not-fraud).\n",
        "* **Data Science Application:**\n",
        "    * **A/B Testing:** \"Did version A or version B of our website have a better conversion rate?\" The number of conversions for a given number of visitors is a binomial variable.\n",
        "    * **Quality Control:** \"If we test 100 products from a batch, how many will be defective?\"\n",
        "    * **Foundation for Logistic Regression:** Logistic regression models the probability of a binary outcome, which is rooted in the Bernoulli distribution (a special case of the Binomial with one trial).\n",
        "\n",
        "    **Mathematical Notation:** $P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}$\n",
        "\n",
        "3. Poisson Distribution\n",
        "\n",
        "This is another discrete distribution that models the number of times an event occurs within a specified interval (e.g., time, area, or volume).\n",
        "\n",
        "* **The Story it Tells:** \"How many events will happen in this time period?\" It assumes events happen at a constant average rate and independently of the time since the last event.\n",
        "* **Data Science Application:**\n",
        "    * **Resource Allocation:** \"How many customer service calls can we expect between 2 PM and 3 PM?\" This helps in staffing.\n",
        "    * **Retail/Inventory:** \"How many customers will walk into a store in the next hour?\"\n",
        "    * **Web Analytics:** \"What is the expected number of clicks on a link per minute?\"\n",
        "\n",
        "    **Mathematical Notation:** $P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$\n",
        "\n",
        "4. Exponential Distribution\n",
        "\n",
        "This distribution is the flip side of the Poisson. It models the time *between* events in a Poisson process.\n",
        "\n",
        "* **The Story it Tells:** \"How long until the next event happens?\"\n",
        "* **Data Science Application:**\n",
        "    * **Customer Lifetime Value (CLV):** Modeling the time until a customer churns (stops using a service).\n",
        "    * **Survival Analysis:** Predicting the lifetime of a mechanical part before it fails.\n",
        "    * **Service Time:** \"How long will a customer have to wait in line?\"\n",
        "\n",
        "    **Mathematical Notation:** $f(x; \\lambda) = \\lambda e^{-\\lambda x}$\n",
        "\n",
        "\n",
        "**The Crucial Distinction: Data Science vs. Information Science vs. Computer Science**\n",
        "\n",
        "This is where the perspective shifts. It's not about *what* the distribution is, but *why* you're using it.\n",
        "\n",
        "---\n",
        "\n",
        "**Data Science**\n",
        "\n",
        "  * **Primary Focus:** Extracting insights and **predictive value** from data.\n",
        "  * **How It Uses Probability Distributions:** To **model real-world phenomena** and the processes that generate data. Distributions are used for statistical inference, hypothesis testing (like A/B tests), and building predictive models.\n",
        "  * **Example Question:** \"Does the number of daily sales follow a normal distribution? How can I use this to forecast next month's revenue?\"\n",
        "\n",
        "-----\n",
        "\n",
        "**Information Science**\n",
        "\n",
        "  * **Primary Focus:** **Organizing, storing, and retrieving** information efficiently and effectively.\n",
        "  * **How It Uses Probability Distributions:** To **analyze the characteristics of information systems** and user behavior. This is used for optimizing system performance, managing resources, and modeling information retrieval patterns.\n",
        "  * **Example Question:** \"Does the frequency of search terms in our library database follow a Zipf distribution? How does this impact our caching strategy?\"\n",
        "\n",
        "-----\n",
        "\n",
        "**Computer Science**\n",
        "\n",
        "  * **Primary Focus:** **Computation, algorithms, and the design of computing systems.**\n",
        "  * **How It Uses Probability Distributions:** To **analyze the theoretical performance (complexity) of algorithms** and to design randomized algorithms. It's about understanding the best, average, and worst-case behavior of code.\n",
        "  * **Example Question:** \"What is the expected runtime of my quicksort algorithm if the input array is drawn from a uniform random distribution?\"\n",
        "\n",
        "-----\n",
        "\n",
        "**In-depth Breakdown:**\n",
        "\n",
        "* A **Data Scientist** looks at your skewed sales data and says, \"This data is right-skewed, possibly following a Log-Normal distribution. This tells me that most sales are small, but we have a few very large, high-impact sales. Let's build a model that accounts for this skew to better predict future revenue.\"\n",
        "    * **Focus:** The meaning of the data itself.\n",
        "\n",
        "* An **Information Scientist** might look at the same data from a database perspective and say, \"The distribution of transaction sizes is skewed. This means our database indexing strategy, which assumes uniform data, might be inefficient. We should re-index on a transformed value to speed up queries for the most common, smaller transactions.\"\n",
        "    * **Focus:** The structure and performance of the information system.\n",
        "\n",
        "* A **Computer Scientist** would be the one who designed the random number generator used to create your original sample. They would use distributions to prove that their algorithm for generating random numbers is truly random and runs in $O(1)$ time on average. They also analyze algorithms by asking how they'd perform on data with different underlying distributions (e.g., best-case, average-case, worst-case analysis).\n",
        "    * **Focus:** The efficiency and correctness of the computation itself.\n",
        "\n",
        "In summary, for a **Data Scientist**, distributions are tools to understand and predict the real world. For an **Information Scientist**, they are tools to manage information systems. For a **Computer Scientist**, they are tools to analyze the abstract behavior of algorithms."
      ],
      "metadata": {
        "id": "iD0E9SQurA0d"
      },
      "id": "iD0E9SQurA0d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Continuous and Discrete\n",
        "\n",
        "The fundamental difference between continuous and discrete distributions lies in the **type of values** they can represent.\n",
        "\n",
        "**Discrete Distributions**\n",
        "\n",
        "Discrete distributions describe events that have a **countable number of possible outcomes**. These outcomes are distinct and separated from one another. You can list all the possible values.\n",
        "\n",
        "* **Values:** Can be integers or specific categories (e.g., number of successes, coin flip results, days of the week).\n",
        "* **Probability:** You can find the probability of a specific, single value, such as $P(X=k)$. The sum of all probabilities for all possible outcomes must equal 1.\n",
        "* **Visual Representation:** Often shown as a bar chart, where each bar represents the probability of a specific value.\n",
        "* **Examples:**\n",
        "    * The number of heads in 5 coin flips (0, 1, 2, 3, 4, or 5).\n",
        "    * The number of students in a classroom.\n",
        "    * The number of defective items in a batch.\n",
        "\n",
        "**Continuous Distributions**\n",
        "\n",
        "Continuous distributions describe events that can take on **any value within a given range**. The number of possible outcomes is infinite and uncountably large.\n",
        "\n",
        "* **Values:** Can be any real number (e.g., height, weight, temperature, time).\n",
        "* **Probability:** The probability of a single, exact value is considered to be zero, so we only talk about probability over an interval, such as $P(a < X < b)$.\n",
        "* **Visual Representation:** Shown as a smooth curve, called a probability density function. The area under the curve over a specific interval represents the probability.\n",
        "* **Examples:**\n",
        "    * The height of a person.\n",
        "    * The exact temperature of a room.\n",
        "    * The time it takes to run a mile.\n",
        "\n",
        "In short, discrete distributions are for **counting**, while continuous distributions are for **measuring**."
      ],
      "metadata": {
        "id": "zgtvRs8pGavi"
      },
      "id": "zgtvRs8pGavi"
    },
    {
      "cell_type": "code",
      "source": [
        "# dist_continu = [d for d in dir(stats) if\n",
        "#                 isinstance(getattr(stats, d), stats.rv_continuous)]\n",
        "# dist_discrete = [d for d in dir(stats) if\n",
        "#                  isinstance(getattr(stats, d), stats.rv_discrete)]\n",
        "# print('number of continuous distributions: %d' % len(dist_continu))\n",
        "# print('number of discrete distributions:   %d' % len(dist_discrete))"
      ],
      "metadata": {
        "id": "SfZkY9pAGOgT"
      },
      "id": "SfZkY9pAGOgT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from scipy.stats import binom\n",
        "\n",
        "# # Parameters for the binomial distribution\n",
        "# # n = number of trials, p = probability of success\n",
        "# n = 10\n",
        "# p = 0.5\n",
        "\n",
        "# # Generate 1000 random samples from the distribution\n",
        "# # This simulates flipping a coin 10 times, 1000 different times\n",
        "# data = binom.rvs(n=n, p=p, size=1000)\n",
        "\n",
        "# # Create a histogram to visualize the distribution\n",
        "# # The bins are set to show only integer values, highlighting the discrete nature\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.hist(data, bins=np.arange(-0.5, n + 1.5, 1), rwidth=0.8, density=True, color='skyblue', edgecolor='black')\n",
        "# plt.title('Discrete Binomial Distribution (n=10, p=0.5)')\n",
        "# plt.xlabel('Number of Successes (Heads)')\n",
        "# plt.ylabel('Probability')\n",
        "# plt.xticks(np.arange(0, n + 1, 1))\n",
        "# plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "u6cfdkeHHOXX"
      },
      "id": "u6cfdkeHHOXX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from scipy.stats import norm\n",
        "\n",
        "# # Parameters for the normal distribution\n",
        "# # loc = mean, scale = standard deviation\n",
        "# mean = 70\n",
        "# std_dev = 5\n",
        "\n",
        "# # Generate 10000 random samples from the distribution\n",
        "# # This simulates collecting data points like people's heights\n",
        "# data = norm.rvs(loc=mean, scale=std_dev, size=10000)\n",
        "\n",
        "# # Create a histogram to visualize the distribution\n",
        "# # The bins are continuous, reflecting the data's nature\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.hist(data, bins=50, density=True, alpha=0.6, color='lightgreen', edgecolor='black', label='Sample Data')\n",
        "\n",
        "# # Overlay the theoretical probability density function (PDF)\n",
        "# xmin, xmax = plt.xlim()\n",
        "# x = np.linspace(xmin, xmax, 100)\n",
        "# p = norm.pdf(x, mean, std_dev)\n",
        "# plt.plot(x, p, 'k', linewidth=2, label='Theoretical PDF')\n",
        "\n",
        "# plt.title('Continuous Normal Distribution')\n",
        "# plt.xlabel('Value')\n",
        "# plt.ylabel('Probability Density')\n",
        "# plt.legend()\n",
        "# plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "aLNqPb1uHSaC"
      },
      "id": "aLNqPb1uHSaC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d3713892",
      "metadata": {
        "id": "d3713892"
      },
      "source": [
        "## Correlation\n",
        "\n",
        "According to Wikipedia (2022):\n",
        "\n",
        ">  In statistics, correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data. In the broadest sense correlation is any statistical association, though it actually refers to the degree to which a pair of variables are linearly related. Familiar examples of dependent phenomena include the correlation between the height of parents and their offspring, and the correlation between the price of a good and the quantity the consumers are willing to purchase... Correlations - 2 are useful because they can indicate a predictive relationship that can be exploited in practice (paras. 1 - 2).\n",
        "\n",
        "Correlation. (2022, February 1). In *Wikipedia*. https://en.wikipedia.org/wiki/Correlation.\n",
        "\n",
        "Correlation does not cause causation. Warm days on the beach, ice cream, and shark bites.\n",
        "\n",
        "Covariance:<br />\n",
        "$cov(x, y) = \\frac{1}{N} \\sum_{i=1}^{N}(x_i - \\bar{x}) (y_i - \\bar{y})$\n",
        "\n",
        "* Shows how variables change together\n",
        "* A measure of correlation\n",
        "* Measures direction\n",
        "\n",
        "Pearson’s r (correlation coefficient):<br />\n",
        "$\\rho_{x,y} = \\frac{cov(x,y)}{\\sigma_x\\sigma_y} = \\frac{\\frac{1}{N}\\sum(x-\\bar{x})(y-\\bar{y})}{\\sqrt\\frac{\\sum(x-\\bar{x})^2}{N}\\sqrt\\frac{\\sum(y-\\bar{y})^2}{N}}  = \\frac{\\sum(x-\\bar{x})(y-\\bar{y})}{\\sqrt{\\sum(x-\\bar{x})^2}\\sqrt{\\sum(y-\\bar{y})^2}}$\n",
        "\n",
        "* Shows linear relationship between two continuous variables\n",
        "* How one variable changes as another variable changes\n",
        "* Measures both strength and direction\n",
        "\n",
        "Resources:\n",
        "\n",
        "* https://statistics.laerd.com/statistical-guides/pearson-correlation-coefficient-statistical-guide.php\n",
        "* https://www.mygreatlearning.com/blog/covariance-vs-correlation/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # show correlation between the features\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# # correlation matrix\n",
        "# sns.set(style=\"white\")\n",
        "\n",
        "# # compute the correlation matrix\n",
        "# corr = X_train[['Tutorial', 'Midterm', 'Quiz', 'Final']].corr()\n",
        "\n",
        "# # generate a mask for the upper triangle\n",
        "# mask = np.zeros_like(corr, dtype=bool)\n",
        "# mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "# # set up the matplotlib figure\n",
        "# # f, ax = plt.subplots()\n",
        "# f = plt.figure(figsize=(8, 8))\n",
        "\n",
        "# # generate a custom diverging colormap\n",
        "# cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "# # draw the heatmap with the mask and correct aspect ratio\n",
        "# sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "#             square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True);\n",
        "\n",
        "# plt.tight_layout()"
      ],
      "metadata": {
        "id": "jn57whZWn1jR"
      },
      "id": "jn57whZWn1jR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c716c7d4",
      "metadata": {
        "id": "c716c7d4"
      },
      "outputs": [],
      "source": [
        "# # showing correlation of multiple features with one target\n",
        "# X_train.corrwith(y_train, numeric_only=True).plot.bar(\n",
        "#         title = \"Correlation with Target\", fontsize = 15,\n",
        "#         rot = 45, grid = True);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multicollinearity\n",
        "\n",
        "**Multicollinearity**: Occurs when predictor variables in a model are correlated with each other, meaning they are not independent.\n",
        "\n",
        "The need to reduce multicollinearity depends on its severity and your primary goal for your regression model. Keep the following three points in mind:\n",
        "* The severity of the problems increases with the degree of the multicollinearity. Therefore, if you have only moderate multicollinearity, you may not need to resolve it.\n",
        "* Multicollinearity affects only the specific independent variables that are correlated. Therefore, if multicollinearity is not present for the independent variables that you are particularly interested in, you may not need to resolve it. Suppose your model contains the experimental variables of interest and some control variables. If high multicollinearity exists for the control variables but not the experimental variables, then you can interpret the experimental variables without problems.\n",
        "* Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics. If your primary goal is to make predictions, and you don’t need to understand the role of each independent variable, you don’t need to reduce severe multicollinearity.\n",
        "https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/"
      ],
      "metadata": {
        "id": "8TAssKLso57R"
      },
      "id": "8TAssKLso57R"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spurious Correlations\n",
        "\n",
        "* https://www.tylervigen.com/spurious-correlations"
      ],
      "metadata": {
        "id": "Z5qASGtTeEbJ"
      },
      "id": "Z5qASGtTeEbJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Law of Large Numbers"
      ],
      "metadata": {
        "id": "YB92k7N3ogzA"
      },
      "id": "YB92k7N3ogzA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Central Limit Theorem"
      ],
      "metadata": {
        "id": "eUrBSeAzokSH"
      },
      "id": "eUrBSeAzokSH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visualization"
      ],
      "metadata": {
        "id": "sA1N-OGXntb6"
      },
      "id": "sA1N-OGXntb6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SweetViz\n",
        "\n",
        "* Pandas-Profiling\n",
        "* Autoviz\n",
        "* D-Tale\n",
        "* https://towardsdatascience.com/4-libraries-that-can-perform-eda-in-one-line-of-python-code-b13938a06ae\n",
        "* https://pypi.org/project/sweetviz/\n",
        "* https://colab.research.google.com/drive/1-md6YEwcVGWVnQWTBirQSYQYgdNoeSWg?usp=sharing"
      ],
      "metadata": {
        "id": "a7LoxB7pDknf"
      },
      "id": "a7LoxB7pDknf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python Graph Gallery\n",
        "\n",
        "* https://python-graph-gallery.com/"
      ],
      "metadata": {
        "id": "wHpo3Mqin-tm"
      },
      "id": "wHpo3Mqin-tm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data to Viz\n",
        "\n",
        "* https://www.data-to-viz.com/"
      ],
      "metadata": {
        "id": "PS4wzD6inxQo"
      },
      "id": "PS4wzD6inxQo"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}