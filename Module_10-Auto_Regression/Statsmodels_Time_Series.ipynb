{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "19INpwxBRM4j8bI4PsxPDDev2MzwXiIA5",
      "authorship_tag": "ABX9TyM9lVGK34TQtnDnWHbI0dj5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC5502/blob/main/Module_10-Auto_Regression/Statsmodels_Time_Series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statsmodels Time Series\n",
        "\n",
        "Name"
      ],
      "metadata": {
        "id": "MBdYRP7vYN2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "* Colab - get notebook from gitmystuff DTSC5502 repository\n",
        "* Save a Copy in Drive\n",
        "* Remove Copy of\n",
        "* Submit shared link in Canvas\n"
      ],
      "metadata": {
        "id": "mCxIO4Zoj2so"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "* Autoregression\n",
        "* Time Series with Statsmodels\n",
        "* ETS\n",
        "* Stationary Time Series\n",
        "* EWMA\n",
        "* Holt-Winters\n",
        "* ACF\n",
        "* PACF\n",
        "* ARIMA\n",
        "* SARIMA\n",
        "* Finance Terms"
      ],
      "metadata": {
        "id": "bB8BpM-rj9XS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoregression\n",
        "\n",
        "The nutshell - Uses observations from previous time steps as input to a regression equation to predict the value at the next time step.\n",
        "\n",
        "Autoregression is a time series modeling technique that predicts future values based on past values of the same series. It's like saying, \"knowing what happened in the past can help us understand what might happen in the future.\"\n",
        "\n",
        "Think of it like this: Imagine you're tracking the daily temperature. You notice that if today is hot, tomorrow is also likely to be hot.  Autoregression captures this relationship by using past temperatures to predict future temperatures.\n",
        "\n",
        "**Here's a more formal definition:**\n",
        "\n",
        "Autoregression is a statistical model that uses a linear combination of *lagged values* of a time series to predict future values. A lagged value is simply a previous value in the series.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "* **Order of Autoregression (p):**  The number of lagged values used in the model. For example, an autoregressive model of order 2 (AR(2)) uses the two previous values to predict the current value.\n",
        "* **Coefficients:** Each lagged value is multiplied by a coefficient that determines its influence on the prediction. These coefficients are estimated from the data.\n",
        "* **Error term:**  The model also includes an error term to account for random fluctuations and unpredictable factors.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "An AR(1) model for predicting temperature (T) might look like this:\n",
        "\n",
        "  T(t) = c + a * T(t-1) + error(t)\n",
        "\n",
        "where:\n",
        "\n",
        "* T(t) is the temperature at time t\n",
        "* T(t-1) is the temperature at the previous time step (t-1)\n",
        "* c is a constant term\n",
        "* a is the coefficient for the lagged temperature\n",
        "* error(t) is the random error at time t\n",
        "\n",
        "**Applications:**\n",
        "\n",
        "* **Forecasting:** Predicting future values of time series data, such as stock prices, sales, or weather patterns.\n",
        "* **Economics:**  Analyzing economic data and understanding relationships between variables over time.\n",
        "* **Signal processing:**  Filtering noise and extracting signals from data.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* **Simple and interpretable:** The model is relatively easy to understand and interpret.\n",
        "* **Effective for many time series:**  Works well for data with clear autocorrelations (relationships between values at different time lags).\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "* **Assumes linearity:**  The model assumes a linear relationship between past and future values, which may not always be true.\n",
        "* **Requires stationary data:**  The time series should be stationary (have constant statistical properties over time) for the model to be effective.\n",
        "\n",
        "Overall, autoregression is a valuable tool for analyzing and forecasting time series data, providing insights into the underlying patterns and relationships within the data.\n"
      ],
      "metadata": {
        "id": "18nk7V2Mn9aX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # https://fred.stlouisfed.org/series/POPTHM\n",
        "# import pandas as pd\n",
        "\n",
        "# pop_url = 'https://raw.githubusercontent.com/gitmystuff/Datasets/main/uspopulation.csv'\n",
        "# pop = pd.read_csv(pop_url, index_col='DATE', parse_dates=True)\n",
        "# pop.index.freq = 'MS'\n",
        "# print(len(pop))\n",
        "\n",
        "# pop['PopEst'].plot().autoscale(axis='x', tight=True);"
      ],
      "metadata": {
        "id": "kUujE0lYz0Zg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # train and predict\n",
        "# import matplotlib.pyplot as plt\n",
        "# from statsmodels.tsa.ar_model import AutoReg\n",
        "\n",
        "# train = pop.iloc[:84]\n",
        "# test = pop.iloc[84:] # test len should be same len as forecast\n",
        "\n",
        "# model = AutoReg(train['PopEst'], lags=[1, 2, 11, 12], seasonal=True, period=12).fit()\n",
        "# predictions = model.predict(start=len(train), end=len(pop)-1)\n",
        "\n",
        "# test['PopEst'].plot(legend=True)\n",
        "# predictions.plot(label='Predictions')\n",
        "# plt.title(f'AR({model.ar_lags}) Fit with Predictions')\n",
        "# plt.legend()\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "eXTpbVZZDlL6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code `model = AutoReg(train['PopEst'], lags=[1, 2, 11, 12], seasonal=True, period=12).fit()`, the `lags=[1, 2, 11, 12]` argument specifies the autoregressive lags to be included in the model.\n",
        "\n",
        "Here's what that means:\n",
        "\n",
        "* **Autoregressive lags (AR lags):** These are the past values of the time series that are used to predict the current value. In this case, the model will use the values from 1, 2, 11, and 12 periods ago.\n",
        "* **`lags=[1, 2, 11, 12]`:** This list specifies that the model should include the first, second, eleventh, and twelfth lags in the autoregressive equation.\n",
        "* **Interpretation:** This suggests that the model is considering both short-term (lags 1 and 2) and seasonal (lags 11 and 12) patterns in the data. Since the `period=12` argument is also provided, it indicates that the data likely has a yearly seasonality (e.g., monthly data with patterns repeating every 12 months).\n",
        "\n",
        "**In simpler terms:**\n",
        "\n",
        "Imagine you're trying to predict the population of a city. This model is saying that the population in the current month is likely influenced by:\n",
        "\n",
        "* The population in the previous month (lag 1)\n",
        "* The population two months ago (lag 2)\n",
        "* The population in the same month last year (lag 11)\n",
        "* The population in the month before the same month last year (lag 12)\n",
        "\n",
        "By including these specific lags, the model aims to capture both the short-term trends and the yearly seasonal patterns in the population data.\n"
      ],
      "metadata": {
        "id": "oFMZMtrmW8Ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # forecast\n",
        "# model = AutoReg(pop['PopEst'], lags=[1, 2, 11, 12], seasonal=True, period=12).fit()\n",
        "# forecast = model.predict(start=len(pop), end=len(pop)+12)\n",
        "\n",
        "# pop['PopEst'].plot(legend=True)\n",
        "# forecast.plot(label='Forecast')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "cfIq2puAH2f8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Time Series"
      ],
      "metadata": {
        "id": "CMptHeeloc6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine you're trying to predict the weather. You know that today's temperature is often related to yesterday's temperature. Maybe it's a bit warmer or cooler, but it usually doesn't jump from freezing to scorching heat overnight. This is the basic idea behind autoregression in time series.\n",
        "\n",
        "**Autoregression (AR)** is a way to predict future values in a time series by looking at past values of the *same* series. It assumes that the current value is influenced by a weighted sum of its own previous values.\n",
        "\n",
        "Think of it like this:\n",
        "\n",
        "* **Time series:** A sequence of data points collected over time (e.g., daily temperature, stock prices, monthly sales).\n",
        "* **Autoregression:**  Using past values within the time series itself to predict future values.\n",
        "* **Regression:** A statistical method to find relationships between variables. In this case, the relationship is between the current value and its own past values.\n",
        "\n",
        "**Here's a simple example:**\n",
        "\n",
        "Let's say you want to predict tomorrow's temperature. An autoregressive model might say:\n",
        "\n",
        "\"Tomorrow's temperature will be 0.8 times today's temperature plus 0.1 times yesterday's temperature plus some random error.\"\n",
        "\n",
        "In this case:\n",
        "\n",
        "* **0.8 and 0.1 are the weights** assigned to the previous two days' temperatures. These weights determine how much influence each past value has on the prediction.\n",
        "* **The random error** accounts for unpredictable factors that the model can't capture.\n",
        "\n",
        "**Key concepts in autoregression:**\n",
        "\n",
        "* **Lag:** The time difference between observations.  A lag of 1 means using the value from the previous time step.\n",
        "* **Order:** The number of past values used in the prediction.  An AR model of order 2 (AR(2)) uses the values from the two previous time steps.\n",
        "* **Autocorrelation:** The correlation between a time series and a lagged version of itself. It measures how strongly past values are related to current values.\n",
        "\n",
        "**Why is autoregression useful?**\n",
        "\n",
        "* **Forecasting:** Predict future values based on historical trends.\n",
        "* **Understanding patterns:** Identify recurring patterns and seasonality in data.\n",
        "* **Control:**  In some cases, AR models can be used to design control systems that react to changes in a time series.\n",
        "\n",
        "Autoregression is a powerful tool for analyzing and predicting time series data. It's used in various fields like finance, economics, weather forecasting, and engineering.\n"
      ],
      "metadata": {
        "id": "kCQCMsJ-oN9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLMs"
      ],
      "metadata": {
        "id": "3XsIIJHoog3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Think of a large language model (LLM) that learns to predict the next word in a sequence based on the words that came before it. This is where autoregression comes in.\n",
        "\n",
        "**Autoregression in LLMs**\n",
        "\n",
        "LLMs use autoregression to generate text by predicting the probability of each word given its preceding words. It's like a chain reaction:\n",
        "\n",
        "1. **Start with a prompt:** You give the LLM a starting phrase or sentence.\n",
        "2. **Predict the next word:** The LLM analyzes the prompt and calculates the probability of different words following it. For example, if the prompt is \"The cat sat on the\", the LLM might predict a high probability for words like \"mat\", \"chair\", or \"table\".\n",
        "3. **Generate the next word:** The LLM selects the most likely word based on its prediction and adds it to the sequence.\n",
        "4. **Repeat:** The LLM continues this process, predicting and generating one word at a time, building a coherent and contextually relevant text.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Prompt: \"The quick brown fox\"\n",
        "\n",
        "LLM prediction:\n",
        "* \"jumps\" - high probability\n",
        "* \"runs\" - medium probability\n",
        "* \"sleeps\" - low probability\n",
        "\n",
        "LLM output: \"The quick brown fox jumps\"\n",
        "\n",
        "**How LLMs learn autoregression:**\n",
        "\n",
        "LLMs are trained on massive amounts of text data. During training, they learn to:\n",
        "\n",
        "* **Identify patterns:**  Recognize common sequences of words and phrases.\n",
        "* **Estimate probabilities:**  Calculate the likelihood of different words following a given context.\n",
        "* **Generate coherent text:** Produce text that flows naturally and makes sense.\n",
        "\n",
        "**Benefits of autoregression in LLMs:**\n",
        "\n",
        "* **High-quality text generation:**  LLMs can generate remarkably human-like text, including stories, articles, and even code.\n",
        "* **Contextual understanding:** LLMs can maintain context over long sequences of text, allowing them to generate coherent and relevant responses.\n",
        "* **Flexibility:** Autoregressive LLMs can be used for various tasks, such as translation, summarization, and question answering.\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "* **Bias:** LLMs can inherit biases from their training data, leading to unfair or discriminatory outputs.\n",
        "* **Lack of common sense:** LLMs may struggle with tasks that require real-world knowledge or reasoning.\n",
        "* **Computational cost:**  Training and running large LLMs can be computationally expensive.\n",
        "\n",
        "Despite these limitations, autoregression is a core technique behind the impressive capabilities of modern LLMs. It enables them to learn from data and generate human-quality text in a wide range of applications.\n"
      ],
      "metadata": {
        "id": "j4WbJT63oZ_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fly Me to the Moon"
      ],
      "metadata": {
        "id": "ebdipRdiQJaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ETS\n",
        "\n",
        "https://www.statsmodels.org/dev/examples/notebooks/generated/ets.html\n",
        "\n",
        "* Error\n",
        "* Trend\n",
        "* Seasonalality\n",
        "* Exponential Smoothing\n",
        "* Trend Methods\n",
        "* ETS Decomposition\n",
        "* Additive Models: Used when trend is linear and/or seasonal variations are constant\n",
        "* Multiplicative: Used when trend is non linear (exponential) and/or seasonality varies proportional to the level of the series"
      ],
      "metadata": {
        "id": "aqBVHYmRZYOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Moving Average\n",
        "\n",
        "The moving average is a simple but powerful technique used to smooth out fluctuations in time series data and highlight underlying trends. It works by calculating the average of a specified number of consecutive data points, creating a new series of averages.\n",
        "\n",
        "**Here's how it works:**\n",
        "\n",
        "1. **Choose a window size:** Decide how many data points you want to include in each average. This is called the \"window\" or \"order\" of the moving average. For example, a 3-month moving average would use the average of the current month and the two preceding months.\n",
        "\n",
        "2. **Calculate the averages:**  Slide the window across the time series, calculating the average of the data points within the window at each step.\n",
        "\n",
        "3. **Create a new series:** The calculated averages form a new time series that is smoother than the original data.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Let's say you have monthly sales data:\n",
        "\n",
        "| Month | Sales |\n",
        "|---|---|\n",
        "| Jan | 100 |\n",
        "| Feb | 120 |\n",
        "| Mar | 110 |\n",
        "| Apr | 130 |\n",
        "| May | 140 |\n",
        "\n",
        "A 3-month moving average would be calculated as follows:\n",
        "\n",
        "| Month | Sales | 3-Month Moving Average |\n",
        "|---|---|---|\n",
        "| Jan | 100 | - |\n",
        "| Feb | 120 | - |\n",
        "| Mar | 110 | (100 + 120 + 110) / 3 = 110 |\n",
        "| Apr | 130 | (120 + 110 + 130) / 3 = 120 |\n",
        "| May | 140 | (110 + 130 + 140) / 3 = 126.67 |\n",
        "\n",
        "**Types of Moving Averages:**\n",
        "\n",
        "* **Simple Moving Average (SMA):**  All data points within the window are given equal weight.\n",
        "* **Weighted Moving Average (WMA):**  Assigns different weights to data points, typically giving more weight to recent observations.\n",
        "* **Exponential Moving Average (EMA):**  A type of WMA that gives exponentially decreasing weights to older data points.\n",
        "\n",
        "**Applications:**\n",
        "\n",
        "* **Smoothing data:**  Reduces noise and highlights trends.\n",
        "* **Technical analysis:** Used in finance to identify trends and generate trading signals.\n",
        "* **Forecasting:** Can be used to make simple forecasts by extrapolating the trend.\n",
        "\n",
        "**Choosing the window size:**\n",
        "\n",
        "* **Smaller window:**  Responds more quickly to changes in the data but is less smooth.\n",
        "* **Larger window:**  Produces a smoother trend but lags behind changes in the data.\n",
        "\n",
        "The choice of window size depends on the specific application and the desired balance between responsiveness and smoothness.\n"
      ],
      "metadata": {
        "id": "Bf6LUJ__VjZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# date_rng = pd.date_range(start='2032-01-01', end='2043-12-01', freq='MS')\n",
        "\n",
        "# # Generate a base series with an upward trend\n",
        "# base_passengers = np.linspace(1000, 5000, len(date_rng))  # Start with 1000, steadily increase to 5000\n",
        "\n",
        "# # Add some random noise to simulate real-world fluctuations\n",
        "# noise = np.random.normal(0, 200, len(date_rng))\n",
        "# passengers = base_passengers + noise\n",
        "\n",
        "# # Ensure no negative or zero values\n",
        "# passengers = np.maximum(passengers, 1)\n",
        "# passengers = np.round(passengers).astype(int)  # Round to integers and convert to int\n",
        "\n",
        "# # Create the DataFrame\n",
        "# # df = pd.DataFrame({'Passengers': passengers}, index=date_rng)\n",
        "\n",
        "# df = pd.DataFrame({'Passengers': passengers, 'Month': date_rng})\n",
        "# df.set_index('Month', inplace=True)\n",
        "\n",
        "# print(df.info())\n",
        "# print(df.head())\n",
        "# print(df.groupby(df.index.year).size())\n",
        "# starliner = df"
      ],
      "metadata": {
        "id": "CnvevnQSMY2k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# starliner.tail()"
      ],
      "metadata": {
        "id": "kmzsewT29d6m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ets decomposition\n",
        "# from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# seasonal_decompose(starliner['Passengers'], model='multiplicative').plot();"
      ],
      "metadata": {
        "id": "tZQUsdYvkLnm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EWMA\n",
        "\n",
        "https://towardsdatascience.com/time-series-analysis-with-statsmodels-12309890539a\n",
        "\n",
        "* Exponential Weighted Moving Average\n",
        "* Same window for SMA vs EWMA allows weighting more relevant data\n",
        "\n",
        "EWMA stands for **Exponentially Weighted Moving Average**. It's a way to smooth out data by giving more weight to recent observations and less weight to older observations. This makes it particularly useful for analyzing time series data where the most recent data points are often the most relevant.\n",
        "\n",
        "Here's a breakdown of how EWMA works:\n",
        "\n",
        "**1. Calculating the EWMA**\n",
        "\n",
        "The EWMA is calculated using the following formula:\n",
        "\n",
        "```\n",
        "EWMA(t) = α * X(t) + (1 - α) * EWMA(t-1)\n",
        "```\n",
        "\n",
        "Where:\n",
        "\n",
        "* `EWMA(t)` is the EWMA at time t\n",
        "* `α` is the smoothing factor (a value between 0 and 1)\n",
        "* `X(t)` is the observation at time t\n",
        "* `EWMA(t-1)` is the EWMA at time t-1\n",
        "\n",
        "**2. The Smoothing Factor (α)**\n",
        "\n",
        "The smoothing factor (`α`) determines how much weight is given to recent observations.\n",
        "\n",
        "* A higher `α` gives more weight to recent observations, making the EWMA more responsive to changes in the data.\n",
        "* A lower `α` gives less weight to recent observations, making the EWMA smoother and less sensitive to noise.\n",
        "\n",
        "**3. How it Works**\n",
        "\n",
        "The EWMA starts with an initial value (often the first observation in the series). Then, for each subsequent observation, the EWMA is calculated by taking a weighted average of the current observation and the previous EWMA.  This process continues, with each new EWMA being influenced by all previous observations, but with exponentially decreasing weights.\n",
        "\n",
        "**Key Advantages of EWMA:**\n",
        "\n",
        "* **Simple to understand and implement.**\n",
        "* **Requires minimal data storage.**\n",
        "* **Adapts to changes in the data.**\n",
        "* **Can be used for forecasting.**\n",
        "\n",
        "**Common Applications:**\n",
        "\n",
        "* **Technical Analysis:** Used to smooth price data and identify trends.\n",
        "* **Volatility Modeling:**  Used to estimate the volatility of financial assets.\n",
        "* **Quality Control:** Used to monitor processes and detect shifts in performance.\n",
        "* **Forecasting:** Used to make predictions about future values.\n",
        "\n",
        "\n",
        "**Analogy:**\n",
        "\n",
        "Imagine you're trying to estimate the average temperature in a room. You could take the average of all the temperature readings you've ever taken, but that wouldn't be very useful because the older readings are less relevant to the current temperature. Instead, you could use an EWMA, which would give more weight to recent readings and less weight to older readings. This would give you a more accurate estimate of the current temperature.\n"
      ],
      "metadata": {
        "id": "GQxnuGnfZvdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # add 6 and 12 month sma and annual ewma\n",
        "# starliner['6-month-SMA'] = starliner['Passengers'].rolling(window=6).mean()\n",
        "# starliner['12-month-SMA'] = starliner['Passengers'].rolling(window=12).mean()\n",
        "# starliner['12-month-EWMA'] = starliner['Passengers'].ewm(span=12,adjust=False).mean()\n",
        "# starliner.plot();"
      ],
      "metadata": {
        "id": "-9nx_vYwKe_v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Holt-Winters\n",
        "\n",
        "* EWMA has just one smoothing factor and doesn't account for trend, seasonality, etc.\n",
        "* HW offers three smoothing factors for level, trend, and season ($\\alpha, \\beta, \\gamma$) and can adjust for divisions per cycle (L)\n",
        "* Offers Single, Double (Holt and Trend), and Triple (Holt-Winters and Seasonality) Exponential Smoothing\n",
        "\n",
        "Both Holt-Winters and EWMA are smoothing methods used for time series analysis and forecasting, but they differ in their complexity and ability to handle trends and seasonality:\n",
        "\n",
        "**EWMA (Exponentially Weighted Moving Average)**\n",
        "\n",
        "* **Focus:** Smooths out data by giving more weight to recent observations.\n",
        "* **Handles:**  Suitable for data with no clear trend or seasonality.\n",
        "* **Parameters:**  One main parameter: the smoothing factor (α).\n",
        "* **Limitations:** Doesn't explicitly account for trends or seasonality in the data.\n",
        "\n",
        "**Holt-Winters**\n",
        "\n",
        "* **Focus:**  A more advanced method that can handle both trend and seasonality.\n",
        "* **Handles:**  Suitable for data with trends and/or seasonal patterns.\n",
        "* **Parameters:** Three main parameters:\n",
        "    *  α (smoothing factor for the level)\n",
        "    *  β (smoothing factor for the trend)\n",
        "    *  γ (smoothing factor for the seasonality)\n",
        "* **Variations:**  \n",
        "    * **Single Exponential Smoothing:**  Similar to EWMA, handles only the level.\n",
        "    * **Double Exponential Smoothing:** Handles level and trend.\n",
        "    * **Triple Exponential Smoothing:**  Handles level, trend, and seasonality.\n",
        "\n",
        "**Here's an analogy:**\n",
        "\n",
        "Imagine you're tracking the sales of ice cream over time.\n",
        "\n",
        "* **EWMA:**  Like averaging daily sales with a focus on recent days. It might show you if sales are generally increasing or decreasing, but it won't capture the summer peak.\n",
        "* **Holt-Winters:** Like considering not just the average sales, but also how quickly sales are increasing towards the summer (trend) and the repeating pattern of high sales every summer (seasonality).\n",
        "\n",
        "**Key Differences:**\n",
        "\n",
        "| Feature | EWMA | Holt-Winters |\n",
        "|---|---|---|\n",
        "| Trend | No explicit handling | Explicitly models trend |\n",
        "| Seasonality | No explicit handling | Explicitly models seasonality |\n",
        "| Complexity | Simpler | More complex |\n",
        "| Parameters | One (α) | Three (α, β, γ) |\n",
        "| Forecasting | Short-term forecasts | Short- to medium-term forecasts |\n",
        "\n",
        "**When to Use Which:**\n",
        "\n",
        "* **EWMA:** When dealing with data that doesn't have a clear trend or seasonality and you need a simple smoothing method.\n",
        "* **Holt-Winters:** When dealing with time series data that exhibits trends and/or seasonality, and you need more accurate forecasts.\n",
        "\n",
        "In essence, Holt-Winters is an extension of EWMA that incorporates trend and seasonality, making it more suitable for complex time series data.\n",
        "\n",
        "\n",
        "Source\n",
        "* https://www.statsmodels.org/dev/tsa.html#exponential-smoothing"
      ],
      "metadata": {
        "id": "mhOEyzteaOm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing\n",
        "\n",
        "# span = 12\n",
        "# alpha = 2/(span+1)\n",
        "\n",
        "# starliner['12-month-SES'] = SimpleExpSmoothing(starliner['Passengers']).fit(smoothing_level=alpha,\n",
        "#                                                                         optimized=False).fittedvalues.shift(-1) # shift corrects for optimization\n",
        "# starliner['12-month-DES'] = ExponentialSmoothing(starliner['Passengers'],\n",
        "#                                                trend='mul').fit().fittedvalues.shift(-1)\n",
        "# starliner['12-month-TES'] = ExponentialSmoothing(starliner['Passengers'],\n",
        "#                                                trend='mul',\n",
        "#                                                seasonal='mul',\n",
        "#                                                seasonal_periods=12).fit().fittedvalues\n",
        "\n",
        "# print(starliner.info())\n",
        "# print(starliner.columns)"
      ],
      "metadata": {
        "id": "euGGtCcrvsN_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plots = ['Passengers', '6-month-SMA', '12-month-SMA', '12-month-EWMA', '12-month-DES', '12-month-TES', '12-month-SES']\n",
        "# starliner[['Passengers', '12-month-SES', '12-month-DES', '12-month-TES']].plot(figsize=(12,5)).autoscale(axis='x', tight=True);"
      ],
      "metadata": {
        "id": "2cJFTDjgNxSF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # zoom in first two years\n",
        "# starliner[['Passengers', '12-month-SES', '12-month-DES', '12-month-TES']].iloc[:24].plot(figsize=(12,5)).autoscale(axis='x', tight=True);"
      ],
      "metadata": {
        "id": "eFMBCrrhOrCv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# starliner.tail()"
      ],
      "metadata": {
        "id": "b6HIZeYEPXfU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # zoom in last two years\n",
        "# starliner[['Passengers', '12-month-SES', '12-month-DES', '12-month-TES']]['2022-01-01':].plot(figsize=(12,5)).autoscale(axis='x', tight=True);"
      ],
      "metadata": {
        "id": "u8GG-j9iOzs_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stationary Time Series"
      ],
      "metadata": {
        "id": "vzE-B_qhfbDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # non-stationary\n",
        "# starliner['Passengers'].plot();"
      ],
      "metadata": {
        "id": "vRJNjQaG2T5P"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # births - can you spot any trends\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# # Set the start and end dates for the year\n",
        "# start_date = pd.to_datetime('2023-01-01')\n",
        "# end_date = pd.to_datetime('2023-12-31')\n",
        "\n",
        "# # Generate a daily datetime index for the year\n",
        "# date_range = pd.date_range(start_date, end_date, freq='D')\n",
        "\n",
        "# # Simulate the number of female births per day\n",
        "# # We'll use a normal distribution with a mean of 50 and a standard deviation of 10\n",
        "# # We'll also ensure there are no negative values or 0s\n",
        "# births_per_day = np.random.normal(loc=50, scale=10, size=len(date_range))\n",
        "# births_per_day = np.maximum(births_per_day, 1).astype(int)  # Ensure no negative values or 0s\n",
        "\n",
        "# # Create a DataFrame with the datetime index and simulated birth data\n",
        "# births = pd.DataFrame({'Births': births_per_day}, index=date_range)\n",
        "\n",
        "# print(births.info())\n",
        "# print(births.head())\n",
        "# births.plot();"
      ],
      "metadata": {
        "id": "8jjUxGQ-X_8_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passenger Data (Non-Stationary)\n",
        "\n",
        "\n",
        "### Key Characteristics:\n",
        "* **Data:** Likely monthly **Passenger** counts over several years (labeled 2033 to 2043). *Note: This is the same underlying data from the previous question.*\n",
        "* **Trend:** It exhibits a very strong, **obvious upward trend**. The passenger numbers are consistently increasing year after year.\n",
        "* **Seasonality/Cycles:** A clear **seasonal pattern** is visible, with regular peaks and troughs recurring every year.\n",
        "* **Variance:** The amplitude of the seasonal peaks seems to be **increasing** as the mean value increases (called **multiplicative seasonality**).\n",
        "\n",
        "### Stationarity Status:\n",
        "* This series is clearly **Non-Stationary** because:\n",
        "    1.  Its **mean** is **not constant** (it has an upward **Trend**).\n",
        "    2.  Its **variance** is **not constant** (the seasonal amplitude increases over time).\n",
        "\n",
        "---\n",
        "\n",
        "### Daily Births (Non-Stationary in Mean, Not in Variance)\n",
        "\n",
        "\n",
        "### Key Characteristics:\n",
        "* **Data:** Daily number of **Births** over a single year (2023).\n",
        "* **Trend:** There is **no clear upward or downward trend** over the year. The average number of births seems to hover around a consistent level (e.g., 40-50).\n",
        "* **Seasonality/Cycles:** There are no strong, repeating seasonal patterns (like monthly peaks/troughs) visible. The fluctuations appear **random** (noise).\n",
        "* **Variance:** The variation (volatility or amplitude) of the line appears relatively **constant** throughout the year.\n",
        "\n",
        "### Stationarity Status:\n",
        "* This series is close to being **stationary in mean**, but the high day-to-day **randomness (noise)** makes it a complex, highly volatile series. For practical modeling, one might assume its **mean and variance are constant** over time.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary of Differences\n",
        "\n",
        "| Feature | Daily Births (Chart 1) | Passenger Data (Chart 2) |\n",
        "| :--- | :--- | :--- |\n",
        "| **Trend** | No significant trend (stationary mean) | **Strong upward trend** (non-stationary mean) |\n",
        "| **Seasonality** | No clear pattern (high noise) | **Strong and clear seasonal pattern** |\n",
        "| **Volatility** | High, but constant variance | **Increasing variance** (multiplicative seasonality) |\n",
        "| **Modeling Goal** | De-noising, identifying daily effects | **Modeling/Removing trend & seasonality** |"
      ],
      "metadata": {
        "id": "KoLmLWkfgrQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forecasting"
      ],
      "metadata": {
        "id": "0lu2STItP2p9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Holt-Winters"
      ],
      "metadata": {
        "id": "42H-KNcAw36a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "\n",
        "# train = starliner.iloc[:108].dropna()\n",
        "# test = starliner.iloc[108:].dropna()\n",
        "# # print(len(test))\n",
        "\n",
        "# model = ExponentialSmoothing(train['Passengers'],\n",
        "#                              trend='mul',\n",
        "#                              seasonal='mul',\n",
        "#                              seasonal_periods=12).fit()\n",
        "\n",
        "# predictions = model.forecast(len(test))\n",
        "\n",
        "# train['Passengers'].plot(legend=True, label='train', figsize=(12,5))\n",
        "# test['Passengers'].plot(legend=True, label='test')\n",
        "# predictions.plot(legend=True, label='predictions');"
      ],
      "metadata": {
        "id": "Ox00fe5j3zSm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "\n",
        "* Accuracy, precision, and recall aren't applicable for time series\n",
        "* MSE: Bigger residuals are emphasized\n",
        "* RMSE: Expresses MSE in the same units of the data compared to the standard deviation and leans towards the mean\\\n",
        "* Compare RMSE to the average values (no 100% answer)\n",
        "* MAE (Mean Absolute Error): Forecasts lean to the median\n",
        "* AIC/BIC: Metrics to compare models, the lower the better at forecasting\n",
        "* Impossible to evaluate future data"
      ],
      "metadata": {
        "id": "e44qgfAj3Ac7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # evaluation\n",
        "# import numpy as np\n",
        "# from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
        "\n",
        "# print('MAE:', mean_absolute_error(test['Passengers'], predictions))\n",
        "# print('MSE:', mean_squared_error(test['Passengers'], predictions))\n",
        "# print('RMSE:', np.sqrt(mean_squared_error(test['Passengers'], predictions)))\n",
        "# print(test.describe())"
      ],
      "metadata": {
        "id": "NxsAYiax2J2W"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ACF\n",
        "\n",
        "https://medium.com/@krzysztofdrelczuk/acf-autocorrelation-function-simple-explanation-with-python-example-492484c32711\n",
        "\n",
        "* Positive Correlation: as x goes up, y goes up\n",
        "* Negative Correlation: as x goes up,\n",
        "y goes down\n",
        "* AutoCorrelation: degree of correlation between successive time intervals (correlation with itself)\n",
        "* Lag: measured by k periods apart\n",
        "\n",
        "ACF stands for **Autocorrelation Function**. It's a tool used in time series analysis to measure the correlation between a time series and lagged versions of itself. In simpler terms, it helps you understand how a data point at a particular time is related to data points that came before it.\n",
        "\n",
        "**Here's a breakdown:**\n",
        "\n",
        "1. **Lag:**  A lag refers to a previous point in time. For example, a lag of 1 means looking at the data point one time unit before the current one, a lag of 2 means two time units before, and so on.\n",
        "\n",
        "2. **Correlation:**  Correlation measures the linear relationship between two variables. A positive correlation means they move in the same direction, a negative correlation means they move in opposite directions, and no correlation means there's no linear relationship.\n",
        "\n",
        "3. **Autocorrelation:**  Autocorrelation specifically measures the correlation between a time series and its lagged versions. It tells you how much a data point at a certain time is influenced by its past values.\n",
        "\n",
        "**How ACF Works**\n",
        "\n",
        "The ACF calculates the correlation coefficient for different lag values. It produces a plot (called a correlogram) that shows these correlation coefficients on the y-axis and the lag values on the x-axis.\n",
        "\n",
        "**Interpreting the ACF Plot**\n",
        "\n",
        "* **Significant correlations:**  Bars that extend beyond the confidence intervals (usually shown as dashed lines) indicate statistically significant correlations.\n",
        "* **Positive autocorrelation:**  Positive values suggest that the data points at a particular lag tend to move in the same direction as the current data point.\n",
        "* **Negative autocorrelation:** Negative values suggest that the data points at a particular lag tend to move in the opposite direction as the current data point.\n",
        "* **Gradual decline:**  A gradual decline in the ACF suggests a trend in the data.\n",
        "* **Sharp drops:**  Sudden drops in the ACF might indicate a seasonal pattern.\n",
        "\n",
        "**Why is ACF Important?**\n",
        "\n",
        "* **Identifying patterns:**  ACF helps identify trends, seasonality, and other patterns in time series data.\n",
        "* **Model selection:** It helps choose the appropriate model for your time series data (e.g., ARIMA models).\n",
        "* **Checking for randomness:**  If the ACF shows no significant correlations, it suggests that the data might be random (white noise).\n",
        "\n",
        "**Analogy:**\n",
        "\n",
        "Imagine you're tracking the daily temperature. If the temperature today is high, there's a good chance it will be high tomorrow as well (positive autocorrelation). The ACF would help you quantify this relationship and understand how the temperature on previous days influences the temperature today."
      ],
      "metadata": {
        "id": "gv6xUO5eae8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from statsmodels.graphics.tsaplots import plot_acf\n",
        "\n",
        "# title = 'Autocorrelation: starliner Passengers'\n",
        "# lags = 40\n",
        "# plot_acf(starliner['Passengers'], title=title, lags=lags);"
      ],
      "metadata": {
        "id": "HBpWZi-WpWv7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Lagged x units (shifts) define the correlation with the original time\n",
        "* This plot indicates non-stationary data, as there are a large number of lags before ACF values drop off\n",
        "* Gradual decline shows bigger shifts have less correlation\n",
        "* Shifts that form peaks express an increase in correlation and is related to seasonality\n",
        "* Describes autocorrelation between two observations over time"
      ],
      "metadata": {
        "id": "QECiskUZqBc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # acf plots default lags = 40\n",
        "# from statsmodels.graphics.tsaplots import plot_acf\n",
        "\n",
        "# title = 'Autocorrelation: Daily Female Births'\n",
        "# lags = 40\n",
        "# plot_acf(births, title=title, lags=lags);"
      ],
      "metadata": {
        "id": "lLQuMH6toeBu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This ACF plot shows stationary data, with lags on the horizontal axis and correlations on the vertical axis. The first value $y_0$ is always 1. A sharp dropoff indicates AR"
      ],
      "metadata": {
        "id": "CI5zQZxupw_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lag Plot"
      ],
      "metadata": {
        "id": "CzZbQfaUaWgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lag Plot**\n",
        "\n",
        "* **What it is:**  A scatter plot that compares a time series against a lagged version of itself.  \n",
        "* **How it works:** It plots the values of the time series on the y-axis against the values of the time series at a previous time (lag) on the x-axis.\n",
        "* **What it shows:**  Helps visualize the relationship between observations and their past values. You can create lag plots for different lags (e.g., lag 1, lag 2, etc.) to see how the relationships change over time.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "If you have daily stock prices, a lag plot with a lag of 1 would plot today's price against yesterday's price. A point in the upper-right quadrant would mean both today and yesterday had high prices.\n",
        "\n",
        "**Relationship to ACF**\n",
        "\n",
        "* **Visual vs. Numerical:** A lag plot provides a *visual* representation of the relationship between data points at different lags, while the ACF provides a *numerical* measure (the correlation coefficient) of that relationship.\n",
        "* **Patterns:**  Both can help identify patterns like:\n",
        "    * **Strong positive correlation:** Points clustered along a rising diagonal in a lag plot often correspond to strong positive bars in the ACF.\n",
        "    * **Strong negative correlation:** Points clustered along a falling diagonal in a lag plot often correspond to strong negative bars in the ACF.\n",
        "    * **No correlation:** A random scatter of points in a lag plot suggests no correlation, which would be reflected by small bars in the ACF.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "Lag plots and ACF plots are complementary tools.\n",
        "\n",
        "* **Start with lag plots:**  They give you a quick visual impression of potential correlations in your time series.\n",
        "* **Then use ACF:** To quantify those correlations and get a more precise understanding of the relationships at different lags.\n",
        "\n",
        "\n",
        "Think of it this way:\n",
        "\n",
        "* **Lag plot:**  A picture of the relationship.\n",
        "* **ACF:** A number that describes the strength of the relationship in the picture.\n"
      ],
      "metadata": {
        "id": "IWAz16qqaIGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pandas.plotting import lag_plot\n",
        "\n",
        "# lag_plot(starliner['Passengers']);"
      ],
      "metadata": {
        "id": "FTCClxVGoTTM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lag_plot(births['Births']);"
      ],
      "metadata": {
        "id": "GxMeb7mdoVd1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PACF\n",
        "\n",
        "* Partial AutoCorrelation: provides the partial correlation of a stationary time series with its own lagged values, regresses the values of the time series at all shorter lags\n",
        "* Helps visualize residuals over time vs real values\n",
        "* It contrasts with the autocorrelation function, which does not control for other lags\n",
        "* Describes relationship between observation and its lag\n",
        "* Statsmodel.tsa.statespace.tools.diff: performs a differencing operation along the zero(th) axis"
      ],
      "metadata": {
        "id": "ESZFnRlAahLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PACF stands for **Partial Autocorrelation Function**. It's another important tool in time series analysis, and it's closely related to the ACF (Autocorrelation Function). However, there's a key difference:\n",
        "\n",
        "**PACF vs. ACF**\n",
        "\n",
        "* **ACF:** Measures the correlation between a time series and its lagged versions *without* considering the influence of intermediate lags.\n",
        "* **PACF:** Measures the correlation between a time series and its lagged versions *after* removing the effect of intermediate lags.\n",
        "\n",
        "**In simpler terms:**\n",
        "\n",
        "Imagine you're looking at the relationship between the temperature today and the temperature three days ago.\n",
        "\n",
        "* **ACF:**  Would simply measure the correlation between these two temperatures.\n",
        "* **PACF:**  Would measure the correlation between these two temperatures *but would remove the influence of the temperatures on the two days in between*.\n",
        "\n",
        "**Why is this \"removing the influence\" important?**\n",
        "\n",
        "Because sometimes, the correlation between two points in time might be due to their shared correlation with points in between.  The PACF helps you isolate the *direct* relationship between two points in time, excluding the influence of intermediate points.\n",
        "\n",
        "**How PACF Works**\n",
        "\n",
        "The PACF is calculated by fitting a series of autoregressive models (AR models) of increasing order. For each lag, it estimates the coefficient of that lag in the AR model, which represents the partial autocorrelation at that lag.\n",
        "\n",
        "**Interpreting the PACF Plot**\n",
        "\n",
        "The PACF plot is similar to the ACF plot:\n",
        "\n",
        "* **Significant correlations:** Bars that extend beyond the confidence intervals indicate statistically significant partial autocorrelations.\n",
        "* **Positive/Negative values:**  Indicate the direction of the direct relationship.\n",
        "* **Sharp drops:**  Sudden drops in the PACF can be helpful in identifying the order of an autoregressive (AR) model.\n",
        "\n",
        "**Why is PACF Important?**\n",
        "\n",
        "* **Identifying direct relationships:**  Helps understand the direct relationship between data points at different lags.\n",
        "* **Model selection:** Particularly useful for determining the order of an autoregressive (AR) model in ARIMA modeling.\n",
        "\n",
        "**Analogy:**\n",
        "\n",
        "Imagine you're studying the relationship between a grandfather and his grandson.\n",
        "\n",
        "* **ACF:** Would measure the overall correlation between them, which might be influenced by the father (the intermediate generation).\n",
        "* **PACF:** Would measure the correlation between the grandfather and grandson *after removing the influence of the father*, giving you a clearer picture of their direct relationship.\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "The PACF provides a more refined view of the relationships within a time series by isolating the direct correlations between points in time. This is crucial for building accurate time series models, especially ARIMA models.\n"
      ],
      "metadata": {
        "id": "XeiHfifSak3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from statsmodels.graphics.tsaplots import plot_pacf\n",
        "\n",
        "# title = 'Autocorrelation: Daily Female Births'\n",
        "# lags = 40\n",
        "# plot_pacf(births, title=title, lags=lags);"
      ],
      "metadata": {
        "id": "cF1yMe_XrhpG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partial autocorrelation works best with stationary data. To make starliner stationary we use the diff command."
      ],
      "metadata": {
        "id": "fEqe64BkvsVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from statsmodels.tsa.statespace.tools import diff\n",
        "\n",
        "# starliner['diff1'] = diff(starliner['Passengers'], k_diff=1)\n",
        "# starliner[['Passengers', 'diff1']].plot();"
      ],
      "metadata": {
        "id": "heyLmLZT_coQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # plot pacf with diff k = 1\n",
        "# title = 'PACF: starliner Passengers Diff k=1'\n",
        "# lags = 40\n",
        "# plot_pacf(starliner['diff1'].dropna(), title=title, lags=np.arange(lags));"
      ],
      "metadata": {
        "id": "kL-8Ce5FxF48"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Month and Quarter Plot\n",
        "\n",
        "**`month_plot`**\n",
        "\n",
        "* **Purpose:** To visualize monthly seasonality in time series data.\n",
        "* **How it works:** It groups your time series data by month and creates a boxplot for each month. This allows you to see the distribution of values (median, quartiles, outliers) for each month of the year.\n",
        "* **What it shows:**\n",
        "    * **Monthly patterns:**  Helps identify if there are consistent differences in the behavior of your time series across different months.\n",
        "    * **Outliers:**  Can reveal unusual values for specific months.\n",
        "    * **Seasonality:** If the boxplots show a clear pattern across the months (e.g., higher values in summer months for ice cream sales), it suggests seasonality.\n",
        "\n",
        "**`quarter_plot`**\n",
        "\n",
        "* **Purpose:**  Similar to `month_plot`, but it visualizes quarterly seasonality.\n",
        "* **How it works:** Groups your time series data by quarter and creates a boxplot for each quarter.\n",
        "* **What it shows:**\n",
        "    * **Quarterly patterns:**  Helps identify if there are consistent differences in the behavior of your time series across different quarters of the year.\n",
        "    * **Outliers:** Can reveal unusual values for specific quarters.\n",
        "    * **Seasonality:**  If the boxplots show a clear pattern across the quarters, it suggests quarterly seasonality.\n",
        "\n",
        "**Important Notes:**\n",
        "\n",
        "* **Data requirements:** Your time series data needs to have a datetime index (e.g., a `pandas` `DatetimeIndex`) with a monthly frequency for these plots to work correctly.\n",
        "* **Aggregation:** If you have daily or weekly data, you might need to aggregate it to monthly or quarterly averages before using these plots.\n",
        "* **Interpretation:** These plots provide a visual indication of potential seasonality. You can combine them with other tools like ACF and PACF to confirm and further analyze the seasonality.\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "`month_plot` and `quarter_plot` are quick and easy ways to visually explore potential monthly or quarterly seasonality in your time series data. They provide valuable insights into how your data behaves across different time periods within a year.\n"
      ],
      "metadata": {
        "id": "_VCTytflbB9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # monthly\n",
        "# from statsmodels.graphics.tsaplots import month_plot\n",
        "\n",
        "# # starliner.index.freq = 'D'\n",
        "# month_plot(starliner['Passengers']);"
      ],
      "metadata": {
        "id": "8kxQ_tnvSJyB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # quarterly\n",
        "# from statsmodels.graphics.tsaplots import quarter_plot\n",
        "\n",
        "# starlinerq = starliner.resample(rule='Q').mean()\n",
        "# quarter_plot(starlinerq['Passengers']);"
      ],
      "metadata": {
        "id": "wtC3YWorFBks"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ARIMA\n",
        "\n",
        "* Auto Regressive Integrated Moving Average\n",
        "* Auto Regressive (p)\n",
        "* Moving Average (q)\n",
        "* Integrated (d - degree of differencing)\n",
        "* Orderings\n",
        "* Works well with univariate data, not so much with feature rich data (like stocks)\n",
        "\n",
        "AR, MA, ARMA, and ARIMA\n",
        "\n",
        "* PACF helps with AR model\n",
        "* Sharp drop off suggests AR(-k) model\n",
        "* ACF helps with MA model\n",
        "* Gradual decline suggests MA model"
      ],
      "metadata": {
        "id": "VtVy4cSEbMvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ARIMA stands for **Autoregressive Integrated Moving Average**. It's a powerful statistical model used for analyzing and forecasting time series data.\n",
        "\n",
        "Here's a breakdown of what each part means:\n",
        "\n",
        "* **AR (Autoregressive):**  This part of the model uses past values of the time series to predict future values. It's like saying, \"Today's value depends on what happened yesterday (or the day before, etc.).\" The 'p' in ARIMA(p,d,q) represents the order of the autoregressive part, meaning how many past values are considered.\n",
        "* **I (Integrated):** This part deals with making the time series stationary.  Stationarity means that the statistical properties of the series (like mean and variance) don't change over time. This is often achieved by differencing the data (taking the difference between consecutive observations). The 'd' in ARIMA(p,d,q) represents the degree of differencing.\n",
        "* **MA (Moving Average):** This part of the model uses past forecast errors to predict future values. It's like saying, \"If we overpredicted yesterday, we should adjust our prediction for today.\" The 'q' in ARIMA(p,d,q) represents the order of the moving average part.\n",
        "\n",
        "**In simpler terms:**\n",
        "\n",
        "Imagine you're trying to predict the daily temperature.\n",
        "\n",
        "* **AR:** You might consider the temperatures of the past few days.\n",
        "* **I:** You might look at the difference in temperature between yesterday and today, rather than the actual temperatures.\n",
        "* **MA:** You might consider how much you over- or under-predicted the temperature yesterday and adjust your prediction for today accordingly.\n",
        "\n",
        "**Why is ARIMA useful?**\n",
        "\n",
        "* **Versatile:** It can handle a wide range of time series patterns, including trends, seasonality, and combinations of these.\n",
        "* **Forecasting:**  It's a popular method for making short-term forecasts.\n",
        "* **Well-established:** It has a strong theoretical foundation and has been widely used in various fields.\n",
        "\n",
        "**How to use ARIMA:**\n",
        "\n",
        "1. **Identify stationarity:** Check if your time series is stationary (using plots, tests like the Augmented Dickey-Fuller test).\n",
        "2. **Differencing:** If it's not stationary, apply differencing to make it stationary.\n",
        "3. **Identify p, d, and q:**  Use ACF and PACF plots to help determine the appropriate values for p, d, and q.\n",
        "4. **Estimate parameters:**  Fit the ARIMA model to your data to estimate the model parameters.\n",
        "5. **Evaluate the model:** Check the model's fit and make adjustments if needed.\n",
        "6. **Forecast:** Use the fitted model to make forecasts.\n",
        "\n",
        "**ARIMA is a powerful tool, but it does have some limitations:**\n",
        "\n",
        "* **Linearity:**  It assumes a linear relationship between past and future values.\n",
        "* **Stationarity:** Requires the time series to be stationary or transformed to become stationary.\n",
        "* **Model selection:**  Choosing the right values for p, d, and q can be challenging.\n"
      ],
      "metadata": {
        "id": "EqSY06ihbNW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pmdarima"
      ],
      "metadata": {
        "id": "FHUiUzvCIork"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pmdarima import auto_arima\n",
        "\n",
        "# help(auto_arima)"
      ],
      "metadata": {
        "id": "Aic2-Kn_IbI8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# auto_arima(births['Births'],\n",
        "#            error_action='ignore',\n",
        "#            seasonal=False,\n",
        "#            stationary=True).summary()"
      ],
      "metadata": {
        "id": "I0lDoh2EI1B5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # train and forecast\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "# train = births.iloc[:90]\n",
        "# test = births.iloc[90:]\n",
        "\n",
        "# model = ARIMA(train['Births'],order=(1,0,1))\n",
        "# results = model.fit()\n",
        "# results.summary()"
      ],
      "metadata": {
        "id": "1xO4Zz12NTiF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # make predictions\n",
        "# predictions = results.predict(start=len(train), end=len(births)-1)\n",
        "# test['Births'].plot(legend=True,figsize=(12,5),label='test').autoscale(axis='x',tight=True)\n",
        "# predictions.plot(legend=True, label='predictions');"
      ],
      "metadata": {
        "id": "6nUHebVsR6Oo"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This makes sense because our dataset has no trend or seasonal component"
      ],
      "metadata": {
        "id": "HoDv23K9S7yu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SARIMA(X)\n",
        "\n",
        "* ARIMA with Seasoning\n",
        "* Uses PDQ as well as pdq\n",
        "* X refers to the support of exogenous (independent) variables\n",
        "\n",
        "**SARIMA (Seasonal ARIMA)**\n",
        "\n",
        "* **What it is:**  An extension of ARIMA that explicitly includes seasonal components.\n",
        "* **Why it's needed:** ARIMA struggles with time series data that exhibits strong seasonal patterns (e.g., peaks and troughs that repeat at regular intervals like monthly or yearly).\n",
        "* **How it works:**  It adds three new sets of parameters (P, D, Q) to the ARIMA model to capture the seasonal autoregressive (SAR), seasonal integrated (I), and seasonal moving average (SMA) components. These components operate at the seasonal level (e.g., with a lag of 12 for monthly data).\n",
        "* **Parameters:** SARIMA(p, d, q)(P, D, Q)m\n",
        "    * `p, d, q`:  Non-seasonal ARIMA parameters (order of autoregression, differencing, and moving average).\n",
        "    * `P, D, Q`: Seasonal ARIMA parameters.\n",
        "    * `m`: The number of periods in each season (e.g., 12 for monthly data, 4 for quarterly data).\n",
        "* **Example:** SARIMA(1, 1, 1)(1, 1, 1)12 would model a time series with both non-seasonal and monthly seasonal patterns.\n",
        "\n",
        "**SARIMAX (Seasonal ARIMA with eXogenous regressors)**\n",
        "\n",
        "* **What it is:** An extension of SARIMA that allows you to include external variables (exogenous variables) in your model.\n",
        "* **Why it's needed:** Sometimes, factors outside of the time series itself can influence its behavior. SARIMAX lets you incorporate these factors.\n",
        "* **How it works:** It adds an 'X' component to SARIMA, which represents the exogenous variables. These variables can be anything that might affect your time series, such as economic indicators, weather patterns, or promotional events.\n",
        "* **Example:** If you're modeling sales data, you could include advertising spending or competitor prices as exogenous variables in a SARIMAX model.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "* **ARIMA:**  A basic model for time series with non-seasonal patterns.\n",
        "* **SARIMA:**  Extends ARIMA to handle time series with seasonal patterns.\n",
        "* **SARIMAX:** Extends SARIMA to include external variables that might influence the time series.\n",
        "\n",
        "**When to use which:**\n",
        "\n",
        "* **ARIMA:** When your time series doesn't have a clear seasonal pattern.\n",
        "* **SARIMA:** When your time series has a seasonal pattern.\n",
        "* **SARIMAX:** When your time series has a seasonal pattern and you have external variables that might influence it.\n",
        "\n",
        "**Key takeaway:**\n",
        "\n",
        "SARIMA and SARIMAX are more complex models than ARIMA, but they can be very powerful for analyzing and forecasting time series data with seasonal patterns and external influences. They provide a more comprehensive approach to understanding the dynamics of your time series.\n"
      ],
      "metadata": {
        "id": "PnfLukaubO1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mind your Ps and Qs"
      ],
      "metadata": {
        "id": "HJMH_5rxcTUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Non-Seasonal Parameters (`p`, `d`, `q`)**\n",
        "\n",
        "These parameters define the non-seasonal part of the ARIMA model, dealing with patterns that don't repeat at regular intervals.\n",
        "\n",
        "* **`p` (Order of Autoregression):**\n",
        "    * The number of past values used to predict the current value.\n",
        "    * Example: `p = 2` means the model uses the values from two time steps ago to predict the current value.\n",
        "    * **How to find it:** Look for significant lags in the PACF plot.\n",
        "\n",
        "* **`d` (Degree of Differencing):**\n",
        "    * The number of times the data needs to be differenced to become stationary.\n",
        "    * Differencing involves taking the difference between consecutive observations to remove trends and stabilize the mean.\n",
        "    * Example: `d = 1` means the model uses the first difference of the time series.\n",
        "    * **How to find it:** Use tests like the Augmented Dickey-Fuller (ADF) test and visual inspection of time series plots.\n",
        "\n",
        "* **`q` (Order of Moving Average):**\n",
        "    * The number of past forecast errors used to predict the current value.\n",
        "    * Example: `q = 1` means the model uses the forecast error from one time step ago to predict the current value.\n",
        "    * **How to find it:** Look for significant lags in the ACF plot.\n",
        "\n",
        "**2. Seasonal Parameters (`P`, `D`, `Q`)**\n",
        "\n",
        "These parameters are specific to SARIMA and SARIMAX models and define the seasonal part of the model, dealing with patterns that repeat at regular intervals.\n",
        "\n",
        "* **`P` (Seasonal Autoregressive Order):**\n",
        "    * The number of seasonal lags used in the model.\n",
        "    * Example: `P = 1` with `m = 12` (monthly data) means the model uses the value from 12 time steps ago (same month last year) to predict the current value.\n",
        "    * **How to find it:** Look for significant lags at multiples of the seasonal period in the PACF plot.\n",
        "\n",
        "* **`D` (Seasonal Differencing Order):**\n",
        "    * The number of times the data needs to be differenced at the seasonal level to become stationary.\n",
        "    * Example: `D = 1` with `m = 12` means taking the difference between the current value and the value from the same month in the previous year.\n",
        "    * **How to find it:** Use tests like the ADF test and visual inspection of time series plots, focusing on seasonal patterns.\n",
        "\n",
        "* **`Q` (Seasonal Moving Average Order):**\n",
        "    * The number of seasonal moving average terms used in the model.\n",
        "    * Example: `Q = 1` with `m = 12` means the model uses the forecast error from 12 time steps ago (same month last year) to predict the current value.\n",
        "    * **How to find it:** Look for significant lags at multiples of the seasonal period in the ACF plot.\n",
        "\n",
        "**3. `m` (Seasonal Period)**\n",
        "\n",
        "* This parameter defines the number of periods in each season.\n",
        "* Example:\n",
        "    * `m = 12` for monthly data (12 months in a year)\n",
        "    * `m = 4` for quarterly data (4 quarters in a year)\n",
        "\n",
        "**Finding the Right Values**\n",
        "\n",
        "Finding the optimal values for these parameters often involves a combination of:\n",
        "\n",
        "* **ACF and PACF plots:** To identify potential autocorrelations and partial autocorrelations.\n",
        "* **Information criteria:**  Like AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) to compare different models.\n",
        "* **Trial and error:**  Experimenting with different parameter combinations and evaluating the model's performance.\n",
        "\n",
        "It's important to note that finding the perfect values can be challenging and may require some iteration and fine-tuning.\n"
      ],
      "metadata": {
        "id": "wLvpXtOIcM9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# auto_arima(starliner['Passengers'],\n",
        "#                       m=12,\n",
        "#                       trace=True,\n",
        "#                       error_action='ignore',\n",
        "#                       seasonal=True, # default\n",
        "#                       suppress_warnings=True,\n",
        "#                       stationary=False).summary()"
      ],
      "metadata": {
        "id": "kSusw7RwKl6M"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stepwise = auto_arima(starliner['Passengers'],\n",
        "#                       m=12,\n",
        "#                       trace=True,\n",
        "#                       error_action='ignore',\n",
        "#                       seasonal=True, # default\n",
        "#                       suppress_warnings=True,\n",
        "#                       stepwise=True)\n",
        "\n",
        "# stepwise.summary()"
      ],
      "metadata": {
        "id": "ZhM-eu7CLRNT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # uses PDQ as well as pdq\n",
        "# import statsmodels.api as sm\n",
        "\n",
        "# train = starliner.iloc[:116]\n",
        "# test = starliner.iloc[116:]\n",
        "\n",
        "# model = sm.tsa.statespace.SARIMAX(train['Passengers'],\n",
        "#                                   order=(2,1,1),\n",
        "#                                   seasonal_order=(0,1,0,12))\n",
        "# results = model.fit()\n",
        "# results.summary()"
      ],
      "metadata": {
        "id": "HncsJ-aQN8Ij"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # make predictions\n",
        "# predictions = results.predict(start=len(train), end=len(starliner)-1)\n",
        "# test['Passengers'].plot(legend=True,figsize=(12,5),label='test').autoscale(axis='x',tight=True)\n",
        "# predictions.plot(legend=True, label='predictions');"
      ],
      "metadata": {
        "id": "iLm8RCoUTs41"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Topics\n",
        "\n",
        "* Vector AutoRegression (Moving Average) VAR(MA)\n",
        "* Prophet Library (Facebook)"
      ],
      "metadata": {
        "id": "iCXCXl1JEdPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tests\n",
        "\n",
        "* Dickey-Fuller for stationality\n",
        "* Granger Causality for one time series forecasting another\n",
        "* Seasonality"
      ],
      "metadata": {
        "id": "A5m9i_et0EaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Stationality Test with Dickey-Fuller\n",
        "# from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# dftest = adfuller(starliner['Passengers'],autolag='AIC')\n",
        "# dfout = pd.Series(dftest[0:4],index=['ADF test statistic','p-value','lags used','observations'])\n",
        "\n",
        "# for key, val in dftest[4].items():\n",
        "#     dfout[f'critical value ({key})']=val\n",
        "\n",
        "# print(dfout)"
      ],
      "metadata": {
        "id": "OonVokF7LkAV"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The p-value is 0.99, which shows the lack of evidence to reject the null hypothesis, we fail to reject the null hypothesis. Our dataset is not stationary."
      ],
      "metadata": {
        "id": "mBYwIT8vMnHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Stationality Test with Dickey-Fuller\n",
        "# from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# dftest = adfuller(births['Births'],autolag='AIC')\n",
        "# dfout = pd.Series(dftest[0:4],index=['ADF test statistic','p-value','lags used','observations'])\n",
        "\n",
        "# for key, val in dftest[4].items():\n",
        "#     dfout[f'critical value ({key})']=val\n",
        "\n",
        "# print(dfout)"
      ],
      "metadata": {
        "id": "sn3y5SkINILU"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The p-value is 0.000052, which shows strong evidence to reject the null hypothesis. Our dataset is stationary."
      ],
      "metadata": {
        "id": "36BLruXxNRyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import statsmodels.api as sm\n",
        "# from statsmodels.tsa.stattools import grangercausalitytests\n",
        "# import numpy as np\n",
        "\n",
        "# data = sm.datasets.macrodata.load_pandas()\n",
        "# data = data.data[['realgdp', 'realcons', 'realinv', 'realgovt']].pct_change().dropna()\n",
        "# print(sm.datasets.macrodata.NOTE)"
      ],
      "metadata": {
        "id": "oIvLuD40Okmk"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data.plot(figsize=(12,5));"
      ],
      "metadata": {
        "id": "WYAXwkLLQrOi"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from statsmodels.tsa.stattools import grangercausalitytests\n",
        "\n",
        "# grangercausalitytests(data[['realgdp','realgovt']],maxlag=3);"
      ],
      "metadata": {
        "id": "t7_VvMR2PgZm"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from statsmodels.tsa.stattools import grangercausalitytests\n",
        "\n",
        "# grangercausalitytests(data[['realinv','realcons']],maxlag=3);"
      ],
      "metadata": {
        "id": "j-W94_oHP68X"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finance Terms\n",
        "\n",
        "Notes from Jose Portilla's Python for Financial Analysis and Algorithmic Trading\n",
        "\n",
        "* Adjusted Close\n",
        "* Moving Average / Rolling Mean\n",
        "* Bollinger Bands\n",
        "* Cumulative Return\n",
        "* Daily Return\n",
        "* Cumulative Daily Return\n",
        "* Time Series\n",
        "* Error - Trends - Seasonality (ETS) Models\n",
        "* ETS Decomposition\n",
        "* Exponentially Weighted Moving Averages (EWMA)\n",
        "* Auto Regressive Integrated Moving Average (ARIMA)\n",
        "* ACF - Autocorrelation Model\n",
        "* PACF - Partial Autocorrelation Model\n",
        "* Volatility\n",
        "* Portfolio Allocation\n",
        "* Sharpe Ratio\n",
        "* Exchange Traded Funds (ETF)\n",
        "* Mutual Funds\n",
        "* Hedge Funds\n",
        "* High Frequency Trading\n",
        "* Selling Short and Long\n",
        "* Capital Asset Pricing Model\n",
        "* Stock Splits and Dividends\n",
        "* Survivorship Bias\n",
        "* Efficient Market Hypothesis\n",
        "* Pairs Trading"
      ],
      "metadata": {
        "id": "uYAvdd34cVsA"
      }
    }
  ]
}