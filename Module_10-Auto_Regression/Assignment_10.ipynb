{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMHk0vli4PXqbpn3lpXw1jq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC5502/blob/main/Module_10-Auto_Regression/Assignment_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 10 - ARIMA(p, d, q)\n",
        "\n",
        "Name"
      ],
      "metadata": {
        "id": "QxRZOiZClpCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment Instructions\n",
        "\n",
        "**Submission:** Replace the placeholder name with your **Full Name**. Submit the shared link to your completed assignment in Canvas.\n",
        "\n",
        "### **Goal and Task**\n",
        "\n",
        "The goal of this assignment is to conduct a complete **$\\text{ARIMA}(p, d, q)$ time series analysis** on simulated data, focusing specifically on how the **Integrated ($d$)** component is determined and managed.\n",
        "\n",
        "As you progress through the sections, your primary task is to:\n",
        "\n",
        "* **First** uncomment all Code Cells and run all Cells.\n",
        "* **Explain the Purpose of the Code:** After each code cell, clearly and concisely describe what the code is doing, *why* that step is necessary for $\\text{ARIMA}$ modeling, and how it relates to the current parameters being identified ($p$, $d$, or $q$).\n",
        "* **Justify the Decisions:** Based on the results of the plots and the $\\text{ADF}$ tests, justify the final choice for each parameter:\n",
        "    * **$d$ (Integrated):** Explain why the data required **detrending** ($d=0$) instead of **differencing** ($d>0$).\n",
        "    * **$p$ (Autoregressive) and $q$ (Moving Average):** State the final optimal order found by the $\\text{AIC}$ grid search.\n",
        "\n",
        "This assignment is designed to teach you not just how to run $\\text{ARIMA}$ code, but how to interpret the results and **diagnose issues** (like the White Noise outcome) to select the correct stationary process."
      ],
      "metadata": {
        "id": "6dB9anMBMwry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ARIMA\n",
        "\n",
        "* Auto Regressive Integrated Moving Average\n",
        "* Auto Regressive (p)\n",
        "* Moving Average (q)\n",
        "* Integrated (d - degree of differencing)\n",
        "* Orderings\n",
        "* Works well with univariate data, not so much with feature rich data (like stocks)\n",
        "\n",
        "AR, MA, ARMA, and ARIMA\n",
        "\n",
        "* PACF helps with AR model\n",
        "* Sharp drop off suggests AR(-k) model\n",
        "* ACF helps with MA model\n",
        "* Gradual decline suggests MA model"
      ],
      "metadata": {
        "id": "VtVy4cSEbMvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ARIMA stands for **Autoregressive Integrated Moving Average**. It's a powerful statistical model used for analyzing and forecasting time series data.\n",
        "\n",
        "* **AR (Autoregressive):**  This part of the model uses past values of the time series to predict future values. It's like saying, \"Today's value depends on what happened yesterday (or the day before, etc.).\" The 'p' in ARIMA(p,d,q) represents the order of the autoregressive part, meaning how many past values are considered.\n",
        "* **I (Integrated):** This part deals with making the time series stationary.  Stationarity means that the statistical properties of the series (like mean and variance) don't change over time. This is often achieved by differencing the data (taking the difference between consecutive observations). The 'd' in ARIMA(p,d,q) represents the degree of differencing.\n",
        "* **MA (Moving Average):** This part of the model uses past forecast errors to predict future values. It's like saying, \"If we overpredicted yesterday, we should adjust our prediction for today.\" The 'q' in ARIMA(p,d,q) represents the order of the moving average part.\n",
        "\n",
        "**Why is ARIMA useful?**\n",
        "\n",
        "* **Versatile:** It can handle a wide range of time series patterns, including trends, seasonality, and combinations of these.\n",
        "* **Forecasting:**  It's a popular method for making short-term forecasts.\n",
        "* **Well-established:** It has a strong theoretical foundation and has been widely used in various fields.\n",
        "\n",
        "**How to use ARIMA:**\n",
        "\n",
        "1. **Identify stationarity:** Check if your time series is stationary (using plots, tests like the Augmented Dickey-Fuller test).\n",
        "2. **Differencing:** If it's not stationary, apply differencing to make it stationary.\n",
        "3. **Identify p, d, and q:**  Use ACF and PACF plots to help determine the appropriate values for p, d, and q.\n",
        "4. **Estimate parameters:**  Fit the ARIMA model to your data to estimate the model parameters.\n",
        "5. **Evaluate the model:** Check the model's fit and make adjustments if needed.\n",
        "6. **Forecast:** Use the fitted model to make forecasts.\n",
        "\n",
        "**ARIMA is a powerful tool, but it does have some limitations:**\n",
        "\n",
        "* **Linearity:**  It assumes a linear relationship between past and future values.\n",
        "* **Stationarity:** Requires the time series to be stationary or transformed to become stationary.\n",
        "* **Model selection:**  Choosing the right values for p, d, and q can be challenging.\n"
      ],
      "metadata": {
        "id": "EqSY06ihbNW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import time\n",
        "# import random\n",
        "# from datetime import datetime, timedelta\n",
        "# from statsmodels.tsa.arima_process import arma_generate_sample\n",
        "\n",
        "# current_time_seed = int(time.time())\n",
        "# np.random.seed(current_time_seed)\n",
        "# random.seed(current_time_seed)\n",
        "# print(f\"Seed: {current_time_seed}\")\n",
        "\n",
        "# start_range_start = datetime(2020, 1, 1)\n",
        "# start_range_end = datetime(2040, 1, 1)\n",
        "# delta = start_range_end - start_range_start\n",
        "# random_seconds = np.random.randint(delta.total_seconds())\n",
        "# start_date = (start_range_start + timedelta(seconds=random_seconds)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
        "\n",
        "# min_years_days = 12 * 365.25\n",
        "# max_years_days = 15 * 365.25\n",
        "# random_days = np.random.uniform(min_years_days, max_years_days)\n",
        "# end_date = (start_date + timedelta(days=random_days)).replace(day=1)\n",
        "\n",
        "# date_rng = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
        "# n_points = len(date_rng)\n",
        "\n",
        "# p_order = np.random.randint(1, 4)\n",
        "# q_order = np.random.randint(1, 4)\n",
        "\n",
        "# ar_coeffs = np.random.uniform(-0.5, 0.5, size=p_order)\n",
        "# ar_params = np.insert(-ar_coeffs, 0, 1.0)\n",
        "\n",
        "# ma_coeffs = np.random.uniform(-0.5, 0.5, size=q_order)\n",
        "# ma_params = np.insert(ma_coeffs, 0, 1.0)\n",
        "\n",
        "# # print(f\"Simulating Data with ARIMA Order: ({p_order}, 0, {q_order})\")\n",
        "# # print(f\"AR Parameters (phi): {ar_coeffs.round(2)}\")\n",
        "# # print(f\"MA Parameters (theta): {ma_coeffs.round(2)}\")\n",
        "\n",
        "# scale=np.random.randint(15, 30)\n",
        "# arma_series = arma_generate_sample(\n",
        "#     ar_params,\n",
        "#     ma_params,\n",
        "#     n_points,\n",
        "#     scale=scale,\n",
        "#     distrvs=np.random.normal\n",
        "# )\n",
        "\n",
        "# trend_slope = np.random.uniform(10, 50)\n",
        "# trend_intercept = np.random.uniform(100, 500)\n",
        "# trend_component = trend_intercept + (trend_slope * np.arange(n_points))\n",
        "\n",
        "# df = pd.DataFrame({\n",
        "#     'Data': trend_component + arma_series\n",
        "# }, index=date_rng)\n",
        "\n",
        "# print(\"Simulated Data Created\")"
      ],
      "metadata": {
        "id": "xTG4JuSk_i4h"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain the previous code cell:"
      ],
      "metadata": {
        "id": "8i5K1GtY04_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # add some noise\n",
        "# base_data = df['Data']\n",
        "# n_points = len(date_rng) # Use the same length as the simulated series\n",
        "\n",
        "# external_noise_sd = random.randint(40, 70)\n",
        "# external_noise = np.random.normal(0, external_noise_sd, n_points)\n",
        "\n",
        "# data_with_noise = base_data + external_noise\n",
        "# data_with_noise = np.maximum(data_with_noise, 1)\n",
        "# final_data = np.round(data_with_noise).astype(int)\n",
        "\n",
        "# df['Data'] = final_data\n",
        "# print(\"\\nExternal noise added to the time series.\")\n",
        "# print(f\"Final data type: {df['Data'].dtype}\")\n",
        "\n",
        "# df['Data'].plot(figsize=(10, 6), title='Final Simulated Series with Trend, ARMA, and External Noise')"
      ],
      "metadata": {
        "id": "jxXONdW5AfSr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain the previous code cell:"
      ],
      "metadata": {
        "id": "e5t_Z9Bs1JeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# seasonal_decompose(df, model='multiplicative').plot();"
      ],
      "metadata": {
        "id": "hbfMY8XbX_Ph"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain the previous code cell:"
      ],
      "metadata": {
        "id": "_SH23T3G1MuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stationary Data\n",
        "\n",
        "The fundamental requirement for the AR (Autoregressive) and MA (Moving Average) components of the model to be valid is that the time series must be stationary. A stationary series is one whose statistical properties (like mean, variance, and autocorrelation) remain constant over time.\n",
        "* ARMA ($p, q$) models (which are $p$ lags of AR and $q$ lags of MA) require the data to be strictly stationary.\n",
        "* ARIMA ($p, d, q$) is a generalization of ARMA that explicitly includes a differencing step. The \"$I$\" stands for Integrated, and the parameter $d$ represents the number of times the series is differenced to make it stationary."
      ],
      "metadata": {
        "id": "RhpHxYtyajcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from statsmodels.tsa.statespace.tools import diff\n",
        "\n",
        "# df['diff1'] = diff(df['Data'], k_diff=1)\n",
        "# df[['Data', 'diff1']].plot();"
      ],
      "metadata": {
        "id": "8NL5Ux4Ko-bX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain the previous code cell:"
      ],
      "metadata": {
        "id": "xO1bFP6B1P5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # test for stationarity of the appropriate series in df\n",
        "# from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# # Drop the first row which contains NaN in 'diff1'\n",
        "# adf_test = adfuller(df['diff1'].dropna())\n",
        "# print('ADF Statistic: %f' % adf_test[0])\n",
        "# print('p-value: %f' % adf_test[1])"
      ],
      "metadata": {
        "id": "tUI4tHKIZYjp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain the previous code cell:"
      ],
      "metadata": {
        "id": "YGcz9xDX1SFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Augmented Dickey-Fuller (ADF) Test\n",
        "\n",
        "The ADF test is a statistical hypothesis test used to determine if a time series is **stationary**. A stationary series has statistical properties like mean and variance that don't change over time.\n",
        "\n",
        "* **Null Hypothesis ($H_0$):** The series **is non-stationary** (it has a unit root).\n",
        "* **Alternative Hypothesis ($H_a$):** The series **is stationary**.\n",
        "\n",
        "### Interpreting the Results\n",
        "\n",
        "The code runs the `adfuller` function from the `statsmodels` library and outputs two key values:\n",
        "\n",
        "1.  **ADF Statistic: $-4.808291$**\n",
        "    This is the test statistic. A more negative value strengthens the evidence against the null hypothesis, suggesting the series is stationary.\n",
        "2.  **p-value: $0.000052$**\n",
        "    This is the critical value for making the statistical decision.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "To conclude whether the series is stationary, you compare the **p-value** to a chosen significance level ($\\alpha$), typically $0.05$.\n",
        "\n",
        "* **Decision Rule:** If the **p-value $\\leq \\alpha$**, you **reject the null hypothesis** ($H_0$).\n",
        "\n",
        "Since the p-value of $0.000052$ is much smaller than $0.05$, you **reject the null hypothesis** of non-stationarity.\n",
        "\n",
        "**The conclusion is that the time series data for `births` is stationary.**\n",
        "\n",
        "Would you like to see an example of how a non-stationary series is made stationary?"
      ],
      "metadata": {
        "id": "3wv8kZ5daEZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding ARIMA Parameters"
      ],
      "metadata": {
        "id": "RA4rJiX9bWxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # finding ARIMA parameters\n",
        "# from statsmodels.graphics.tsaplots import plot_acf\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plot_acf(df['diff1'], lags=20)\n",
        "# plt.title('Autocorrelation Function')\n",
        "# plt.xlabel('Lag')\n",
        "# plt.ylabel('Correlation Coefficient')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "WQ-G2V5Qbb1N"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why the ACF is Flat\n",
        "\n",
        "A completely flat line in the **Autocorrelation Function (ACF)** plot for your differenced series, $\\text{df['diff1']}$, means you have successfully removed all linear dependency but have potentially gone too far.\n",
        "\n",
        "The flat ACF indicates that the differenced series, $\\text{df['diff1']}$, is behaving like **White Noise**.\n",
        "\n",
        "A White Noise series is characterized by:\n",
        "* A **zero mean**.\n",
        "* **Constant variance**.\n",
        "* **Zero autocorrelation** at all lags (except for lag 0, which is always 1).\n",
        "\n",
        "The ACF plot shows that the series is **stationary** and has **no memory**â€”meaning the value at any time $t$ is completely independent of its past values.\n",
        "\n",
        "## The Problem\n",
        "\n",
        "The goal of differencing is not just to create a stationary series, but to create a stationary series that retains some **meaningful structure** (i.e., some statistically significant autocorrelation) that can be modeled by the AR and MA components of an ARIMA model.\n",
        "\n",
        "If the series is pure White Noise, then:\n",
        "\n",
        "* **You can't fit an AR or MA model to it.** There's no dependency for the model to capture.\n",
        "* **The best forecast is simply the mean of the series** (which is zero if no constant term is used).\n",
        "\n",
        "This suggests you've **over-differenced** or the original data was so perfectly linear that differencing removed everything.\n",
        "\n",
        "This results in something like a perfect Random Walk process and the first differencing yielded White Noise, where $\\text{ARIMA}(0, 1, 0)$ might be a good model.\n",
        "\n",
        "But in our simulated data, we want to model the data without the trend."
      ],
      "metadata": {
        "id": "vpKbNYDvtnO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## On to Plan B\n",
        "\n",
        "**\"We want to model the data without the trend\"**\n",
        "\n",
        "1.  The fact that $\\text{first differencing}$ resulted in **White Noise** ($\\text{ARIMA}(0, 1, 0)$) strongly implies that our original data was well-modeled by a simple Random Walk.\n",
        "2.  However, we want to arrive at an **$\\text{ARIMA}(1, 0, 1)$** model, we need to go back and use **detrending** instead of differencing.\n",
        "\n",
        "By **detrending** the data (removing a deterministic linear slope), you expect to be left with the original stationary $\\text{ARMA}(1, 1)$ error structure, thus leading to our desired $\\text{ARIMA}(1, 0, 1)$ conclusion."
      ],
      "metadata": {
        "id": "2fy3kk9quCoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Detrending\n",
        "\n",
        "Choosing Between Differencing ($d=1$) and Detrending ($d=0$).\n",
        "\n",
        "Our initial differencing attempt ($\\mathbf{d=1}$) resulted in White Noise, suggesting we over-corrected or that the original trend was deterministic. We must now formally test the original data to confirm if a simple detrending approach is statistically valid.\n",
        "\n",
        "We use a specialized version of the Augmented Dickey-Fuller (ADF) test to see if the original $\\texttt{df['Data']}$ is Trend-Stationary. Trend-Stationarity implies that by simply removing a linear trend, the remaining signal becomes stationary.\n",
        "\n",
        "The key is the $\\texttt{regression='ct'}$ parameter. This tells the ADF test to include both a Constant (c) and a Trend (t) in its test equation.\n",
        "* Null Hypothesis ($H_0$): The series requires $\\mathbf{d>0}$ (i.e., it's a Random Walk with Drift or a more complex unit root process).\n",
        "* Alternative Hypothesis ($H_a$): The series is Trend-Stationary (meaning $\\mathbf{d=0}$ with a deterministic trend)."
      ],
      "metadata": {
        "id": "WMBYdptmLnEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Test the ORIGINAL data for Trend-Stationarity (to see if detrending is a good option)\n",
        "# from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# # 1. Run ADF test on the ORIGINAL non-stationary series (df['Data'])\n",
        "# #    Use regression='ct' (Constant and Trend) to test for trend-stationarity.\n",
        "# #    This is the proper statistical test for a detrended series.\n",
        "\n",
        "# adf_test_detrended = adfuller(\n",
        "#     df['Data'],\n",
        "#     regression='ct' # This is the crucial argument\n",
        "# )\n",
        "\n",
        "# print('--- ADF Test for Trend-Stationarity ---')\n",
        "# print(f\"ADF Statistic: {adf_test_detrended[0]:.6f}\")\n",
        "# print(f\"p-value: {adf_test_detrended[1]:.6f}\")\n",
        "# print(f\"Critical Values (1%): {adf_test_detrended[4]['1%']:.3f}\")\n",
        "# print(f\"Critical Values (5%): {adf_test_detrended[4]['5%']:.3f}\")\n",
        "# print(f\"Critical Values (10%): {adf_test_detrended[4]['10%']:.3f}\")"
      ],
      "metadata": {
        "id": "ojufX7dyuOJR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain the previous code cell:"
      ],
      "metadata": {
        "id": "MCPqeRy5xBKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detrend the Data"
      ],
      "metadata": {
        "id": "4V30zR3ILxMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # detrend df['Data']\n",
        "# import pandas as pd\n",
        "# import statsmodels.api as sm\n",
        "# import numpy as np\n",
        "\n",
        "# # Create a time index (t=1, 2, 3, ...) for the regression\n",
        "# time_index = np.arange(len(df['Data']))\n",
        "# X = sm.add_constant(time_index) # Add a constant term for the intercept\n",
        "\n",
        "# # Fit a linear regression model\n",
        "# model = sm.OLS(df['Data'], X)\n",
        "# results = model.fit()\n",
        "\n",
        "# # The 'detrended' series is the residual of the regression\n",
        "# df['detrended'] = results.resid"
      ],
      "metadata": {
        "id": "-oqmMCJLw7wq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain the previous code cell:"
      ],
      "metadata": {
        "id": "1WSdYpj11jnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ADF Detrended Data"
      ],
      "metadata": {
        "id": "73P8sFv3L5T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # test for stationarity of the appropriate series in df\n",
        "# from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# adf_test = adfuller(df['detrended'].dropna())\n",
        "# print('ADF Statistic: %f' % adf_test[0])\n",
        "# print('p-value: %f' % adf_test[1])"
      ],
      "metadata": {
        "id": "_UbK2P5R0UgB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain the previous code cell:"
      ],
      "metadata": {
        "id": "PCZ-1gxW1lTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot ACF"
      ],
      "metadata": {
        "id": "XB0hNXodL9rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from statsmodels.graphics.tsaplots import plot_acf\n",
        "\n",
        "# plot_acf(df['detrended'], lags=40)\n",
        "# plt.title('Autocorrelation Function')\n",
        "# plt.xlabel('Lag')\n",
        "# plt.ylabel('Correlation Coefficient')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "ojv3ZWIixZdn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain the previous code cell:"
      ],
      "metadata": {
        "id": "6j0czG4S1ni3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot PACF"
      ],
      "metadata": {
        "id": "2GdGUirVMBei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from statsmodels.graphics.tsaplots import plot_pacf\n",
        "\n",
        "# plot_pacf(df['detrended'], lags=40)\n",
        "# plt.title('Partial Autocorrelation Function')\n",
        "# plt.xlabel('Lag')\n",
        "# plt.ylabel('Correlation Coefficient')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "SB3xUyaUbjCI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain the previous code cell:"
      ],
      "metadata": {
        "id": "bNtvn5Ew1pf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find the Best ARIMA(p, d, q) Model"
      ],
      "metadata": {
        "id": "aOPOxaVmMGKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import statsmodels.api as sm\n",
        "# from statsmodels.tsa.arima.model import ARIMA\n",
        "# from itertools import product\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore') # Filter out convergence warnings\n",
        "\n",
        "# df = df.asfreq('MS')\n",
        "\n",
        "# time_index = np.arange(len(df['Data']))\n",
        "# X = sm.add_constant(time_index)\n",
        "# model_detrend = sm.OLS(df['Data'], X)\n",
        "# results_detrend = model_detrend.fit()\n",
        "\n",
        "# stationary_series = results_detrend.resid.rename('stationary_series')\n",
        "# stationary_series.index = df.index\n",
        "\n",
        "# train_size = int(len(stationary_series) * 0.8)\n",
        "# train = stationary_series[0:train_size]\n",
        "# test = stationary_series[train_size:]\n",
        "\n",
        "# p_range = range(0, 3)\n",
        "# q_range = range(0, 3)\n",
        "# d = 0\n",
        "\n",
        "# pdq_combinations = list(product(p_range, [d], q_range))\n",
        "# best_aic = float(\"inf\")\n",
        "# best_order = None\n",
        "\n",
        "# print('\\n' + '='*30)\n",
        "# print('Running AIC Grid Search on STATIONARY Residuals...')\n",
        "# print('='*30)\n",
        "\n",
        "# for order in pdq_combinations:\n",
        "#     try:\n",
        "#         # Fit the ARIMA model on the CORRECT stationary 'train' series\n",
        "#         model = ARIMA(train, order=order, trend='n') # trend='n' because mean is assumed zero\n",
        "#         model_fit = model.fit()\n",
        "\n",
        "#         if model_fit.aic < best_aic:\n",
        "#             best_aic = model_fit.aic\n",
        "#             best_order = order\n",
        "\n",
        "#         print(f\"ARIMA Order {order}: AIC {round(model_fit.aic, 3)}\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         # print(f\"ARIMA Order {order}: Failed with error {e}\")\n",
        "#         continue\n",
        "\n",
        "# print('\\n' + '---' * 10)\n",
        "# print(f\"Best ARIMA Order {best_order}: Lowest AIC {round(best_aic, 3)}\")\n",
        "# print(f\"Scale: {scale}; Noise: {external_noise_sd}; Seed: {current_time_seed}\")\n",
        "# print('---' * 10)"
      ],
      "metadata": {
        "id": "i4ZQ85wTCeow"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain the previous code cell:"
      ],
      "metadata": {
        "id": "s_aU0rG7LV7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Test Best Model"
      ],
      "metadata": {
        "id": "E6VLnsPQMQxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "# import numpy as np\n",
        "\n",
        "# model_train = ARIMA(train, order=best_order)\n",
        "# model_train_fit = model_train.fit()\n",
        "# test_forecast = model_train_fit.get_forecast(steps=len(test))\n",
        "# test_forecast_series = pd.Series(test_forecast.predicted_mean, index=test.index)\n",
        "\n",
        "# plt.figure(figsize=(14,7))\n",
        "# plt.plot(train, label='Training Data')\n",
        "# plt.plot(test, label='Actual Data', color='orange')\n",
        "# plt.plot(test_forecast_series, label='Forecasted Data', color='green')\n",
        "# plt.fill_between(test.index,\n",
        "#                  test_forecast.conf_int().iloc[:, 0],\n",
        "#                  test_forecast.conf_int().iloc[:, 1],\n",
        "#                  color='k', alpha=.15)\n",
        "# plt.title('ARIMA Model Evaluation')\n",
        "# plt.xlabel('Date')\n",
        "# plt.ylabel('Number of Births')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# # Calculate Errors\n",
        "# mse = mean_squared_error(test, test_forecast_series)\n",
        "# rmse = mse**0.5\n",
        "# mae = mean_absolute_error(test, test_forecast_series)\n",
        "# mape = np.mean(np.abs((test.values - test_forecast_series.values) / test.values)) * 100\n",
        "\n",
        "# # Print the metrics\n",
        "# print('RMSE:', round(rmse, 2))\n",
        "# print('MAE:', round(mae, 2))\n",
        "# print('MAPE:', round(mape, 2), '%')\n",
        "\n",
        "# # AIC for model selection is already available in the model_train_fit object\n",
        "# print('Model AIC:', round(model_train_fit.aic, 2))"
      ],
      "metadata": {
        "id": "s-3bClt5huYK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain the previous code cell:"
      ],
      "metadata": {
        "id": "ZHmjhTsALXY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean Absolute Percentage Error (MAPE)**\n",
        "\n",
        "**MAPE** measures the accuracy as a percentage of the actual value. It's often preferred because it provides an easily interpretable, **scale-independent** measure of accuracy.\n",
        "\n",
        "$$\\text{MAPE} = \\frac{100}{n} \\sum_{t=1}^{n} \\left| \\frac{A_t - F_t}{A_t} \\right|$$\n",
        "\n",
        "Where $A_t$ is the actual value and $F_t$ is the forecasted value.\n",
        "\n",
        "A MAPE of 5% is clearly understood as a forecast error that is, on average, 5% of the true value, regardless of whether you are predicting births (small numbers) or national GDP (large numbers).\n",
        "\n",
        "**Mean Absolute Error (MAE)**\n",
        "\n",
        "**MAE** is the most intuitive measure of error, as it simply calculates the average magnitude of the errors. Like RMSE, it is **scale-dependent**, but unlike RMSE, it does not involve squaring the errors.\n",
        "\n",
        "$$\\text{MAE} = \\frac{1}{n} \\sum_{t=1}^{n} |A_t - F_t|$$\n",
        "\n",
        "MAE is **less sensitive to outliers** than RMSE. Because RMSE squares the errors, it disproportionately penalizes large prediction errors. If avoiding high penalties for rare, large errors is important, MAE is a better choice. It is also in the same units as your data (e.g., \"number of births\").\n",
        "\n",
        "**Akaike Information Criterion (AIC)**\n",
        "\n",
        "For **model selection** (choosing the best $p$, $d$, and $q$ for your ARIMA model), the **AIC** is the standard metric. You can't use AIC to compare forecasts to the test set, but you use it to find the *best model* to create those forecasts.\n",
        "\n",
        "AIC estimates the quality of your model relative to other models. It balances model fit (measured by the log-likelihood) against model complexity (the number of parameters).\n",
        "\n",
        "$$\\text{AIC} = -2 \\ln(L) + 2k$$\n",
        "\n",
        "Where $L$ is the maximized value of the likelihood function for the model, and $k$ is the number of estimated parameters.\n",
        "\n",
        "AIC automatically implements the principle of **parsimony** (simplicity). When comparing two models, the one with the **lower AIC** is preferred. It guides you to a model that is a good fit to the data without being overly complex (overfitting). You would use this metric when iterating through different $p$ and $q$ values (e.g., ARIMA(1, 0, 0) vs. ARIMA(2, 0, 0)).\n"
      ],
      "metadata": {
        "id": "6acle70ugHiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Summary"
      ],
      "metadata": {
        "id": "fc3-rdlfMZde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # model summary\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "# model = ARIMA(df['detrended'], order=(1,0,1))\n",
        "# results = model.fit()\n",
        "# results.summary()"
      ],
      "metadata": {
        "id": "1xO4Zz12NTiF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain the previous code cell:"
      ],
      "metadata": {
        "id": "l-h8W30GLZuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # predictions closeup\n",
        "# predictions = results.predict(start=len(train), end=len(df)-1)\n",
        "# test.plot(legend=True,figsize=(12,5),label='test').autoscale(axis='x',tight=True)\n",
        "# predictions.plot(legend=True, label='predictions');"
      ],
      "metadata": {
        "id": "6nUHebVsR6Oo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain the previous code cell:"
      ],
      "metadata": {
        "id": "8M0I7WohLaZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Purpose of $d$\n",
        "\n",
        "You would use the **$d$** parameter in $\\text{ARIMA}(p, d, q)$ when your time series data is **non-stationary** and requires **differencing** to achieve a stationary mean.\n",
        "\n",
        "The \"I\" in $\\text{ARIMA}$ stands for **Integrated**, and the parameter $d$ specifies the **order of differencing** applied to the series. The fundamental purpose is to remove trends that violate the stationarity assumption needed for the $\\text{AR}$ (Autoregressive) and $\\text{MA}$ (Moving Average) components of the model.\n",
        "\n",
        "A time series is **non-stationary** when its statistical properties, such as the mean and variance, change over time. The most common cause is a persistent trend.\n",
        "\n",
        "* **$d = 0$**: You use $d=0$ when the series is **already stationary**. In this case, the model reduces to an $\\text{ARMA}(p, q)$ model.\n",
        "* **$d = 1$**: You use $d=1$ when the series has a **linear trend** caused by a unit root (stochastic trend), like a Random Walk. This involves calculating the difference between consecutive observations ($Y_t - Y_{t-1}$).\n",
        "* **$d = 2$**: You use $d=2$ when the first difference is still non-stationary, indicating a **quadratic trend**. Using $d \\geq 2$ is rare and should be avoided if possible, as it can lead to **over-differencing**, which obscures the true signal.\n",
        "\n",
        "**How to Determine the Correct $d$**\n",
        "\n",
        "To correctly determine $d$, you must use a formal statistical test, typically the **Augmented Dickey-Fuller (ADF) test**.\n",
        "\n",
        "1.  **Start with $d=0$**: Assume the series is stationary.\n",
        "2.  **Run the ADF test**: The test's **null hypothesis ($H_0$) is that the series is non-stationary**.\n",
        "3.  **Iterate Differencing**:\n",
        "    * If the ADF test fails to reject $H_0$, the series is non-stationary. You then calculate the first difference ($d=1$) and re-run the ADF test on the newly differenced series.\n",
        "    * You continue increasing $d$ by one and re-testing until the ADF test **rejects $H_0$** (usually with a p-value less than 0.05), confirming that the series is stationary.\n",
        "\n",
        "You should always choose the **smallest integer $d$** that achieves stationarity."
      ],
      "metadata": {
        "id": "nzvMNxkHJika"
      }
    }
  ]
}