{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kkSXsFae8Cct"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC5502/blob/main/Module_14-Big_Data/Big_Data_and_Cloud_Computing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Big Data and Cloud Computing\n",
        "\n",
        "Your Name\n",
        "\n",
        "**Instructions**\n",
        "* Edit your name\n",
        "* Make a copy of this notebook and remove Copy of in the filename\n",
        "* Run all cells without errors\n",
        "* Answer all the questions asked of you\n",
        "* Submit the shared link to this notebook"
      ],
      "metadata": {
        "id": "LpgmefIa6fNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key Points and Concepts\n",
        "\n",
        "The following concepts and terms are taken from the following videos:\n",
        "\n",
        "* Pt I - https://www.youtube.com/watch?v=XUE5lx4Y96g\n",
        "* Pt II - https://www.youtube.com/watch?v=DEdoH8m-U8w"
      ],
      "metadata": {
        "id": "1-hp_QH47-GA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Big Data Foundations\n",
        "\n",
        "* **Big Data**\n",
        "* **“V” characteristics** (Volume, Variety, Veracity, Velocity, etc.)\n",
        "* **Data-driven computing**\n",
        "* **Data science as the overlap of ML, statistics, computing**\n",
        "* **Remote sensing data / hyperspectral and multispectral data**\n",
        "* **Point clouds and geographic-scale datasets**\n",
        "* **Scalability in data systems**\n",
        "* **Throughput vs. number-crunching computation**\n"
      ],
      "metadata": {
        "id": "V-Sy2who6kzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cloud Computing Fundamentals\n",
        "\n",
        "* **Cloud computing definition**\n",
        "* **Public, Private, Hybrid Clouds**\n",
        "* **Service delivery models**\n",
        "\n",
        "  * Infrastructure as a Service (IaaS)\n",
        "  * Platform as a Service (PaaS)\n",
        "  * Software as a Service (SaaS)\n",
        "  * “Everything as a Service” (XaaS)\n",
        "* **Virtualization**\n",
        "* **Virtual Machines (VMs)**\n",
        "* **Data center design**\n",
        "* **Elasticity and on-demand provisioning**\n",
        "* **Pay-per-use and subscription models**\n",
        "* **Resource sharing**\n",
        "* **Web-based computing interfaces (Jupyter, Colab)**\n",
        "* **Vendor lock-in**\n",
        "* **Cloud cost models**\n",
        "* **Data center cooling, racks, and physical layers** (tour context)"
      ],
      "metadata": {
        "id": "CyjeaAMKhicM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## High Performance Computing (HPC) & Parallelism\n",
        "\n",
        "* **Distributed computing**\n",
        "* **Parallel processing**\n",
        "* **Scalability across CPUs/GPUs**\n",
        "* **Cluster and supercomputer architectures**\n",
        "* **Interconnects and worker nodes**\n",
        "* **Exascale computing** (Jupiter supercomputer)\n",
        "* **HPC vs. Cloud Computing distinctions**\n",
        "* **Throughput-oriented computing (MapReduce)**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G3dZxcbThiqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning Foundations\n",
        "\n",
        "* **Artificial Intelligence (AI)**\n",
        "* **Machine Learning (ML)**\n",
        "* **Deep Learning (DL)**\n",
        "* **Perceptron** (binary and multiclass)\n",
        "* **Artificial Neural Networks (ANNs)**\n",
        "* **Feature engineering vs. feature learning**\n",
        "* **Model parameters vs. hyperparameters**\n",
        "* **Supervised vs. unsupervised learning** (implied through examples)\n",
        "* **Classic algorithms**\n",
        "\n",
        "  * Support Vector Machines (SVM)\n",
        "  * Random Forests\n",
        "* **ML tools**\n",
        "\n",
        "  * PyTorch\n",
        "  * TensorFlow\n",
        "  * scikit-learn\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BsGTgH4Uhi2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Learning Architectures\n",
        "\n",
        "* **Convolutional Neural Networks (CNNs)**\n",
        "* **Autoencoders**\n",
        "* **Sequence models** (LSTM, GRU)\n",
        "* **Large parameter models (billions of parameters)**\n",
        "* **Training time vs. model size trade-offs**\n",
        "* **GPU-accelerated training**\n",
        "* **Neural feature detectors** (edges, textures, objects)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PZuM8rJ5hjDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Large Language Models (LLMs) & Generative AI\n",
        "\n",
        "* **Large Language Models (LLMs)**\n",
        "* **Generative AI** (text, image, video, audio)\n",
        "* **Transformer architecture (implied by LLMs)**\n",
        "* **Foundation models**\n",
        "* **Parameter scale (billions → trillions)**\n",
        "* **Compute requirements for LLMs**\n",
        "* **European AI Factories**\n",
        "* **AI regulation: GDPR, EU AI Act**\n",
        "* **Public-sector AI adoption considerations**\n",
        "* **Privacy-sensitive contexts (e.g., healthcare)**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6qiPQmoLhjOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Big Data Ecosystems & Frameworks\n",
        "\n",
        "* **MapReduce paradigm** (map → reduce)\n",
        "* **Apache Hadoop**\n",
        "* **Apache Spark**\n",
        "* **MLlib** (Spark’s machine learning library)\n",
        "* **Graph computing**\n",
        "* **SQL-style interfaces in big data systems**\n",
        "* **Streaming data processing**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eE28uDEdhjY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cloud-Based ML & Applications\n",
        "\n",
        "* **Cloud ML services (AWS/GCP/Azure)**\n",
        "* **Recommendation engines**\n",
        "* **Shopping basket analysis / association rules**\n",
        "* **Data mining techniques**\n",
        "* **Real-time analytics**\n",
        "* **Scalable model training** (multi-GPU and multi-node training)\n",
        "* **Remote sensing ML applications**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hb0sURAihjj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Containers and Cloud-Native Tools\n",
        "\n",
        "* **Containerization**\n",
        "* **Docker**\n",
        "* **Docker Hub**\n",
        "* **Kubernetes (k8s)**\n",
        "* **Obtainer / Singularity (HPC-aligned containers)**\n",
        "* **Container orchestration**\n",
        "* **Container portability across cloud vendors**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e9vA10xrhjuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Optimization & AutoML\n",
        "\n",
        "* **Hyperparameters vs. trainable parameters**\n",
        "* **Learning rate, layers, neurons, optimizers**\n",
        "* **Search strategies for hyperparameters**\n",
        "* **Systematic tuning vs. human guesswork**\n",
        "* **AutoML frameworks** (e.g., AutoGluon)\n",
        "* **Ray Tune**\n",
        "* **Foundation model fine-tuning**\n",
        "* **Massive search across GPU clusters**"
      ],
      "metadata": {
        "id": "EJRzCAYBhj3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cloud Operating Systems & Deployment\n",
        "\n",
        "* **OpenStack**\n",
        "* **Cloud orchestration**\n",
        "* **Building an internal/private cloud**\n",
        "* **Lego-style modular services**\n",
        "* **Cloud provisioning workflows**\n",
        "* **In-house computing for sensitive data**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dyfx2x37hkBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Industry and Real-World Integration\n",
        "\n",
        "* **Industry projects in cloud/AI**\n",
        "* **Collaborations with companies**\n",
        "* **Scaling apps from small to global (e.g., Angry Birds)**\n",
        "* **Cloud-backed game architectures**\n",
        "* **Software engineering for cloud environments**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bmoz0iafhkKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Glossary"
      ],
      "metadata": {
        "id": "4k0AAwKc8DOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Artificial Intelligence (AI)**\n",
        "\n",
        "A broad field focused on creating systems that can perform tasks requiring human-like intelligence, such as reasoning, perception, and decision-making.\n",
        "\n",
        "**Machine Learning (ML)**\n",
        "\n",
        "A branch of AI that uses algorithms to learn patterns from data and make predictions or decisions without being explicitly programmed.\n",
        "\n",
        "**Deep Learning (DL)**\n",
        "\n",
        "A specialized area of ML that uses multi-layer neural networks to automatically learn complex representations from large datasets. Requires significant computing power.\n",
        "\n",
        "**Big Data**\n",
        "\n",
        "Extremely large datasets characterized by properties such as volume, variety, velocity, and veracity. Big data enables advanced analytics and machine learning but requires specialized tools and infrastructure.\n",
        "\n",
        "**Cloud Computing**\n",
        "\n",
        "A model that provides on-demand access to computing resources (servers, storage, databases, etc.) over the internet rather than on local hardware.\n",
        "\n",
        "**Scalability**\n",
        "\n",
        "The ability of a system to handle increasing amounts of work by adding resources. A core advantage of cloud computing.\n",
        "\n",
        "**Virtualization**\n",
        "\n",
        "A technique that allows multiple virtual machines to run on one physical machine, enabling efficient resource sharing in cloud data centers.\n",
        "\n",
        "**GPU (Graphics Processing Unit)**\n",
        "\n",
        "A highly parallel processor originally designed for graphics but now essential for ML, DL, and large-scale cloud workloads.\n",
        "\n",
        "**CPU (Central Processing Unit)**\n",
        "\n",
        "The general-purpose processor that handles most standard computing tasks but is less efficient for massively parallel workloads.\n",
        "\n",
        "**Infrastructure as a Service (IaaS)**\n",
        "\n",
        "Cloud service model that provides virtualized computing infrastructure such as machines, storage, and networks.\n",
        "\n",
        "**Platform as a Service (PaaS)**\n",
        "\n",
        "A cloud service that provides development tools, environments, and services for building, testing, and deploying applications.\n",
        "\n",
        "**Software as a Service (SaaS)**\n",
        "\n",
        "Cloud-delivered software that users access through a browser without managing the underlying infrastructure. Examples include Google Docs and Office 365.\n",
        "\n",
        "**MapReduce**\n",
        "\n",
        "A programming model for processing large datasets in parallel by splitting tasks into independent “map” tasks and combining results in a “reduce” step.\n",
        "\n",
        "**Apache Hadoop**\n",
        "\n",
        "An ecosystem for distributed storage (HDFS) and processing (MapReduce) of large-scale datasets across many machines.\n",
        "\n",
        "**Apache Spark**\n",
        "\n",
        "A unified analytics framework that improves on Hadoop by enabling faster, in-memory processing and providing ML, SQL, and streaming libraries.\n",
        "\n",
        "**Jupyter Notebook**\n",
        "\n",
        "A browser-based interactive computing tool used widely in cloud platforms to run code, visualize data, and build ML models.\n",
        "\n",
        "**Containerization**\n",
        "\n",
        "Packaging applications and their dependencies into isolated units (containers) that can run consistently across environments.\n",
        "\n",
        "**Docker**\n",
        "\n",
        "A popular container technology used to build, run, and share containerized applications.\n",
        "\n",
        "**Kubernetes**\n",
        "\n",
        "A system for automating deployment, scaling, and management of containerized applications across clusters of machines.\n",
        "\n",
        "**Obtainer (formerly Singularity)**\n",
        "\n",
        "A container system tailored for HPC environments where Docker is less suitable.\n",
        "\n",
        "**Hyperparameters**\n",
        "\n",
        "Settings that control the learning process of a model (e.g., learning rate, number of layers). They are not learned automatically and must be tuned.\n",
        "\n",
        "**Hyperparameter Optimization**\n",
        "\n",
        "A systematic process for finding the best hyperparameter combinations to maximize model performance, often requiring large-scale cloud or HPC resources.\n",
        "\n",
        "**AutoML (Automated Machine Learning)**\n",
        "\n",
        "Tools that automatically build, tune, and evaluate models—including hyperparameters, architectures, and preprocessing steps.\n",
        "\n",
        "**Foundation Models**\n",
        "\n",
        "Large pre-trained models (e.g., for text or tabular data) that can be fine-tuned for new tasks instead of training from scratch.\n",
        "\n",
        "**Large Language Models (LLMs)**\n",
        "\n",
        "A class of generative AI models trained on massive text datasets to understand and generate human-like language (e.g., ChatGPT).\n",
        "\n",
        "**Generative AI**\n",
        "\n",
        "AI that creates new data—such as text, images, audio, or video—based on patterns learned from training data.\n",
        "\n",
        "**Distributed Computing**\n",
        "\n",
        "A computing approach that uses multiple machines (nodes) working together to solve a problem faster or at larger scale.\n",
        "\n",
        "**Throughput Computing**\n",
        "\n",
        "A style of computing focused on processing many independent tasks quickly, often used in big data processing frameworks like Spark.\n",
        "\n",
        "**High Performance Computing (HPC)**\n",
        "\n",
        "Supercomputing resources designed for tightly coupled, massively parallel workloads such as simulations, physics, or climate modeling.\n",
        "\n",
        "**Data Mining**\n",
        "\n",
        "The process of discovering patterns in large datasets using techniques such as clustering, association rules, and classification.\n",
        "\n",
        "**Recommendation Systems**\n",
        "\n",
        "Machine learning systems that suggest relevant items to users (e.g., products, movies) based on patterns in historical data.\n",
        "\n",
        "**Remote Sensing**\n",
        "\n",
        "Collecting data about the Earth from satellites or aerial sensors. Often produces very large datasets used in big data and ML applications.\n",
        "\n",
        "**European AI Act**\n",
        "\n",
        "Regulation aimed at governing the use of AI within the EU, especially regarding data privacy, transparency, and high-risk systems.\n",
        "\n",
        "**Vendor Lock-In**\n",
        "\n",
        "A situation where an organization becomes dependent on one cloud provider’s services and cannot easily migrate elsewhere.\n",
        "\n",
        "**OpenStack**\n",
        "\n",
        "An open-source cloud operating system that allows organizations to build their own private cloud using modular components (like Lego bricks).\n"
      ],
      "metadata": {
        "id": "Jv3GuB-M64O3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32fjpkeS-nYP"
      },
      "source": [
        "#Getting Started with PySpark in Google Colab\n",
        "\n",
        "PySpark is Python interface for Apache Spark. The primary use cases for PySpark are to work with huge amounts of data and for creating data pipelines.\n",
        "\n",
        "You don't need to work with big data to benefit from PySpark. I find that the SparkSQL is a great tool for performing routine data anlysis. Pandas can get slow and you may find yourself writing a lot of code for data cleaning whereas the same actions take much less code in SQL. Let's get started!\n",
        "\n",
        "See more here! http://spark.apache.org/docs/latest/api/python/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are likely using **PySpark** because you have outgrown the capabilities of a single computer (and tools like pandas).\n",
        "\n",
        "To understand why, you need to understand the relationship between the tool you are typing into (PySpark) and the engine running underneath (Apache Spark).\n",
        "\n",
        "**1. What is Apache Spark? (The Engine)**\n",
        "\n",
        "Apache Spark is a **distributed computing system**. It is an engine designed to process massive amounts of data by splitting the work across many computers (a \"cluster\") instead of relying on just one.\n",
        "\n",
        "* **The Problem it Solves:** If you have a 500GB dataset but your laptop only has 16GB of RAM, you cannot open that file with standard tools. It will crash.\n",
        "* **The Spark Solution:** Spark breaks that 500GB file into small chunks, distributes them to 10 or 20 different computers, processes them in parallel, and gives you back the result.\n",
        "\n",
        "**2. What is PySpark? (The Interface)**\n",
        "\n",
        "Spark was originally written in a language called Scala (which runs on the Java Virtual Machine).\n",
        "\n",
        "**PySpark** is simply the **Python API** for Spark. It allows you to write standard Python code that \"talks\" to the Spark engine.\n",
        "\n",
        "* **You write:** Python code (which feels familiar).\n",
        "* **It executes:** Highly optimized distributed commands across a cluster (which is powerful).\n",
        "\n",
        "**The \"Kitchen\" Analogy**\n",
        "\n",
        "To visualize the difference between **Pandas** and **PySpark**:\n",
        "\n",
        "**Pandas = One Master Chef**\n",
        "\n",
        "* **Scenario:** You have one chef (your CPU) and one cutting board (your RAM).\n",
        "* **Pro:** The chef is incredibly fast and agile. For small meals (small data), this is the most efficient way.\n",
        "* **Con:** If you order a banquet for 5,000 people (Big Data), the chef runs out of space and time. The kitchen crashes.\n",
        "\n",
        "**PySpark = A Brigade of Chefs**\n",
        "\n",
        "* **Scenario:** You act as the **Head Chef** (The Driver). You shout instructions (\"Chop onions!\") to a team of 20 **Line Cooks** (The Executors/Worker Nodes).\n",
        "* **Pro:** You can handle massive banquets easily because 20 people are working at once.\n",
        "* **Con:** There is \"management overhead.\" It takes time to shout instructions and coordinate the team. For a single salad (small data), this approach is actually *slower* and more complicated than doing it yourself.\n",
        "\n",
        "**Why are you using it?**\n",
        "\n",
        "If your team or course has assigned you PySpark, it is usually for one of these three reasons:\n",
        "\n",
        "1.  **Volume (Big Data):** The data you are analyzing is too large to fit into the memory (RAM) of a single machine.\n",
        "2.  **Speed (Parallelism):** Even if the data *could* fit, processing it would take hours on one machine, whereas a cluster can do it in minutes.\n",
        "3.  **Future-Proofing:** You are building a pipeline that needs to scale. It might process 1GB today (which pandas could handle), but it will need to process 1TB next year.\n",
        "\n",
        "**Summary Table**\n",
        "\n",
        "| Feature | Pandas | PySpark |\n",
        "| :--- | :--- | :--- |\n",
        "| **Data Size** | Small to Medium (<10 GB) | Massive (Terabytes/Petabytes) |\n",
        "| **Processing** | Single Machine (1 CPU) | Distributed (Many CPUs/Nodes) |\n",
        "| **Execution** | **Eager** (Happens immediately) | **Lazy** (Waits until you ask for a result) |\n",
        "| **Best For** | Data Exploration, Cleaning | ETL Pipelines, Big Data Analytics |\n",
        "\n"
      ],
      "metadata": {
        "id": "UIr0J0WnHs78"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9yXqV3LigUA"
      },
      "source": [
        "## 1. PySpark in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- INSTALLATION SECTION ---\n",
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "Kmvcyr1RE3mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "EiYO5BgIFcEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONFIGURATION SECTION ---\n",
        "import sys\n",
        "import pyspark\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark = SparkSession \\\n",
        "       .builder \\\n",
        "       .master(\"local[*]\") \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "id": "BR36e5IQE62w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain what you just did here (do not delete this text or cell):"
      ],
      "metadata": {
        "id": "k87cR8rm9q_p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHvKMqLQ4ezk"
      },
      "source": [
        "## 2. Reading Data\n",
        "\n",
        "For this example, I am going to use a publicly available data set in a CSV format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzvNxiQSixRU"
      },
      "source": [
        "import requests\n",
        "path = \"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv\"\n",
        "req = requests.get(path)\n",
        "url_content = req.content\n",
        "\n",
        "csv_file_name = 'owid-covid-data.csv'\n",
        "csv_file = open(csv_file_name, 'wb')\n",
        "\n",
        "csv_file.write(url_content)\n",
        "csv_file.close()\n",
        "\n",
        "df = spark.read.csv('/content/'+csv_file_name, header=True, inferSchema=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYRUC46L_8zX"
      },
      "source": [
        "##3. PySpark DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-PgzP3IjZsV"
      },
      "source": [
        "#Viewing the dataframe schema\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "metadata": {
        "id": "5QEy1UqFF2tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option A: Get the clean string name\n",
        "print(dict(df.dtypes)[\"date\"])\n"
      ],
      "metadata": {
        "id": "8FEv12M0GRoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KKBv0ZCFbP5"
      },
      "source": [
        "#Converting a date column\n",
        "df.select(F.to_date(df.date).alias('date'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import month, year, dayofmonth\n",
        "\n",
        "# This works immediately because it IS a date\n",
        "df.select(\n",
        "    year(\"date\"),\n",
        "    month(\"date\"),\n",
        "    dayofmonth(\"date\")\n",
        ").show(5)"
      ],
      "metadata": {
        "id": "nUKUD9bsG9zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2ylA4B2kfd2"
      },
      "source": [
        "#Summary stats\n",
        "df.describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRX6qF_dEp9l"
      },
      "source": [
        "#DataFrame Filtering\n",
        "df.filter(df.location == \"United States\").orderBy(F.desc(\"date\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzGaFJ3QEG19"
      },
      "source": [
        "#Simple Group by Function\n",
        "df.groupBy(\"location\").sum(\"new_cases\").orderBy(F.desc(\"sum(new_cases)\")).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain what you just did here (do not delete this text or cell):"
      ],
      "metadata": {
        "id": "m9JrPemo-g2b"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt6e41cFEFSR"
      },
      "source": [
        "## 4. Spark SQL\n",
        "\n",
        "What I really like about the SQL module is that it's very approachable to interact with your data while still using Spark. There is less to learn since it's basically the same SQL syntax you might already be comfortable with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBpoPIGDrb-c"
      },
      "source": [
        "#Creating a table from the dataframe\n",
        "df.createOrReplaceTempView(\"covid_data\") #temporary view\n",
        "# df.saveAsTable(\"covid_data\") #Save as a table\n",
        "# df.write.mode(\"overwrite\").saveAsTable(\"covid_data\") #Save as table and overwrite table if exits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFcoi5l7kyLq"
      },
      "source": [
        "\n",
        "df2 = spark.sql(\"SELECT * from covid_data\")\n",
        "df2.printSchema()\n",
        "df2.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teHD2Up4k4Cd"
      },
      "source": [
        "groupDF = spark.sql(\"SELECT location, count(*) from covid_data group by location\")\n",
        "groupDF.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain what you just did here (do not delete this text or cell):"
      ],
      "metadata": {
        "id": "nw3ihIf6-lJY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTvI4jbZjX31"
      },
      "source": [
        "## 5. Example with Another Data Set\n",
        "This data set comes with your Google Colab Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-To1oW2S4mZL"
      },
      "source": [
        "df = spark.read.csv(\"/content/sample_data/california_housing_train.csv\", header=True, inferSchema=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvDrXp8w4pFi"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-ra4P9Z7sut"
      },
      "source": [
        "#print N rows\n",
        "df.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKzuJaaw79Kf"
      },
      "source": [
        "df.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvjbab-J7_t_"
      },
      "source": [
        "df.select(\"housing_median_age\",\"total_rooms\").show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAzEcJp78NIH"
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl85UrLC8PYW"
      },
      "source": [
        "df.select('total_rooms').distinct().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5tOUkPJ8XRb"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "test = df.groupBy('total_rooms').agg(F.sum('housing_median_age'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv6f3pGb_XR8"
      },
      "source": [
        "test.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaUs3Q-F8lA3"
      },
      "source": [
        "#Counting and removing missing values\n",
        "\n",
        "df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain what you just did here (do not delete this text or cell):"
      ],
      "metadata": {
        "id": "R6VSQQfE-oQy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbsONR7IRZnl"
      },
      "source": [
        "# Spark Tips and Tricks\n",
        "\n",
        "This is a collection of code snippets for common or tricky tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pandas DataFrame to Spark DataFrame"
      ],
      "metadata": {
        "id": "kkSXsFae8Cct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame(np.random.randint(100,size=(1000, 3)),columns=['A','B','C'])\n",
        "spark_df = spark.createDataFrame(df)\n",
        "spark_df.show()"
      ],
      "metadata": {
        "id": "858-wUBy8BhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert Object columns in pandas dataframe to a string\n",
        "for i in df.select_dtypes(include='object').columns.tolist():\n",
        "\tdf[i] = df[i].astype(str)\n",
        "\n",
        "#Convert datetimes to UTC\n",
        "for i in [col for col in df.columns if df[col].dtype == 'datetime64[ns]']:\n",
        "   df[i] = pd.to_datetime(df[i], utc=True)\n",
        "\n",
        "#Replace nan and \"None\" in pandas dataframe to null in the spark dataframe\n",
        "spark_df = spark.createDataFrame(df).replace('None', None).replace(float('nan'), None)"
      ],
      "metadata": {
        "id": "M3C2b3PX-7FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGa0hwxCRlEf"
      },
      "source": [
        "##Window Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIL2p4P3Z6bu"
      },
      "source": [
        "data = [\n",
        "        (1,'2021-01-01 10:00:00'),\n",
        "        (1,'2021-01-01 11:00:00'),\n",
        "        (1,'2021-01-01 12:00:00'),\n",
        "        (2,'2021-01-01 12:00:00'),\n",
        "        (2,'2021-01-01 13:00:00'),\n",
        "        (2,'2021-01-01 14:00:00'),\n",
        "]\n",
        "\n",
        "columns = [\"id\",\"datetime\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)\n",
        "df.createOrReplaceTempView(\"window_test\")\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6ZCcOiCZ74G"
      },
      "source": [
        "#Selecting the min and max by a specific Group\n",
        "spark.sql('''\n",
        "Select\n",
        "  id,\n",
        "\n",
        "  max(datetime) OVER (Partition BY id ORDER BY datetime) as max_date,\n",
        "  min(datetime) OVER (Partition BY id ORDER BY datetime) as min_date,\n",
        "\n",
        "  ROW_NUMBER() OVER (Partition BY id ORDER BY datetime) as row_number\n",
        "\n",
        "  FROM window_test\n",
        "\n",
        "''').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouYaOvMDcYYS"
      },
      "source": [
        "# Selecting the row number or order rank for each row within a specified grouping.\n",
        "# This is great for sub rankings in a table\n",
        "\n",
        "spark.sql('''\n",
        "Select\n",
        "  id,\n",
        "  datetime,\n",
        "\n",
        "  ROW_NUMBER() OVER (Partition BY id ORDER BY datetime) as row_number\n",
        "\n",
        "  FROM window_test\n",
        "\n",
        "''').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## De-duplicate data by returning the most recently updated row using a window function"
      ],
      "metadata": {
        "id": "eJgMqCiJGfZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "        (1,'2021-01-01',100,'A'),\n",
        "        (1,'2021-01-31',105,'A'),\n",
        "        (2,'2021-02-04',160,'B'),\n",
        "        (2,'2021-02-07',145,'B'),\n",
        "]\n",
        "\n",
        "columns = [\"id\",\"date\",\"score\",\"type\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)\n",
        "df.createOrReplaceTempView(\"window_test\")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "JBbA3LY3GwEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = spark.sql(\"\"\"\n",
        "WITH T AS (\n",
        "  SELECT\n",
        "  *,\n",
        "  ROW_NUMBER() OVER (PARTITION BY id ORDER BY date DESC) AS version_number\n",
        "  FROM window_test\n",
        ")\n",
        "\n",
        "SELECT * FROM T WHERE version_number = 1;\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "df2.show()"
      ],
      "metadata": {
        "id": "HwhLxFkibR0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "  SELECT\n",
        "  *,\n",
        "  SUM(score) OVER (PARTITION by type ORDER BY date) as score_cumulative\n",
        "  FROM window_test\n",
        "\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "TByf27YO6rdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limit the number of results per group window function"
      ],
      "metadata": {
        "id": "hDNlXaL0QEPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame(\n",
        "np.hstack((\n",
        "    np.random.randint(1,5,size=(100000, 1)),\n",
        "    np.random.randint(100,size=(100000, 1))\n",
        "))\n",
        ", columns=['company_id', 'number'])\n",
        "\n",
        "dff = spark.createDataFrame(df)\n",
        "dff.createOrReplaceTempView(\"window_test_limits\")\n"
      ],
      "metadata": {
        "id": "pSyMC_ypQIfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "WITH T AS (\n",
        "  SELECT\n",
        "    company_id,\n",
        "    number,\n",
        "    ROW_NUMBER() OVER (PARTITION BY company_id ORDER BY number) AS row_number\n",
        "  FROM window_test_limits\n",
        "    )\n",
        "\n",
        "SELECT * FROM T WHERE row_number <= 100\n",
        "\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "LLSEEW6VQNLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate a 7 day moving average"
      ],
      "metadata": {
        "id": "T0Ynq3qAVWs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(pd.date_range('1/1/2022','1/31/2022',freq='D'), columns=['date'])\n",
        "import random\n",
        "df['company_id'] = 1\n",
        "df['number'] = df.apply(lambda x: random.randint(0,100), axis = 1)\n",
        "\n",
        "dff = spark.createDataFrame(df)\n",
        "dff.createOrReplaceTempView(\"window_data\")\n",
        "\n",
        "dff.show()"
      ],
      "metadata": {
        "id": "e1Rkc0bCVaoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "  date,\n",
        "  company_id,\n",
        "  number,\n",
        "  AVG(number) OVER (PARTITION BY company_id ORDER BY date ASC RANGE BETWEEN INTERVAL 6 DAYS PRECEDING AND CURRENT ROW) as last_7_day_avg\n",
        "FROM window_data\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "atZLJhqIXYIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monthly Active Users"
      ],
      "metadata": {
        "id": "yjKv-xeZKOdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(pd.date_range('1/1/2022','1/31/2022',freq='D'), columns=['login_date'])\n",
        "import random\n",
        "df['company_id'] = 1\n",
        "df['user_id'] = df.apply(lambda x: random.randint(0,3), axis = 1)\n",
        "\n",
        "dff = spark.createDataFrame(df)\n",
        "dff.createOrReplaceTempView(\"users_data\")\n",
        "\n",
        "dff.show()"
      ],
      "metadata": {
        "id": "KP7bOaRnKZO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Revisit this transform\n",
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "  login_date,\n",
        "  COUNT(user_id) OVER (PARTITION BY login_date ORDER BY login_date ASC RANGE BETWEEN INTERVAL 30 DAYS PRECEDING AND CURRENT ROW) AS monthly_active_users\n",
        "  FROM users_data\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "rtaaCPuZKfTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find the time difference between related rows using a window function"
      ],
      "metadata": {
        "id": "BCmuBJ9rGwb-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYfSAEIKRcsQ"
      },
      "source": [
        "data = [\n",
        "        (1,'start','2021-01-01',100,'A'),\n",
        "        (1,'end','2021-01-31',200,'A'),\n",
        "        (2,'start','2021-03-05 4:53:11',100,'A'),\n",
        "        (2,'end','2021-05-01 05:06:38',200,'A'),\n",
        "]\n",
        "\n",
        "columns = [\"id\",\"session\",\"datetime\",\"station_return\",\"type\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)\n",
        "df.createOrReplaceTempView(\"window_test\")\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZWxv4Z4WXsE"
      },
      "source": [
        "spark.sql('''\n",
        "SELECT\n",
        "  id,\n",
        "  datetime,\n",
        "  lead(datetime) OVER (PARTITION BY id ORDER BY datetime) as next_datetime,\n",
        "  DATEDIFF(lead(datetime) OVER (PARTITION BY id ORDER BY datetime),datetime) as duration_in_days\n",
        "\n",
        "FROM window_test\n",
        "\n",
        "''').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unpivotting"
      ],
      "metadata": {
        "id": "7AYS0MlUvfrI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ--Ii0-bCPt"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "\n",
        "data = [\n",
        "        ('tim', 10, 9, 8, 5),\n",
        "        ('john', 5, 6, 3, 6),\n",
        "        ('jane', 7, 8, 9, 10),\n",
        "\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "   StructField(\"name\", StringType(), True),\n",
        "   StructField(\"experience\", IntegerType(), True),\n",
        "   StructField(\"satisfaction\", IntegerType(), True),\n",
        "   StructField(\"customer_service\", IntegerType(), True),\n",
        "   StructField(\"speed_of_service\", IntegerType(), True)])\n",
        "\n",
        "\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['experience', 'satisfaction', 'customer_service', 'speed_of_service']\n",
        "\n",
        "exprs = f\"\"\"stack({len(cols)}, {\", \".join([f\"'{i}',{i}\" for i in cols])}) as (question,score)\"\"\"\n",
        "\n",
        "unpivotted_df = df.select(\"name\",F.expr(exprs))\n",
        "\n",
        "unpivotted_df.show()"
      ],
      "metadata": {
        "id": "IjeFoWcF9Fqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replace Values using a Dictionary"
      ],
      "metadata": {
        "id": "WNaboQo22HOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = (spark\n",
        "    .createDataFrame([\n",
        "        (1, 'hello',3),\n",
        "        (2, 'hello',5),\n",
        "        (3, 'hello',5),\n",
        "        (135246, 'hello',4),\n",
        "        (54936, 'hello',4)\n",
        "        ],\n",
        "        [\"id\", \"text\",\"num\"]))"
      ],
      "metadata": {
        "id": "cYU3M9DnL1am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = {\n",
        "1: 5555,\n",
        "4:9999\n",
        "}"
      ],
      "metadata": {
        "id": "pFUiVV-V2XHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.replace(mapping, subset=['id']).replace(mapping, subset=['num']).show()"
      ],
      "metadata": {
        "id": "QsZ_Dx6D5Lxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create a Date Range"
      ],
      "metadata": {
        "id": "2fj4XecA5nX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "date_range_df = spark.sql(\"SELECT explode(sequence(to_date('2018-01-01'), to_date('2018-03-01'), interval 1 day)) as date\")\n",
        "date_range_df.show()"
      ],
      "metadata": {
        "id": "2R8ks2qR5Uh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Concat Row Values after Grouping"
      ],
      "metadata": {
        "id": "VxqgKv6w5x04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = (spark\n",
        "    .createDataFrame([\n",
        "        (1, 'hello',3),\n",
        "        (2, 'hello',5),\n",
        "        (3, 'hello',5),\n",
        "        (3, 'hello',5),\n",
        "        (3, 'hello',5),\n",
        "        ],\n",
        "        [\"id\", \"text\"]))\n",
        "\n",
        "df.createOrReplaceTempView(\"group_array\")\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "FlcyEw_j9o-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Return every element\n",
        "spark.sql(\"Select g.text, collect_list(g.id) FROM group_array as g GROUP BY 1\").show()"
      ],
      "metadata": {
        "id": "opBXrwRc-KVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Return unique list\n",
        "spark.sql(\"Select g.text, collect_set(g.id) FROM group_array as g GROUP BY 1\").show()"
      ],
      "metadata": {
        "id": "WurXJUP55r4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rename Spark Columns with a Dictionary"
      ],
      "metadata": {
        "id": "4PfBVvnxDhAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "data = [\n",
        "    (101, \"Alpha\"),\n",
        "    (102, \"Beta\"),\n",
        "    (103, \"Gamma\"),\n",
        "    (104, \"Delta\")\n",
        "]\n",
        "columns = [\"id\", \"test\"]\n",
        "\n",
        "ex = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"--- Original Schema ---\")\n",
        "ex.printSchema()\n",
        "\n",
        "col_dict = {\n",
        "    'id': 'ID',\n",
        "    'test': 'hello'\n",
        "}\n",
        "\n",
        "# Rename the columns\n",
        "for old_name, new_name in col_dict.items():\n",
        "    ex = ex.withColumnRenamed(old_name, new_name)\n",
        "\n",
        "# Create the SQL View\n",
        "ex.createOrReplaceTempView(\"test_view\") # I renamed this to \"test_view\" to avoid confusion\n",
        "\n",
        "print(\"\\n--- Renamed Schema ---\")\n",
        "ex.printSchema()\n",
        "\n",
        "# --- CORRECTION ON THE DISPLAY LOGIC ---\n",
        "\n",
        "# 1. To show the Python DataFrame variable:\n",
        "print(\"\\n--- DataFrame Output ---\")\n",
        "ex.show()\n",
        "\n",
        "# 2. To show the SQL View (You cannot run test.show(), you must use SQL):\n",
        "print(\"--- SQL View Output ---\")\n",
        "spark.sql(\"SELECT * FROM test_view\").show()"
      ],
      "metadata": {
        "id": "c5hKJL5Z9hIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "026bc5c8"
      },
      "source": [
        "# 1. Load the initial data\n",
        "initial_covid_df = spark.sql(\"SELECT * FROM covid_data\")\n",
        "\n",
        "initial_covid_df.show(5)\n",
        "\n",
        "# 2. Define the mapping dictionary\n",
        "col_dict = {\n",
        "    'iso_code': 'ISO_Code',\n",
        "    'location': 'Country_Region',\n",
        "    'total_cases': 'Total_Cases'\n",
        "}\n",
        "\n",
        "# 3. Rename using a loop\n",
        "# We initialize the new DF with the original DF\n",
        "df_renamed_covid = initial_covid_df\n",
        "\n",
        "for old_name, new_name in col_dict.items():\n",
        "    # PySpark DataFrames are immutable, so we must re-assign the result\n",
        "    df_renamed_covid = df_renamed_covid.withColumnRenamed(old_name, new_name)\n",
        "\n",
        "# 4. Create Temp View and Show\n",
        "df_renamed_covid.createOrReplaceTempView(\"covid_data_renamed\")\n",
        "df_renamed_covid.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split and get last element in Spark SQL"
      ],
      "metadata": {
        "id": "D6Syynf5QCxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note the 'r' BEFORE the triple quotes\n",
        "spark.sql(r\"\"\"\n",
        " SELECT\n",
        "  \"This.is.a.test\" AS text,\n",
        "  SPLIT(\"This.is.a.test\", '[\\.]') AS split,\n",
        "  REVERSE(SPLIT(\"This.is.a.test\", '[\\.]'))[0] AS last_word\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "nLV1bLK_QIuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Handling NULL Values"
      ],
      "metadata": {
        "id": "0k8qNqs2T2a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = (spark\n",
        "    .createDataFrame([\n",
        "        (1, 'hello',None),\n",
        "        (2, 'hello',None),\n",
        "        (3, 'hello',5),\n",
        "        (3, 'hello',5),\n",
        "        (3, 'hello',5),\n",
        "        ],\n",
        "        [\"id\", \"text\"]))\n",
        "\n",
        "df.createOrReplaceTempView(\"group_array\")\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "6zIJ3J1UQfpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"Select * from group_array where _3 IS NOT NULL\").show()"
      ],
      "metadata": {
        "id": "DXsG3qgkUfXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regex"
      ],
      "metadata": {
        "id": "zSv6sT7Rd2O5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "  '(5) Strongly Agree',\n",
        "  regexp_extract('(10) Strongly Agree', '([0-9]+)')\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "Qeln1haMd3By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain what RegEx is and why it is so important (do not delete this text or cell):"
      ],
      "metadata": {
        "id": "tN85Wk_1-_6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please provide a summary of your understanding of this assignment here (do not delete this text or cell):"
      ],
      "metadata": {
        "id": "W4TUTCwl_Ju4"
      }
    }
  ]
}