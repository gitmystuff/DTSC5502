{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC5502/blob/main/Module_07-Features/AB_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AB Testing\n",
        "\n",
        "Name"
      ],
      "metadata": {
        "id": "OfwTGduzoE6V"
      },
      "id": "OfwTGduzoE6V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started\n",
        "\n",
        "* Colab - get notebook from our gitmystuff repository\n",
        "* Save a Copy in Drive\n",
        "* Remove Copy of\n",
        "* Edit name\n",
        "* Take attendance\n",
        "* Clean up Colab Notebooks folder\n",
        "* Submit shared link\n"
      ],
      "metadata": {
        "id": "bGezCn18xkLd"
      },
      "id": "bGezCn18xkLd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Facts and Figures\n",
        "\n",
        "* https://ci.unt.edu/about-us/facts.html"
      ],
      "metadata": {
        "id": "4xrKhqag4Z6E"
      },
      "id": "4xrKhqag4Z6E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flash Cards\n",
        "\n",
        "* https://apps.ankiweb.net/"
      ],
      "metadata": {
        "id": "gXaRwEqF5ABH"
      },
      "id": "gXaRwEqF5ABH"
    },
    {
      "cell_type": "markdown",
      "id": "0ace9043",
      "metadata": {
        "id": "0ace9043"
      },
      "source": [
        "# RCTs and AB Testing\n",
        "\n",
        "Free AB Testing Course: https://www.udacity.com/course/ab-testing--ud257\n",
        "\n",
        "**Randomized Controlled Trials (RCTs)**\n",
        "\n",
        "* **Core Concept:**\n",
        "    * RCTs are considered the \"gold standard\" for determining the effectiveness of an intervention.\n",
        "    * Participants are randomly assigned to either a \"control\" group (which receives a standard treatment or a placebo) or an \"experimental\" group (which receives the new intervention).\n",
        "    * Randomization minimizes bias by ensuring that both groups are as similar as possible at the start of the experiment.\n",
        "    * Outcomes are then compared between the groups to determine if the intervention had a significant effect.\n",
        "* **Common Applications:**\n",
        "    * Medical research (testing new drugs or treatments)\n",
        "    * Social science research (evaluating the impact of social programs)\n",
        "    * Any field where it is important to establish a causal relationship.\n",
        "\n",
        "**A/B Testing**\n",
        "\n",
        "* **Core Concept:**\n",
        "    * A/B testing is a specific type of RCT commonly used in online environments.\n",
        "    * It involves comparing two versions of a webpage, app, or other digital product (version A and version B) to see which performs better.\n",
        "    * Users are randomly assigned to see either version A or version B.\n",
        "    * Metrics such as click-through rates, conversion rates, or user engagement are then compared to determine which version is more effective.\n",
        "* **Common Applications:**\n",
        "    * Website optimization\n",
        "    * Marketing campaigns\n",
        "    * App development\n",
        "    * User experience (UX) design\n",
        "\n",
        "**Similarities**\n",
        "\n",
        "* **Randomization:** Both RCTs and A/B testing rely on random assignment to minimize bias and ensure that groups are comparable.\n",
        "* **Control Groups:** Both involve a control group (or version A) that serves as a baseline for comparison.\n",
        "* **Causal Inference:** Both aim to establish a causal relationship between the intervention (or version B) and the observed outcomes.\n",
        "* **Statistical Analysis:** Both use statistical methods to analyze the data and determine if the observed differences are statistically significant.\n",
        "\n",
        "**Key Takeaways**\n",
        "\n",
        "* A/B testing is essentially a specific, practical application of the broader concept of randomized controlled trials, particularly in the digital realm.\n",
        "* They both provide a reliable way to get evidence based results.\n",
        "* The goal of both is to remove bias from testing, and provide accurate data to make informed decisions.\n",
        "\n",
        "https://vwo.com/ab-testing/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hypothesis Testing\n",
        "\n",
        "**Hypothesis Testing**\n",
        "\n",
        "Hypothesis testing is a statistical method used to determine if there is enough evidence in a sample of data to infer that a certain condition is true for the entire population.\n",
        "\n",
        "**Null and Alternative Hypotheses**\n",
        "\n",
        "* **Null Hypothesis (H0):** A statement of no effect or no difference. It's what you're trying to disprove.\n",
        "* **Alternative Hypothesis (Ha or H1):** A statement that contradicts the null hypothesis, proposing the effect or difference you're interested in.\n",
        "\n",
        "**Types of Tests**\n",
        "\n",
        "* **One-Tailed Test:** Used when you have a directional hypothesis (e.g., you expect one group to be greater than or less than the other).\n",
        "    * Symbols:  ≤ (less than or equal to) or ≥ (greater than or equal to) for the null hypothesis, and < (less than) or > (greater than) for the alternative hypothesis.\n",
        "* **Two-Tailed Test:** Used when you're looking for a difference in either direction (greater than or less than).\n",
        "    * Symbols: = (equals) for the null hypothesis, and ≠ (not equals) for the alternative hypothesis.\n",
        "\n",
        "**Examples**\n",
        "\n",
        "* **One-Tailed:**\n",
        "    * H0: The average weight of apples in Group A is greater than or equal to the average weight of apples in Group B (x̄1 ≥ x̄2).\n",
        "    * Ha: The average weight of apples in Group A is less than the average weight of apples in Group B (x̄1 < x̄2).\n",
        "* **Two-Tailed:**\n",
        "    * H0: The average height of men is equal to the average height of women (x̄1 = x̄2).\n",
        "    * Ha: The average height of men is not equal to the average height of women (x̄1 ≠ x̄2).\n",
        "\n",
        "**Important Notes**\n",
        "\n",
        "* **Decision:** In hypothesis testing, you either *reject* the null hypothesis (if there's enough evidence against it) or *fail to reject* it (if there's not enough evidence). You never \"accept\" the null hypothesis.\n",
        "* **Business vs. Academia:** In business contexts, like A/B testing, the focus is often on finding practical, meaningful differences that drive success. Academic research may explore more subtle differences that might not have immediate business implications.\n",
        "\n",
        "**Key takeaway:** Hypothesis testing helps you use data to make informed decisions about whether your observed results are likely due to chance or a real effect."
      ],
      "metadata": {
        "id": "cVLv0-UVCrT9"
      },
      "id": "cVLv0-UVCrT9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48242722"
      },
      "source": [
        "### Test of Means\n",
        "\n",
        "The test of means is a specific type of hypothesis test. It's used when you want to compare the means of two or more groups to see if there's a statistically significant difference between them.\n",
        "\n",
        "Here's a breakdown of how they connect:\n",
        "\n",
        "**Hypothesis Testing Framework:**\n",
        "\n",
        "* **Formulate Hypotheses:**\n",
        "    * Null hypothesis (H0): States that there's no difference between the means of the groups.\n",
        "    * Alternative hypothesis (Ha): States that there is a difference between the means.\n",
        "* **Collect Data:** Gather data from samples representing each group.\n",
        "* **Choose a Test:** Select an appropriate test of means based on the nature of your data and research question (e.g., t-test, ANOVA).\n",
        "* **Calculate Test Statistic:** Compute the test statistic, which measures the difference between the sample means relative to the variability within the groups.\n",
        "* **Determine P-value:** Calculate the probability of observing the obtained results (or more extreme results) if the null hypothesis were true.\n",
        "* **Make a Decision:**\n",
        "    * If the p-value is less than your significance level (alpha), reject the null hypothesis and conclude that there is a statistically significant difference between the means.\n",
        "    * If the p-value is greater than or equal to alpha, fail to reject the null hypothesis, indicating that there's not enough evidence to support a difference.\n",
        "\n",
        "**Types of Tests of Means:**\n",
        "\n",
        "* **t-test:** Used to compare the means of two groups.\n",
        "    * Independent samples t-test: For comparing means of two independent groups.\n",
        "    * Paired samples t-test: For comparing means of two related groups (e.g., before-and-after measurements).\n",
        "* **ANOVA (Analysis of Variance):** Used to compare the means of three or more groups.\n",
        "\n",
        "**Key Points:**\n",
        "\n",
        "* Tests of means are a subset of hypothesis tests, specifically designed for comparing means.\n",
        "* The choice of which test of means to use depends on factors like the number of groups being compared, the nature of the data, and the assumptions of the test.\n",
        "* Hypothesis testing provides the overall framework for conducting tests of means, guiding the decision-making process based on the p-value and significance level.\n",
        "\n",
        "In essence, tests of means are tools within the broader hypothesis testing framework, providing specific methods for comparing means and drawing conclusions about differences between groups.\n",
        "\n",
        "Many, if not most experiments are designed to compare means. The experiment may involve only one sample mean that is to be compared to a specific value. Or the experiment could be testing differences among many different experimental conditions, and the experimenter could be interested in comparing each mean with each of the other means.\n",
        "\n",
        "https://onlinestatbook.com/2/tests_of_means/testing_means.html\n",
        "\n",
        "### Assumptions\n",
        "\n",
        "* Groups are normally distributed (no significant outliers)\n",
        "* Groups are independent\n",
        "* Equal variance between groups\n",
        "* Rule of thumb: equal variances if the ratio is less than 4 (larger / smaller)\n",
        "* Scipy.ttest_ind equal_var: if True, perform a standard independent 2 sample t-test that assumes equal population variances. If False, perform Welch’s t-test, which does not assume equal population variances. This is True by default.\n",
        "* Scipy.ttest_ind alternative: two-sided, less, greater\n",
        "\n",
        "The loc and scale parameters let you adjust the location and scale of a distribution. For example, to model IQ data, you'd build iq = scipy.stats.norm(loc=100, scale=15) because IQs are constructed so as to have a mean of 100 and a standard deviation of 15. Why don't we just call them mean and sd? It helps to have a more generalized concept because not every distribution has a mean.\n",
        "\n",
        "https://stats.stackexchange.com/questions/560281/what-is-the-meaning-of-loc-and-scale-for-the-distributions-in-scipy-stats"
      ],
      "id": "48242722"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcfc11f4"
      },
      "source": [
        "### Margin Error\n",
        "\n",
        "In a normal distribution with a desired 95% confidence level, the margin of error is calculated as:\n",
        "\n",
        "**Margin of Error = Z * (σ / √n)**\n",
        "\n",
        "Where:\n",
        "\n",
        "* **Z** is the critical value for a 95% confidence level. For a normal distribution, this is approximately **1.96**.  (This value comes from the standard normal distribution table or can be calculated using statistical software).\n",
        "* **σ** is the population standard deviation. If you don't know the population standard deviation, you can estimate it with the sample standard deviation (s).\n",
        "* **n** is the sample size.\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* The margin of error tells you how much your sample estimate (like the sample mean) is likely to differ from the true population parameter (the population mean).\n",
        "* The Z value (1.96 for 95% confidence) reflects the number of standard deviations you need to go from the mean to capture the desired percentage of the distribution.\n",
        "* The standard error (σ / √n) measures the variability of the sample mean.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Let's say you want to estimate the average height of adults in a city with 95% confidence. You take a sample of 100 adults and find the sample mean height is 5'8\" with a standard deviation of 4 inches.\n",
        "\n",
        "* Z = 1.96\n",
        "* σ (estimated by s) = 4 inches\n",
        "* n = 100\n",
        "\n",
        "Margin of Error = 1.96 * (4 / √100) = 0.784 inches\n",
        "\n",
        "This means you can be 95% confident that the true average height of adults in the city is within 0.784 inches of your sample mean (5'8\").\n",
        "\n",
        "Standard error: The standard error measures the variability or spread of the sample mean. It tells you how much you can expect the sample mean to vary from sample to sample.\n",
        "\n",
        "**Important Note:** This formula assumes a normal distribution. If your data is not normally distributed, you may need to use a different critical value or a different method altogether.\n",
        "\n",
        "$\\bar{x} \\pm margin error$\n",
        "\n",
        "where the margin of error is a statistic expressing the amount of random sampling error in the results of a survey. The larger the margin of error, the less confidence one should have that a poll result would reflect the result of a census of the entire population.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Margin_of_error"
      ],
      "id": "fcfc11f4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0722134"
      },
      "source": [
        "### Confidence Intervals\n",
        "\n",
        "Confidence intervals are used to express how likely $\\bar{x}$ falls within a range of values. If the hypothesized value falls in the tail outside of the one-directional area of interest, we reject the null hypothesis. If our hypothesized value falls outside of the two-tailed interval, we reject the null hypothesis.\n",
        "\n",
        "In frequentist statistics, a confidence interval (CI) is a range of estimates for an unknown parameter. A confidence interval is computed at a designated confidence level. The 95% level is most common, but other levels (such as 90% or 99%) are sometimes used. The confidence level represents the long-run proportion of correspondingly computed intervals that end up containing the true value of the parameter. For example, out of all confidence intervals computed at the 95% level, 95% of them should contain the parameter's true value.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Confidence_interval"
      ],
      "id": "c0722134"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70bb5714"
      },
      "source": [
        "**Calculating Confidence Intervals for Proportions**\n",
        "\n",
        "**1. Estimate the Proportion (phat)**\n",
        "\n",
        "* `phat = x / n` where:\n",
        "    * x is the number of successes (e.g., users who clicked)\n",
        "    * n is the total number of trials (e.g., number of users)\n",
        "\n",
        "**2. Check for Normal Approximation**\n",
        "\n",
        "* If `N * phat > 5` and `N * (1 - phat) > 5`, you can use the normal distribution to approximate the binomial distribution.\n",
        "\n",
        "**3. Calculate the Margin of Error**\n",
        "\n",
        "* `Margin of Error = z * SE` where:\n",
        "    * z is the critical value from the standard normal distribution (e.g., 1.96 for 95% confidence)\n",
        "    * SE (Standard Error) = `sqrt(phat * (1 - phat) / n)`\n",
        "\n",
        "**4. Construct the Confidence Interval**\n",
        "\n",
        "* `Confidence Interval = phat ± Margin of Error`\n",
        "\n",
        "**Key Points**\n",
        "\n",
        "* **Factors Affecting Confidence Interval Width:**\n",
        "    * **Distance of phat from 0.5:** The closer phat is to 0.5, the wider the interval (more uncertainty).\n",
        "    * **Sample Size (n):** Larger sample sizes lead to narrower intervals (more precision).\n",
        "* **Z-Score:** The z-score corresponds to the desired confidence level. For a 95% confidence interval, the z-score is approximately 1.96. This means that 95% of the time, the true population proportion will fall within 1.96 standard errors of the sample proportion.\n",
        "* **Hypothesis Testing:** Confidence intervals are often used in hypothesis testing. If the confidence interval for the difference between two proportions (e.g., treatment and control groups) does not include 0, you can reject the null hypothesis that there is no difference.\n",
        "\n",
        "This summary provides a clear and concise guide to calculating and interpreting confidence intervals for proportions, which is essential for analyzing A/B testing data and making informed decisions."
      ],
      "id": "70bb5714"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67a4fbc4"
      },
      "source": [
        "### One Sided"
      ],
      "id": "67a4fbc4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb1d876b"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from scipy import stats\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# x = np.arange(-3, 3, 0.001)\n",
        "\n",
        "# pdf = stats.norm.pdf(x,loc=0,scale=1)\n",
        "# plt.plot(x, pdf)\n",
        "\n",
        "# z = 1.645 # 95% CI\n",
        "# uci = z\n",
        "\n",
        "# plt.fill_between(x, pdf, where=(x < uci), alpha=0.5)\n",
        "# plt.text(uci, 0, uci, ha='center', fontsize=20)\n",
        "# plt.text(-0.4, .2, f'95%', fontsize=20)\n",
        "# plt.text(-1.1, .15, f'fail to reject', fontsize=20)\n",
        "# plt.grid(True)"
      ],
      "id": "fb1d876b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e2677f6"
      },
      "source": [
        "Note: A null hypothesis is either true or false. Unfortunately, we do not know which is the case, and we almost never will. It is important to realize that there is no probability that the null hypothesis is true or that it is false, because there is no element of chance.\n",
        "\n",
        "http://strata.uga.edu/8370/lecturenotes/errors.html#:~:text=A%20null%20hypothesis%20is%20either,is%20no%20element%20of%20chance."
      ],
      "id": "8e2677f6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32620997"
      },
      "source": [
        "### Two Sided"
      ],
      "id": "32620997"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e0c531e"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from scipy import stats\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# x = np.arange(-3, 3, 0.001)\n",
        "\n",
        "# pdf = stats.norm.pdf(x,loc=0,scale=1)\n",
        "# plt.plot(x, pdf)\n",
        "\n",
        "# z = 1.96 # 95% CI\n",
        "# lci = -z\n",
        "# uci = z\n",
        "\n",
        "# plt.fill_between(x, pdf, where=(lci < x) & (x < uci), alpha=0.5)\n",
        "# plt.text(lci, 0, lci, ha='center', fontsize=20)\n",
        "# plt.text(uci, 0, uci, ha='center', fontsize=20)\n",
        "# plt.text(-0.4, .2, f'95%', fontsize=20)\n",
        "# plt.grid(True)"
      ],
      "id": "1e0c531e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90f9a38e"
      },
      "source": [
        "### Alpha (Significance Levels)\n",
        "\n",
        "The significance level or alpha level is the probability of making the wrong decision when the null hypothesis is true. Alpha levels (sometimes just called “significance levels”) are used in hypothesis tests. Usually, these tests are run with an alpha level of .05 (5%), but other levels commonly used are .01 and .10.\n",
        "\n",
        "https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/what-is-an-alpha-level/\n",
        "\n",
        "Stephanie Glen. \"Alpha Level (Significance Level): What is it?\" From StatisticsHowTo.com: Elementary Statistics for the rest of us! https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/what-is-an-alpha-level/"
      ],
      "id": "90f9a38e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "def212ba"
      },
      "source": [
        "### One Sided"
      ],
      "id": "def212ba"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43d59884"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from scipy import stats\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# x = np.arange(-3, 3, 0.001)\n",
        "\n",
        "# pdf = stats.norm.pdf(x,loc=0,scale=1)\n",
        "# plt.plot(x, pdf)\n",
        "\n",
        "# z = 1.645 # 95% CI\n",
        "# uci = z\n",
        "\n",
        "# plt.fill_between(x, pdf, where=(x > uci), alpha=0.5)\n",
        "# plt.text(uci, 0, uci, ha='center', fontsize=20)\n",
        "# plt.text(uci, .05, f'5%', fontsize=20)\n",
        "# plt.grid(True)"
      ],
      "id": "43d59884"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Region of Rejection\n",
        "\n",
        "The region of rejection, also known as the critical region, is a crucial concept in hypothesis testing. It's the range of values for the test statistic that would lead you to reject the null hypothesis.\n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "1. **Hypothesis Testing:**\n",
        "   - You start with a null hypothesis (H0), which is a statement of no effect or no difference.\n",
        "   - You also have an alternative hypothesis (Ha), which contradicts the null hypothesis.\n",
        "   - The goal of hypothesis testing is to determine if there's enough evidence to reject the null hypothesis in favor of the alternative hypothesis.\n",
        "\n",
        "2. **Test Statistic:**\n",
        "   - You calculate a test statistic from your sample data. This statistic measures how far your sample data deviates from what you'd expect if the null hypothesis were true.\n",
        "\n",
        "3. **Distribution of the Test Statistic:**\n",
        "   - The test statistic follows a specific probability distribution (e.g., t-distribution, chi-squared distribution) under the assumption that the null hypothesis is true.\n",
        "\n",
        "4. **Critical Value(s) and Significance Level:**\n",
        "   - You choose a significance level (alpha), typically 0.05. This represents the probability of rejecting the null hypothesis when it's actually true (Type I error).\n",
        "   - Based on the chosen alpha and the distribution of the test statistic, you determine the critical value(s). These values define the boundaries of the rejection region.\n",
        "\n",
        "5. **Rejection Region:**\n",
        "   - The rejection region is the area in the tails of the distribution beyond the critical value(s).\n",
        "   - If your calculated test statistic falls within the rejection region, you reject the null hypothesis.\n",
        "   - If the test statistic falls outside the rejection region, you fail to reject the null hypothesis.\n",
        "\n",
        "**Visual Example:**\n",
        "\n",
        "Imagine a normal distribution. For a two-tailed test with alpha = 0.05, the rejection region would be the 2.5% in each tail. The critical values would be the points on the x-axis that mark these boundaries (approximately ±1.96 standard deviations from the mean).\n",
        "\n",
        "**Key Points:**\n",
        "\n",
        "* The rejection region is determined by the chosen significance level (alpha) and the distribution of the test statistic.\n",
        "* If the test statistic falls in the rejection region, it suggests that the observed data is unlikely to have occurred if the null hypothesis were true, leading you to reject the null hypothesis.\n",
        "* The region of rejection helps make objective decisions in hypothesis testing by providing a clear criterion for rejecting or failing to reject the null hypothesis."
      ],
      "metadata": {
        "id": "AbzzH9n9OY48"
      },
      "id": "AbzzH9n9OY48"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "088c8c47"
      },
      "source": [
        "### Two Sided"
      ],
      "id": "088c8c47"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db89285e"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from scipy import stats\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# x = np.arange(-3, 3, 0.001)\n",
        "\n",
        "# pdf = stats.norm.pdf(x,loc=0,scale=1)\n",
        "# plt.plot(x, pdf)\n",
        "\n",
        "# z = 1.96 # 95% CI\n",
        "# lci = -z\n",
        "# uci = z\n",
        "\n",
        "# plt.fill_between(x, pdf, where=(lci > x) | (x > uci), alpha=0.5)\n",
        "# plt.text(lci, 0, lci, ha='center', fontsize=20)\n",
        "# plt.text(uci, 0, uci, ha='center', fontsize=20)\n",
        "# # plt.text(-0.4, .2, f'95%', fontsize=20)\n",
        "# plt.text(lci-.5, 0.05, f'2.5%', ha='center', fontsize=20)\n",
        "# plt.text(uci+.5, 0.05, f'2.5%', ha='center', fontsize=20)\n",
        "# plt.grid(True)"
      ],
      "id": "db89285e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f6b239c"
      },
      "source": [
        "### Test Statistic vs Critical Value\n",
        "\n",
        "In order to make a decision whether to reject the null hypothesis a test statistic is calculated. The decision is made on the basis of the numerical value of the test statistic. There are two approaches how to derive at that decision: The critical value approach and the p-value approach.\n",
        "\n",
        "https://www.geo.fu-berlin.de/en/v/soga/Basics-of-statistics/Hypothesis-Tests/Introduction-to-Hypothesis-Testing/Critical-Value-and-the-p-Value-Approach/index.html\n",
        "\n",
        "Critical value example:\n",
        "\n",
        "* 1.96 - if the test statistic is greater than the critical value we reject the null"
      ],
      "id": "8f6b239c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2767fc4b"
      },
      "source": [
        "### P-Value\n",
        "\n",
        "For a p value test:\n",
        "* Get the test statistic\n",
        "* Use it to determine the p-value\n",
        "* Compare the p-value to the level of significance\n",
        "* If the p-value is low the null must go! Reject $H_0$\n",
        "* If the p-value is high the null must fly! Fail to reject $H_0$\n",
        "\n",
        "\"The p value is the evidence against a null hypothesis. The smaller the p-value, the stronger the evidence that you should reject the null hypothesis.\"\n",
        "\n",
        "Stephanie Glen. \"P-Value in Statistical Hypothesis Tests: What is it?\" From StatisticsHowTo.com: Elementary Statistics for the rest of us! https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/p-value/\n",
        "\n",
        "The lower the p value, the more predictive the feature is in principle. When we run tests, we are often concerned with the alpha level to help us reject or fail to reject the null hypothesis. The alpha level is (1 - our confidence interval), so if we wanted to have a confidence level of 95% we would use a alpha value of 5%. If our p value is less than the alpha value then the evidence points to rejecting the null hypothesis. If our p values is less than the alpha value then we can say our results are statistcally significant. We found something that is probably not the result of chance. But beware."
      ],
      "id": "2767fc4b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7297ad82"
      },
      "source": [
        "### P Value vs Alpha\n",
        "\n",
        "Alpha, the significance level, is the probability that you will make the mistake of rejecting the null hypothesis when in fact it is true. The p-value measures the probability of getting a more extreme value than the one you got from the experiment. If the p-value is greater than alpha, we fail to reject the null hypothesis.\n",
        "\n",
        "https://www.spcforexcel.com/knowledge/basic-statistics/interpretation-alpha-and-p-value"
      ],
      "id": "7297ad82"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74f2ef41"
      },
      "source": [
        "### Statistical Power (Sensitivity)"
      ],
      "id": "74f2ef41"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a5fad79"
      },
      "source": [
        "https://www.statisticshowto.com/wp-content/uploads/2015/04/statistical-power.png\n",
        "\n",
        "Beta is directly related to the power of a test. Power relates to how likely a test is to distinguish an actual effect from one you could expect to happen by chance alone. Beta plus the power of a test is always equal to 1. Usually, researchers will refer to the power of a test (e.g. a power of .8), leaving the beta level (.2 in this case) as implied.\n",
        "\n",
        "https://www.statisticshowto.com/beta-level/\n",
        "\n",
        "The **statistical power of a binary hypothesis test** is the probability that the test correctly rejects the null hypothesis $H_{0}$ when a specific alternative hypothesis $H_{a}$ is true. It is commonly **denoted by $1-\\beta$** , and represents the chances of a \"true positive\" detection conditional on the actual existence of an effect to detect. Statistical power ranges from 0 to 1, and as the power of a test increases, the probability $\\beta$  of making a type II error by wrongly failing to reject the null hypothesis decreases.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Power_of_a_test"
      ],
      "id": "7a5fad79"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Types of Errors in Hypothesis Testing\n",
        "\n",
        "When conducting hypothesis testing, there's always a risk of making an incorrect decision. These incorrect decisions are classified into two types of errors:\n",
        "\n",
        "* **Type I Error (False Positive):** This occurs when you reject the null hypothesis when it is actually true. In other words, you conclude that there is a significant effect or difference when there really isn't.\n",
        "* **Type II Error (False Negative):** This occurs when you fail to reject the null hypothesis when it is actually false. In other words, you miss a real effect or difference.\n",
        "\n",
        "**Alpha (α)**\n",
        "\n",
        "* Alpha is the probability of making a Type I error. It's the significance level you set for your hypothesis test, typically 0.05. This means you're willing to accept a 5% chance of rejecting the null hypothesis when it's true.\n",
        "\n",
        "**Beta (β)**\n",
        "\n",
        "* Beta is the probability of making a Type II error. It's influenced by factors like the sample size, the effect size, and the variability in the data.\n",
        "\n",
        "**Statistical Power (1 - β)**\n",
        "\n",
        "* Statistical power, also known as the power of a test, is the probability of correctly rejecting the null hypothesis when it is false. In other words, it's the probability of detecting a real effect.\n",
        "* It's calculated as 1 - beta.\n",
        "* Higher power means a lower chance of missing a real effect.\n",
        "\n",
        "**1 - Alpha**\n",
        "\n",
        "* 1 - alpha represents the confidence level. It's the probability of correctly failing to reject the null hypothesis when it is true.\n",
        "* For example, if alpha is 0.05, then 1 - alpha is 0.95, or a 95% confidence level.\n",
        "\n",
        "**Relationship Between Concepts**\n",
        "\n",
        "These concepts are interconnected:\n",
        "\n",
        "* **Alpha and Beta:** There's a trade-off between alpha and beta. Decreasing alpha (lowering the risk of a Type I error) generally increases beta (increasing the risk of a Type II error), and vice versa.\n",
        "* **Power and Sample Size:** Increasing the sample size generally increases statistical power, as you have more data to detect a real effect.\n",
        "* **Power and Effect Size:** Larger effect sizes are easier to detect, leading to higher power.\n",
        "\n",
        "**Practical Implications**\n",
        "\n",
        "Understanding these concepts is crucial for designing and interpreting hypothesis tests:\n",
        "\n",
        "* **Choosing Alpha:** You need to balance the risks of Type I and Type II errors based on the consequences of each in your specific context.\n",
        "* **Ensuring Adequate Power:** Aim for sufficient statistical power (often 80% or higher) to increase the chances of detecting a real effect if one exists.\n",
        "* **Interpreting Results:** Consider the possibility of both types of errors when interpreting the results of a hypothesis test.\n",
        "\n",
        "By carefully considering these factors, you can conduct more robust and reliable hypothesis tests."
      ],
      "metadata": {
        "id": "UZItULGcPkf3"
      },
      "id": "UZItULGcPkf3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebdbd2ab"
      },
      "source": [
        "The importance of proper A/B testing methodology and focusing on the dangers of p-hacking and the benefits of power analysis.\n",
        "\n",
        "**The Problem with P-hacking**\n",
        "\n",
        "P-hacking refers to the practice of repeatedly analyzing data or manipulating test parameters until a statistically significant result (low p-value) is achieved. This can lead to false positives, where you mistakenly conclude that a change has an effect when it actually doesn't.\n",
        "\n",
        "Why is this bad?\n",
        "\n",
        "* **Misleading Results:** P-hacking can lead to implementing changes based on flawed data, wasting resources and potentially harming user experience.\n",
        "* **Lack of Reproducibility:** P-hacked results are often not reproducible, as they are based on chance findings rather than true effects.\n",
        "\n",
        "**The Solution: Power Analysis**\n",
        "\n",
        "Instead of p-hacking, the recommended approach is to conduct a power analysis before running the A/B test. This involves determining the minimum sample size needed to detect a meaningful effect with a certain level of confidence.\n",
        "\n",
        "**Key Components of Power Analysis**\n",
        "\n",
        "* **Significance Level (Alpha):** The probability of rejecting a true null hypothesis (false positive). It's the risk you're willing to take of concluding there's an effect when there isn't.\n",
        "* **Statistical Power (1 - Beta):** The probability of correctly rejecting a false null hypothesis (true positive). It's the likelihood of detecting a real effect if one exists.\n",
        "* **Minimum Detectable Effect (MDE):** The smallest effect size that is practically meaningful for your business.\n",
        "\n",
        "**How Power Analysis Helps**\n",
        "\n",
        "* **Determines Sample Size:** By specifying the desired significance level, power, and MDE, you can calculate the minimum sample size needed for a reliable A/B test.\n",
        "* **Reduces Bias:** This approach reduces the risk of p-hacking by pre-determining the sample size and avoiding the temptation to stop the test early based on seemingly significant results.\n",
        "* **Increases Confidence:** A well-powered A/B test gives you greater confidence that your results are valid and reproducible.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "The passage emphasizes the importance of conducting A/B tests with a robust methodology. By avoiding p-hacking and utilizing power analysis, you can increase the reliability and trustworthiness of your results, leading to better data-driven decisions."
      ],
      "id": "ebdbd2ab"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Terms\n",
        "\n",
        "### A/B Cheat Sheet:\n",
        "\n",
        "https://towardsdatascience.com/25-a-b-testing-concepts-interview-cheat-sheet-c998a501f911\n",
        "\n",
        "Descriptive / Summary Statistics\n",
        "\n",
        "* Population\n",
        "* Sample\n",
        "* Sample Mean\n",
        "* Sample Variability\n",
        "\n",
        "Experiment Design\n",
        "\n",
        "* Null Hypothesis\n",
        "* Key Metrics\n",
        "* Lifetime Value (LTV)\n",
        "* Objectives and Key Results (OKR)\n",
        "* Overall Evaluation Criteria (OEC)\n",
        "* Gaurdrail Metrics\n",
        "* Randomization Unit\n",
        "* Interference\n",
        "\n",
        "A/B Test Statistics\n",
        "\n",
        "* Confidence Level\n",
        "* Margin of Error\n",
        "* Confidence Interval\n",
        "* Type I Error\n",
        "* Type II Error\n",
        "* p-Value\n",
        "* Statistical Significance\n",
        "* Statistical Power\n",
        "* Minimum Detectable Effect\n",
        "* Practical Significance\n",
        "* Sample Size and Duration\n",
        "\n",
        "Threats to Experiment Validity\n",
        "\n",
        "* Novelty Effect\n",
        "* Primary Effect\n",
        "* Seasonality\n",
        "* Day of the Week\n",
        "\n",
        "### Dictionary of Terms\n",
        "\n",
        "https://marketingexperiments.com/a-b-testing/marketing-and-online-testing-dictionary"
      ],
      "metadata": {
        "id": "zsmBQgI_1OZl"
      },
      "id": "zsmBQgI_1OZl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Descriptive / Summary Statistics\n",
        "\n",
        "* **Descriptive/Summary Statistics:**\n",
        "    * These are values that summarize and describe the main features of a dataset. They provide a concise overview of the data without making inferences about a larger population.\n",
        "    * Descriptive statistics include:\n",
        "        * Measures of central tendency (e.g., mean, median, mode).\n",
        "        * Measures of variability or dispersion (e.g., range, variance, standard deviation).\n",
        "        * Measures of frequency distribution (e.g., counts, percentages).\n",
        "    * Essentially, they help you understand the \"shape\" of your data.\n",
        "\n",
        "* **Population:**\n",
        "    * In statistics, a population is the entire group of individuals, objects, or events that are of interest in a study.\n",
        "    * It's the complete set of all possible observations.\n",
        "    * For example, if you're studying the heights of all high school students in a country, then all high school students in that country constitute the population.\n",
        "\n",
        "* **Sample:**\n",
        "    * A sample is a subset of a population selected for study.\n",
        "    * Because it's often impractical or impossible to study an entire population, researchers collect data from a sample and use it to make inferences about the population.\n",
        "    * A good sample is representative of the population, meaning it reflects the characteristics of the population as a whole.\n",
        "\n",
        "* **Sample Mean:**\n",
        "    * The sample mean is the average of the values in a sample.\n",
        "    * It's calculated by summing all the values in the sample and dividing by the number of values.\n",
        "    * It's used as an estimate of the population mean.\n",
        "    * It is often represented by the symbol x̄.\n",
        "\n",
        "* **Sample Variability:**\n",
        "    * Sample variability refers to how spread out or dispersed the data points are in a sample.\n",
        "    * It measures the extent to which the values in the sample differ from each other and from the sample mean.\n",
        "    * Common measures of sample variability include:\n",
        "        * **Variance:** The average of the squared differences from the mean.\n",
        "        * **Standard deviation:** The square root of the variance, which provides a measure of variability in the same units as the data.\n",
        "        * **Range:** The difference between the largest and smallest values.\n",
        "    * Sample variability is important because it indicates how representative the sample mean is of the data as a whole.\n"
      ],
      "metadata": {
        "id": "v41eo37L1Siq"
      },
      "id": "v41eo37L1Siq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimental Design\n",
        "\n",
        "* **Experiment Design:**\n",
        "    * This is the process of planning a study to test a hypothesis. It involves determining how to manipulate variables, assign participants, and collect data to minimize bias and ensure valid results.\n",
        "    * Key aspects include:\n",
        "        * Defining the independent and dependent variables.\n",
        "        * Establishing control groups.\n",
        "        * Implementing randomization.\n",
        "        * Choosing appropriate statistical methods.\n",
        "\n",
        "* **Null Hypothesis:**\n",
        "    * In hypothesis testing, the null hypothesis is a statement that there is no significant difference or effect. It's the default assumption that researchers aim to disprove.\n",
        "\n",
        "* **Key Metrics:**\n",
        "    * These are the specific, measurable values that are tracked to assess the performance or impact of an experiment or process. They are vital for determining whether objectives are being met.\n",
        "\n",
        "* **Lifetime Value (LTV):**\n",
        "    * LTV is a prediction of the total value a customer will bring to a business over the entire duration of their relationship. It's a crucial metric for understanding customer profitability.\n",
        "\n",
        "* **Objectives and Key Results (OKR):**\n",
        "    * OKR is a framework for setting and tracking goals.\n",
        "    * Objectives are qualitative, aspirational goals.\n",
        "    * Key Results are quantitative, measurable outcomes that indicate progress toward the objective.\n",
        "\n",
        "* **Overall Evaluation Criteria (OEC):**\n",
        "    * OEC refers to the primary metrics that will be used to judge the overall success of an experiment or project. It's the overarching measure of whether the desired outcome was achieved.\n",
        "\n",
        "* **Guardrail Metrics:**\n",
        "    * These are metrics that are monitored to ensure that an experiment or change does not have unintended negative consequences. They act as \"safety checks\" to prevent harmful side effects.\n",
        "\n",
        "* **Randomization Unit:**\n",
        "    * This is the entity (e.g., individual, user, group) that is randomly assigned to different experimental conditions. It's the level at which randomization occurs.\n",
        "\n",
        "* **Interference:**\n",
        "    * In experimental contexts, interference occurs when the behavior of one experimental unit affects the behavior of another. This can compromise the validity of the results, as it violates the assumption of independent observations."
      ],
      "metadata": {
        "id": "QSqn9xPB15aa"
      },
      "id": "QSqn9xPB15aa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A/B Test Statistics\n",
        "\n",
        "* **Confidence Level:**\n",
        "    * This represents the probability that the confidence interval contains the true population parameter. Commonly expressed as a percentage (e.g., 95%). A 95% confidence level means that if you repeated the experiment many times, 95% of the confidence intervals would contain the true value.\n",
        "* **Margin of Error:**\n",
        "    * This is the range of values above and below the sample statistic within which the true population parameter is likely to fall. It quantifies the uncertainty in your estimate.\n",
        "* **Confidence Interval:**\n",
        "    * This is the range of values, calculated from sample data, that is likely to contain the true population parameter with a certain level of confidence.\n",
        "* **Type I Error (False Positive):**\n",
        "    * This occurs when you reject the null hypothesis when it is actually true. In A/B testing, it means concluding that there is a significant difference between variations when there isn't.\n",
        "* **Type II Error (False Negative):**\n",
        "    * This occurs when you fail to reject the null hypothesis when it is actually false. In A/B testing, it means missing a real difference between variations.\n",
        "* **p-Value:**\n",
        "    * This is the probability of obtaining the observed results (or more extreme) if the null hypothesis were true. A small p-value indicates strong evidence against the null hypothesis.\n",
        "* **Statistical Significance:**\n",
        "    * This refers to the likelihood that an observed effect is not due to chance. It is typically determined by comparing the p-value to a predetermined significance level (alpha).\n",
        "* **Statistical Power:**\n",
        "    * This is the probability of correctly rejecting the null hypothesis when it is false. It represents the test's ability to detect a real effect.\n",
        "* **Minimum Detectable Effect (MDE):**\n",
        "    * This is the smallest effect size that you want to be able to detect with your A/B test. It helps determine the required sample size.\n",
        "* **Practical Significance:**\n",
        "    * While statistical significance indicates whether an effect is likely real, practical significance addresses whether the effect is meaningful in a real-world context. A statistically significant result may not be practically significant if the effect size is too small.\n",
        "* **Sample Size and Duration:**\n",
        "    * Sample size is the number of participants or observations in a study. Duration refers to the length of time the study is conducted. Both are critical for achieving statistically reliable results. Larger sample sizes and longer durations generally increase statistical power.\n",
        "\n"
      ],
      "metadata": {
        "id": "OhQKcJAS2IYv"
      },
      "id": "OhQKcJAS2IYv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Threats to Validity\n",
        "\n",
        "When conducting experiments, it's crucial to be aware of potential threats to their validity. These factors can introduce bias and distort the results, making it difficult to draw accurate conclusions. Here's a breakdown of the terms you provided:\n",
        "\n",
        "* **Novelty Effect:**\n",
        "    * This occurs when participants in an experiment react differently simply because something is new or unfamiliar. The introduction of a new feature, interface, or product can lead to a temporary surge in engagement or positive behavior, which may not be sustained over time.\n",
        "    * It's important to allow sufficient time for the novelty effect to wear off before drawing conclusions about the long-term impact of a change.\n",
        "* **Primary Effect:**\n",
        "    * This is a cognitive bias where people tend to remember the first items in a series better than subsequent items. In an experiment, if participants are exposed to multiple treatments or variations, their responses to the initial ones may be disproportionately influential.\n",
        "    * This is a form of an order effect.\n",
        "* **Seasonality:**\n",
        "    * This refers to predictable patterns in data that occur at specific times of the year. For example, retail sales tend to increase during the holiday season, and website traffic may fluctuate depending on school schedules.\n",
        "    * Seasonality can confound experimental results if the study is conducted during a period of unusually high or low activity. It's important to account for seasonality when analyzing data and drawing conclusions.\n",
        "* **Day of the Week:**\n",
        "    * This is a specific form of seasonality, but focused on the weekly cycle. User behavior and activity levels can vary significantly depending on the day of the week. For example, website traffic may be higher on weekdays than on weekends.\n",
        "    * Similar to seasonality, the day of the week can introduce bias into experimental results if the study is not designed to account for these variations.\n"
      ],
      "metadata": {
        "id": "i4pj2WDA2j6G"
      },
      "id": "i4pj2WDA2j6G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example\n",
        "\n",
        "**Scenario:**\n",
        "\n",
        "Imagine you have a website with a \"Sign Up\" button. You want to test whether changing the button's color from blue (version A) to green (version B) will increase the sign-up rate.\n"
      ],
      "metadata": {
        "id": "uiWHqXIY3Ri3"
      },
      "id": "uiWHqXIY3Ri3"
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import scipy.stats as stats\n",
        "# from statsmodels.stats.proportion import proportions_ztest # Import the correct function\n",
        "\n",
        "# # Simulated data (replace with actual data)\n",
        "# np.random.seed(42) # For reproducibility\n",
        "\n",
        "# # Number of users (trials) in each group\n",
        "# n_A = 1000\n",
        "# n_B = 1000\n",
        "\n",
        "# # Simulated sign-up rates (replace with actual data)\n",
        "# conversion_rate_A = 0.10 # 10% for blue button\n",
        "# conversion_rate_B = 0.12 # 12% for green button\n",
        "\n",
        "# # Generate simulated sign-up data (1 = sign-up, 0 = no sign-up)\n",
        "# signups_A = np.random.choice([1, 0], size=n_A, p=[conversion_rate_A, 1 - conversion_rate_A])\n",
        "# signups_B = np.random.choice([1, 0], size=n_B, p=[conversion_rate_B, 1 - conversion_rate_B])\n",
        "\n",
        "# # Calculate number of successes (sign-ups) and trials (users)\n",
        "# count_A = np.sum(signups_A)\n",
        "# count_B = np.sum(signups_B)\n",
        "\n",
        "# # Calculate conversion rates from the simulated data.\n",
        "# actual_conversion_rate_A = count_A / n_A\n",
        "# actual_conversion_rate_B = count_B / n_B\n",
        "\n",
        "# print(f\"Version A (Blue): Successes = {count_A}, Trials = {n_A}, Rate = {actual_conversion_rate_A:.4f}\")\n",
        "# print(f\"Version B (Green): Successes = {count_B}, Trials = {n_B}, Rate = {actual_conversion_rate_B:.4f}\")\n",
        "\n",
        "# count = np.array([count_A, count_B])\n",
        "# nobs = np.array([n_A, n_B])\n",
        "\n",
        "# # The proportions_ztest function returns (z_statistic, p_value)\n",
        "# z_stat, p_value = proportions_ztest(count, nobs, alternative='two-sided')\n",
        "\n",
        "# print(\"\\n--- A/B Test Results (Z-Test for Proportions) ---\")\n",
        "# print(f\"Z-statistic: {z_stat:.4f}\")\n",
        "# print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# # Determine statistical significance\n",
        "# alpha = 0.05 # Significance level\n",
        "\n",
        "# if p_value < alpha:\n",
        "#     print(\"Result: Statistically significant. Reject the null hypothesis.\")\n",
        "#     if actual_conversion_rate_B > actual_conversion_rate_A:\n",
        "#         print(\"Conclusion: Version B (Green) performed better.\")\n",
        "#     else:\n",
        "#         print(\"Conclusion: Version A (Blue) performed better.\")\n",
        "# else:\n",
        "#     print(\"Result: Not statistically significant. Fail to reject the null hypothesis.\")\n",
        "#     print(\"Conclusion: There is no strong evidence to suggest a difference between the two versions.\")"
      ],
      "metadata": {
        "id": "fuuFadIw3NeS"
      },
      "id": "fuuFadIw3NeS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation\n",
        "\n",
        "This Python code simulates and analyzes the results of an **A/B test** to determine if there is a statistically significant difference between the conversion rates of two versions, Version A and Version B. It uses statistical methods appropriate for comparing two binary proportions.\n",
        "\n",
        "#### 1. Setup and Simulation\n",
        "\n",
        "* **Imports:** The code imports `numpy` for numerical operations, `scipy.stats` for general statistics, and `proportions_ztest` from `statsmodels.stats.proportion` for the core statistical test.\n",
        "* **Seed:** `np.random.seed(42)` ensures that the simulated random data is the same every time the code runs, making the results **reproducible**.\n",
        "* **Parameters:**\n",
        "    * `n_A` and `n_B` define the sample size (number of users) in each group (1,000 each).\n",
        "    * `conversion_rate_A` (10%) and `conversion_rate_B` (12%) are the true underlying rates used to generate the data.\n",
        "* **Data Generation:** `np.random.choice` simulates the outcome for each user (1 for a sign-up, 0 for no sign-up) based on the defined conversion rates and sample sizes. This simulates the observed data from an actual experiment.\n",
        "* **Success Counts:** `count_A = np.sum(signups_A)` and `count_B = np.sum(signups_B)` calculate the total number of successes (sign-ups) in each group.\n",
        "\n",
        "***\n",
        "\n",
        "#### 2. Rate Calculation\n",
        "\n",
        "* The **actual conversion rates** are calculated by dividing the number of successes by the number of trials (`count / n`).\n",
        "* The code prints these observed rates to provide context for the statistical test.\n",
        "\n",
        "***\n",
        "\n",
        "#### 3. Statistical Test (Two-Sample Z-Test for Proportions)\n",
        "\n",
        "The central part of the analysis uses the **Two-Sample Z-Test for Proportions**, which is the standard statistical test for A/B testing with conversion data.\n",
        "\n",
        "* **Test Function:** `proportions_ztest(count, nobs, alternative='two-sided')` is called, where:\n",
        "    * `count` is an array containing `[count_A, count_B]`.\n",
        "    * `nobs` is an array containing the total observations `[n_A, n_B]`.\n",
        "    * `alternative='two-sided'` means the test is checking if the rates are different (either A > B or B > A).\n",
        "* **Outputs:** The function returns the **Z-statistic** and the **P-value**.\n",
        "    * The **Z-statistic** measures how many standard deviations the difference between the two conversion rates is from zero (the null hypothesis).\n",
        "    * The **P-value** is the probability of observing a difference as extreme as the one calculated, assuming the null hypothesis (that there is no real difference between the versions) is true.\n",
        "\n",
        "***\n",
        "\n",
        "#### 4. Conclusion and Interpretation\n",
        "\n",
        "* **Significance Level ($\\alpha$):** A standard alpha value of **0.05** is set.\n",
        "* **Decision Rule:**\n",
        "    * If the **P-value is less than 0.05**, the result is declared **statistically significant**. This means we have enough evidence to **reject the null hypothesis** and conclude that there is a real difference between the two versions. The code then checks which version had the higher observed rate to state the final conclusion.\n",
        "    * If the **P-value is greater than or equal to 0.05**, the result is **not statistically significant**. This means we **fail to reject the null hypothesis**, concluding that the observed difference could be due to random chance."
      ],
      "metadata": {
        "id": "4umsgwhrBKNN"
      },
      "id": "4umsgwhrBKNN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Tests Used for AB Testing\n",
        "You bet! Here are the common A/B testing statistical tests, organized by the type of data they are designed for.\n",
        "\n",
        "***\n",
        "\n",
        "## 1. Tests for Categorical/Binary Data (Counts and Rates)\n",
        "\n",
        "These tests are used for metrics that are binary (yes/no) and measured as a **proportion** or **rate** (e.g., conversion rate).\n",
        "\n",
        "* **Z-Test for Proportions**\n",
        "    * **Purpose:** Compares the proportions (rates) of two large groups (A and B) to see if the difference is statistically significant.\n",
        "    * **Common Use Case:** The most frequent test for A/B testing conversion rates (Did a user convert: Yes/No?).\n",
        "    * **Assumption:** Requires a large sample size.\n",
        "* **Chi-Square ($\\chi^2$) Test**\n",
        "    * **Purpose:** Checks if the distribution of counts across categories is independent of the group. It is mathematically very similar to the Z-Test for proportions when only two groups are compared.\n",
        "    * **Common Use Case:** Used for Click-Through Rate (CTR) and other binary success/failure counts.\n",
        "* **Fisher's Exact Test**\n",
        "    * **Purpose:** A non-approximate alternative to the Chi-Square test.\n",
        "    * **Common Use Case:** Used for categorical data when the overall **sample size is small** or when expected counts are very low, where the Chi-Square's approximation might be unreliable.\n",
        "\n",
        "***\n",
        "\n",
        "## 2. Tests for Continuous Data (Means and Averages)\n",
        "\n",
        "These tests are used for numerical metrics that have a mean and standard deviation.\n",
        "\n",
        "* **Welch's T-Test**\n",
        "    * **Purpose:** Compares the **means** (averages) of a numerical metric between two groups (A and B).\n",
        "    * **Common Use Case:** The standard test for continuous metrics because it does not require the two groups to have equal variances, which is often a safer assumption in real-world A/B experiments.\n",
        "    * **A/B Metric Examples:** Average Order Value (AOV), Revenue Per User (RPU), or Time Spent on Site.\n",
        "* **Student's T-Test (Independent Samples)**\n",
        "    * **Purpose:** Also compares the means of two groups, but specifically assumes the variances of the two groups are equal.\n",
        "    * **Common Use Case:** Used less frequently than Welch's T-Test in industry unless there is a strong reason to assume equal variance.\n",
        "\n",
        "***\n",
        "\n",
        "## 3. Test for Multiple Variants ($\\text{A/B/n}$ Testing)\n",
        "\n",
        "* **ANOVA (Analysis of Variance)**\n",
        "    * **Purpose:** Compares the **means** of three or more independent groups (e.g., Variant A, Variant B, and Variant C) to determine if at least one group mean is different from the others.\n",
        "    * **Common Use Case:** When running an A/B/C/D test on a new feature or design.\n",
        "    * **Note:** If ANOVA shows a difference, you must use a **post-hoc test** (like Tukey's HSD) to find out *exactly which pairs* are different."
      ],
      "metadata": {
        "id": "gbeX4I1pDiTj"
      },
      "id": "gbeX4I1pDiTj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chi-Square Tests\n",
        "\n",
        "The **Chi-Square ($\\chi^2$) test** is a fundamental statistical tool in A/B testing used to determine if the **observed difference** in conversion rates between your variants (A and B) is likely due to a real effect or simply **random chance**.\n",
        "\n",
        "The Chi-Square test is specifically designed for **categorical data**, which perfectly describes the outcomes of most A/B tests:\n",
        "1.  **Group/Variable 1:** The version shown (Category 1: Version A, Category 2: Version B).\n",
        "2.  **Outcome/Variable 2:** The result (Category 1: Converted/Success, Category 2: Did Not Convert/Failure).\n",
        "\n",
        "The test compares the **Observed Frequencies** (your actual A/B test results) to the **Expected Frequencies** (what you would expect if there were **NO difference** between the two versions, which is the **null hypothesis**).\n",
        "\n",
        "* **Null Hypothesis ($H_0$):** The conversion rate for Version A is equal to the conversion rate for Version B (the two variables—version and outcome—are independent).\n",
        "* **Alternative Hypothesis ($H_a$):** The conversion rates are different (the two variables are dependent).\n",
        "\n",
        "The $\\chi^2$ statistic is calculated using the following formula, summed over all cells in your contingency table:\n",
        "$$\\chi^2 = \\sum \\frac{(O - E)^2}{E}$$\n",
        "Where:\n",
        "* $O$ = **Observed** count in a cell.\n",
        "* $E$ = **Expected** count in a cell (calculated under the null hypothesis).\n",
        "\n",
        "A **larger $\\chi^2$ value** indicates a greater deviation between what you observed and what you expected, making it more likely that the difference is statistically significant.\n",
        "\n",
        "The Chi-Square test is most appropriate and commonly used in A/B testing for **conversion metrics** when the data is structured as a **contingency table**.\n",
        "\n",
        "### 1. Simple A/B Tests (2x2 Tables)\n",
        "\n",
        "This is the most common scenario: comparing two groups (A and B) on a binary outcome (e.g., converted or not converted).\n",
        "\n",
        "| Outcome | Version A (Control) | Version B (Variant) | Total |\n",
        "| :---: | :---: | :---: | :---: |\n",
        "| **Converted** | Count $A_{success}$ | Count $B_{success}$ | Total Successes |\n",
        "| **Did Not Convert** | Count $A_{failure}$ | Count $B_{failure}$ | Total Failures |\n",
        "| **Total Users** | $N_A$ | $N_B$ | $N_{Total}$ |\n",
        "\n",
        "In this $2 \\times 2$ case, the Chi-Square test is mathematically **equivalent** to the **two-sample Z-test for proportions**. Both tests will yield the same P-value, and the $\\chi^2$ statistic will be the square of the $Z$-statistic ($\\chi^2 = Z^2$) with one degree of freedom. Therefore, you can use either one.\n",
        "\n",
        "### 2. A/B/C/D... Tests (Multiple Variants)\n",
        "\n",
        "This is where the Chi-Square test shines and provides value beyond the Z-test. If you are testing **three or more versions (A, B, C, etc.) simultaneously**, the Chi-Square test is the correct choice to determine if **at least one** of the variants is performing significantly differently from the others.\n",
        "\n",
        "### 3. Multiple Outcomes (Funnel Drop-off)\n",
        "\n",
        "The test can also be extended to situations with multiple categorical outcomes, such as a multi-step funnel:\n",
        "* Did not convert.\n",
        "* Converted to Lead.\n",
        "* Converted to Purchase.\n",
        "\n",
        "Here, you would use a Chi-Square test (specifically, a **Test of Homogeneity**) to see if the overall distribution of users across these three outcome categories is the same for Version A and Version B.\n"
      ],
      "metadata": {
        "id": "tyMLfMw3CpPN"
      },
      "id": "tyMLfMw3CpPN"
    },
    {
      "cell_type": "code",
      "source": [
        "# # remember the normal distribution?\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from scipy.stats import norm\n",
        "\n",
        "# # 1. Define the parameters for the standard normal distribution\n",
        "# mu = 0\n",
        "# sigma = 1\n",
        "\n",
        "# # 2. Create the x-axis range and the corresponding PDF curve (y-axis)\n",
        "# x = np.linspace(mu - 4*sigma, mu + 4*sigma, 500)\n",
        "# y = norm.pdf(x, mu, sigma)\n",
        "\n",
        "# # 3. Create the plot\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(x, y, color='black', linewidth=1)\n",
        "# plt.title('The Empirical Rule (68-95-99.7) on the Normal Distribution')\n",
        "# plt.xlabel('Z-score (Standard Deviations)')\n",
        "# plt.ylabel('Probability Density Function (PDF)')\n",
        "# plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# # 4. Define the boundaries for 1, 2, and 3 standard deviations\n",
        "# # Shade the regions from the center outwards for visual clarity\n",
        "# # 99.7% (3-sigma) region\n",
        "# x_3sigma = x[(x >= mu - 3*sigma) & (x <= mu + 3*sigma)]\n",
        "# y_3sigma = norm.pdf(x_3sigma, mu, sigma)\n",
        "# plt.fill_between(x_3sigma, y_3sigma, color='skyblue', alpha=0.2, label='99.7% (±3σ)')\n",
        "\n",
        "# # 95% (2-sigma) region\n",
        "# x_2sigma = x[(x >= mu - 2*sigma) & (x <= mu + 2*sigma)]\n",
        "# y_2sigma = norm.pdf(x_2sigma, mu, sigma)\n",
        "# plt.fill_between(x_2sigma, y_2sigma, color='skyblue', alpha=0.4, label='95% (±2σ)')\n",
        "\n",
        "# # 68% (1-sigma) region\n",
        "# x_1sigma = x[(x >= mu - 1*sigma) & (x <= mu + 1*sigma)]\n",
        "# y_1sigma = norm.pdf(x_1sigma, mu, sigma)\n",
        "# plt.fill_between(x_1sigma, y_1sigma, color='skyblue', alpha=0.7, label='68% (±1σ)')\n",
        "\n",
        "# # Add lines for the boundaries\n",
        "# boundaries = [-3, -2, -1, 1, 2, 3]\n",
        "# for b in boundaries:\n",
        "#     plt.axvline(b, color='gray', linestyle=':', linewidth=0.8)\n",
        "\n",
        "# plt.legend(loc='upper right')\n",
        "# plt.ylim(0, 0.45)\n",
        "# plt.tight_layout()"
      ],
      "metadata": {
        "id": "Afxz6W31ScZ-"
      },
      "id": "Afxz6W31ScZ-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normal Distribution (Gaussian Distribution)**\n",
        "\n",
        "The normal distribution is defined by two parameters: the mean ($\\mu$) and the standard deviation ($\\sigma$).\n",
        "\n",
        "The Probability Density Function (PDF) is:\n",
        "\n",
        "$$f(x | \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu}{\\sigma} \\right)^2}$$\n",
        "\n",
        "* **$x$**: The variable value (your input).\n",
        "* **$\\mu$ (Mean)**: The center of the distribution.\n",
        "* **$\\sigma$ (Standard Deviation)**: The spread of the distribution.\n",
        "* **$\\pi$** ($\\approx 3.14159...$) and **$e$** ($\\approx 2.71828...$) are mathematical constants.\n",
        "\n",
        "The curve in the **Empirical Rule image** is a special case called the **Standard Normal Distribution**, where $\\mu=0$ and $\\sigma=1$."
      ],
      "metadata": {
        "id": "b_GqZjpCU_mA"
      },
      "id": "b_GqZjpCU_mA"
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from scipy.stats import chi2\n",
        "\n",
        "# x = np.linspace(0, 9, 100)\n",
        "# plt.title('The Chi Square Distribution')\n",
        "# plt.plot(x, chi2(1).pdf(x), label=f'df = 1')\n",
        "# plt.plot(x, chi2(2).pdf(x), label=f'df = 2')\n",
        "# plt.plot(x, chi2(3).pdf(x), label=f'df = 3')\n",
        "# plt.plot(x, chi2(4).pdf(x), label=f'df = 4')\n",
        "# plt.plot(x, chi2(6).pdf(x), label=f'df = 6')\n",
        "# plt.plot(x, chi2(9).pdf(x), label=f'df = 9')\n",
        "# plt.ylim(0, 1)\n",
        "\n",
        "# plt.legend();"
      ],
      "metadata": {
        "id": "z3pcTNxyFUwI"
      },
      "id": "z3pcTNxyFUwI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chi-Square ($\\chi^2$) Distribution**\n",
        "\n",
        "The Chi-Square distribution is defined by a single parameter: the **degrees of freedom ($k$ or $df$)**. It is always non-negative and is right-skewed.\n",
        "\n",
        "The Probability Density Function (PDF) is:\n",
        "\n",
        "$$f(x; k) = \\frac{1}{2^{k/2} \\Gamma(k/2)} x^{(k/2) - 1} e^{-x/2} \\quad \\text{for } x > 0$$\n",
        "\n",
        "* **$x$**: The $\\chi^2$ value (your input, which must be non-negative).\n",
        "* **$k$ (Degrees of Freedom, $df$)**: The parameter that defines the shape of the curve.\n",
        "* **$e$** ($\\approx 2.71828...$): The mathematical constant.\n",
        "* **$\\Gamma$ (Gamma Function)**: A generalization of the factorial function to real and complex numbers. For integer $n$, $\\Gamma(n) = (n-1)!$.\n",
        "\n",
        "The multiple curves in the **Chi-Square distribution image** show how the shape changes as the degrees of freedom ($df$) increases. As $df$ gets larger, the $\\chi^2$ distribution shifts right and becomes less skewed, eventually resembling a normal distribution."
      ],
      "metadata": {
        "id": "vYxlnFsTUe6E"
      },
      "id": "vYxlnFsTUe6E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The degrees of freedom (df) is the only parameter needed to define the shape of a chi-squared distribution because it encapsulates the essential information about the variability and constraints within the data being analyzed.\n",
        "\n",
        "Here's a breakdown of why df is so crucial:\n",
        "\n",
        "1. **Sum of Squared Variables:** The chi-squared distribution arises from the sum of squared standard normal random variables. Each squared standard normal variable contributes one degree of freedom.\n",
        "\n",
        "2. **Constraints and Independence:** The degrees of freedom represent the number of independent pieces of information available in the data. When you have constraints or dependencies within the data, the degrees of freedom are reduced.\n",
        "\n",
        "3. **Shape of the Distribution:** The degrees of freedom directly determine the shape of the chi-squared distribution. Here's how:\n",
        "    * **Lower df:** The distribution is highly skewed to the right, with a long tail.\n",
        "    * **Higher df:** The distribution becomes more symmetrical and approaches a normal distribution.\n",
        "\n",
        "4. **Variability:** The degrees of freedom also reflect the variability of the distribution. Lower df means higher variability (and heavier tails).\n",
        "\n",
        "**In simpler terms:**\n",
        "\n",
        "Think of degrees of freedom as the amount of \"wiggle room\" or freedom you have in your data. If you have few degrees of freedom, your data is more constrained, and the distribution will be more skewed. As you gain more degrees of freedom, the constraints loosen, and the distribution becomes more like a normal curve.\n",
        "\n",
        "**Key Takeaway:**\n",
        "\n",
        "The degrees of freedom essentially \"tell\" the chi-squared distribution how much variability and constraint to exhibit. This single parameter is sufficient to define its shape and behavior.\n"
      ],
      "metadata": {
        "id": "7oRq5z96FuqV"
      },
      "id": "7oRq5z96FuqV"
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import scipy.stats as stats\n",
        "\n",
        "# # Parameters\n",
        "# df = 1  # Degrees of freedom\n",
        "# alpha = 0.05  # Significance level\n",
        "# chi2_stat = 0.779  # Example test statistic (from your previous output)\n",
        "\n",
        "# # Generate x-values for the distribution\n",
        "# x = np.linspace(0, 10, 500)\n",
        "\n",
        "# # Calculate the chi-squared distribution PDF\n",
        "# pdf = stats.chi2.pdf(x, df)\n",
        "\n",
        "# # Calculate the critical value\n",
        "# critical_value = stats.chi2.ppf(1 - alpha, df)\n",
        "\n",
        "# # Plot the chi-squared distribution\n",
        "# plt.plot(x, pdf, label=f\"Chi-Square Distribution (df={df})\")\n",
        "\n",
        "# # Shade the rejection region\n",
        "# plt.fill_between(x[x > critical_value], pdf[x > critical_value], color='red', alpha=0.5, label=\"Rejection Region\")\n",
        "\n",
        "# # Mark the critical value\n",
        "# plt.axvline(x=critical_value, color='black', linestyle='--', label=f\"Critical Value ({critical_value:.2f})\")\n",
        "\n",
        "# # Mark the test statistic\n",
        "# plt.axvline(x=chi2_stat, color='green', linestyle='--', label=f\"Test Statistic ({chi2_stat:.2f})\")\n",
        "\n",
        "# # Add labels and title\n",
        "# plt.xlabel(\"Chi-Squared Statistic\")\n",
        "# plt.ylabel(\"Probability Density\")\n",
        "# plt.title(\"Chi-Squared Distribution with Critical Value and Test Statistic\")\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "09YSnwb2ErPI"
      },
      "id": "09YSnwb2ErPI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the chi-squared test for A/B testing with conversions and non-conversions, the degrees of freedom (df) should be **(number of rows - 1) * (number of columns - 1)**.\n",
        "\n",
        "Since we have:\n",
        "\n",
        "* 2 rows (Group A and Group B)\n",
        "* 2 columns (Conversions and Non-conversions)\n",
        "\n",
        "The df should be (2-1) * (2-1) = 1.\n",
        "\n",
        "**Why 1 df in this case?**\n",
        "\n",
        "In this specific scenario, with a 2x2 contingency table, the constraint is that the total number of observations in each group is fixed. Once you know the number of conversions in one group, the number of non-conversions is automatically determined. This reduces the independent pieces of information by one, leading to 1 degree of freedom.\n",
        "\n",
        "**Key takeaway:**\n",
        "\n",
        "While the chi-squared distribution itself can have various degrees of freedom, in the context of a 2x2 A/B test with conversions and non-conversions, the df will always be 1 due to the inherent constraints in the data."
      ],
      "metadata": {
        "id": "pwly_YQVGFBP"
      },
      "id": "pwly_YQVGFBP"
    },
    {
      "cell_type": "markdown",
      "id": "37e980de",
      "metadata": {
        "id": "37e980de"
      },
      "source": [
        "## AB Terms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2233bd78",
      "metadata": {
        "id": "2233bd78"
      },
      "source": [
        "### Overall Evaluation Criteria (OEC)\n",
        "\n",
        "An Overall Evaluation Criterion (OEC) is a (usually composite) quantitative measure of the experiment's objective. Other names include Response or Dependent Variable, Outcome Variable, Evaluation metric, Performance metric.\n",
        "\n",
        "https://www.analytics-toolkit.com/glossary/overall-evaluation-criterion/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e34239a0",
      "metadata": {
        "id": "e34239a0"
      },
      "source": [
        "### Gaurdrail Metrics\n",
        "\n",
        "Business metrics designed to indirectly measure business value and provide alerts about any potentially misleading or erroneous results and analysis.\n",
        "\n",
        "https://www.split.io/glossary/guardrail-metrics/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a43885bc",
      "metadata": {
        "id": "a43885bc"
      },
      "source": [
        "### Randomization Unit\n",
        "\n",
        "A who or what randomly assigned to a group.\n",
        "\n",
        "https://ianwhitestone.work/choosing-randomization-unit/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d781e3b",
      "metadata": {
        "id": "0d781e3b"
      },
      "source": [
        "### Data Leakage (Interference)\n",
        "\n",
        "The behavior of the control group is influenced by the treatment given to the test group.\n",
        "\n",
        "https://towardsdatascience.com/25-a-b-testing-concepts-interview-cheat-sheet-c998a501f911"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c349d919",
      "metadata": {
        "id": "c349d919"
      },
      "source": [
        "### SUTVA Assumptions\n",
        "\n",
        "The Stable Unit Treatment Value Assumption (SUTVA) is a key assumption that is usually made in causal inference. Reference 1 gives a clear definition of SUTVA, which points out that SUTVA is really two assumptions rolled into one:\n",
        "\n",
        "* The potential outcomes for any unit do not vary with the treatments assigned to other units.\n",
        "* For each unit, there are no different forms or versions of each treatment level, which lead to different potential outcomes.\n",
        "\n",
        "https://statisticaloddsandends.wordpress.com/2021/06/08/what-is-the-stable-unit-treatment-value-assumption-sutva/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e267c86",
      "metadata": {
        "id": "5e267c86"
      },
      "source": [
        "### The Stable Unit Treatment Value Assumption (SUTVA)\n",
        "\n",
        "**What is SUTVA?**\n",
        "\n",
        "SUTVA states that the outcome of a treatment on one individual should not be affected by the treatment assignment of other individuals. In simpler terms, there should be no interaction or interference between the treatment and control groups.\n",
        "\n",
        "**Why is SUTVA Important?**\n",
        "\n",
        "* **Valid Causal Inference:** SUTVA is essential for drawing valid conclusions about cause-and-effect relationships in A/B tests. If there's interference between groups, it becomes difficult to isolate the true effect of the treatment.\n",
        "* **Unbiased Estimates:** Violations of SUTVA can lead to biased estimates of the treatment effect. This means your results may be inaccurate and misleading.\n",
        "\n",
        "**Example of SUTVA Violation**\n",
        "\n",
        "The passage provides a helpful example with Joe and Mary:\n",
        "\n",
        "* Joe's blood pressure is the outcome of interest.\n",
        "* The treatment is a drug that Mary might receive.\n",
        "* If the drug causes Mary to cook with more salt, and Joe eats Mary's cooking, then Joe's blood pressure could be affected by Mary's treatment, even if Joe himself doesn't receive the drug. This is a violation of SUTVA.\n",
        "\n",
        "**How to Address SUTVA Violations**\n",
        "\n",
        "* **Identify Potential Interference:** Carefully consider the nature of your experiment and identify any potential sources of interference between groups.\n",
        "* **Adjust Design:** If interference is likely, you may need to adjust your experimental design. This could involve:\n",
        "    * Isolating groups to prevent interaction.\n",
        "    * Accounting for the interference in your analysis (e.g., by including interaction terms in a statistical model).\n",
        "    * Redesigning the experiment to measure the indirect effects as well.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "SUTVA is a critical assumption in A/B testing that ensures the treatment and control groups don't influence each other's outcomes. Violations of SUTVA can lead to biased and misleading results. By understanding and addressing potential SUTVA violations, you can improve the validity and reliability of your A/B testing conclusions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b933dcb4",
      "metadata": {
        "id": "b933dcb4"
      },
      "source": [
        "### Minimum Detectable Effect\n",
        "\n",
        "Minimum Detectable Effect = Practical Significance Level\n",
        "\n",
        "Minimum detectable effect (MDE) is a calculation that estimates the smallest improvement you are willing to be able to detect. It determines how \"sensitive\" an experiment is. Use MDE to estimate how long an experiment will take given the following:\n",
        "\n",
        "* Baseline conversion rate\n",
        "* Statistical significance\n",
        "* Traffic allocation\n",
        "\n",
        "https://support.optimizely.com/hc/en-us/articles/4410288881293-Use-minimum-detectable-effect-MDE-when-designing-an-experiment\n",
        "\n",
        "Minimum effect is a business decision more than anything else, not really a data scientist decision. At work, it will typically be a product manager decision. After all, for that you need to take into account things like engineering costs, time, and opportunity-cost of not using those resources to run other tests. And that requires a comprehensive company vision which is typical of product managers, or VP/Director of product in smaller companies.\n",
        "\n",
        "https://productds.com/wp-content/uploads/Sample_size.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8792cbab",
      "metadata": {
        "id": "8792cbab"
      },
      "source": [
        "### Practical, or Substantive, Significance\n",
        "\n",
        "The fact that an estimated regression coefficient is “statistically significant” (i.e., you can reject the null hypothesis that the true β is 0 with a high level of confidence) does not mean that your independent variable is substantively important. Did it reach MDE?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9da1e2f",
      "metadata": {
        "id": "a9da1e2f"
      },
      "source": [
        "### A/B Testing and Statistical Significance\n",
        "\n",
        "**Power Analysis**\n",
        "\n",
        "* **Significance Level and Sample Size:** A lower significance level (alpha) means you require stronger evidence to reject the null hypothesis. This often necessitates a larger sample size to detect a true effect with higher confidence.\n",
        "* **Statistical Power and Sample Size:** Higher statistical power (1 - beta) means a greater ability to detect a real effect.  Increasing the sample size generally increases power.\n",
        "* **MDE and Sample Size:** A smaller Minimum Detectable Effect (MDE) means you're looking for more subtle differences.  Detecting smaller effects requires a larger sample size.\n",
        "\n",
        "**A/A Testing**\n",
        "\n",
        "* **Purpose:** A/A tests involve splitting users into two groups but giving them the *same* treatment. This helps assess random variability and understand how much difference can occur between groups simply due to chance.\n",
        "* **Observations:**\n",
        "    * Initially, you might see noticeable differences between the groups, even though they receive the same treatment.\n",
        "    * Over time, these differences tend to decrease as the sample size increases and random variations average out.\n",
        "\n",
        "**Statistical Significance**\n",
        "\n",
        "* **Meaning:** Statistical significance indicates whether an observed effect is likely due to chance or a real difference. A low p-value (typically less than 0.05) suggests that the observed effect is unlikely to be due to random variation.\n",
        "* **Misinterpretations:** The passage highlights common misunderstandings of statistical significance:\n",
        "    * **Not Evidence of No Improvement:** A high p-value doesn't necessarily mean there's no improvement; it could just mean you don't have enough data to detect it.\n",
        "    * **Confusing with Practical Significance:** Statistical significance doesn't guarantee practical significance. An effect might be statistically significant but too small to be meaningful in a real-world context.\n",
        "    * **Not the Likelihood of True Improvement:** Statistical significance doesn't tell you how likely the observed improvement is the true improvement. It only tells you how likely it is to observe the data if there were no improvement.\n",
        "    * **Not the Likelihood of the Alternative Hypothesis:** Statistical significance doesn't directly tell you the probability of the alternative hypothesis being true or false.\n",
        "\n",
        "**Key Takeaways**\n",
        "\n",
        "* Power analysis helps determine the appropriate sample size for your A/B test.\n",
        "* A/A testing helps assess random variability and understand the baseline differences between groups.\n",
        "* Statistical significance should be interpreted carefully, avoiding common misinterpretations.\n",
        "* Focus on both statistical and practical significance when making decisions based on A/B testing results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1d239f3",
      "metadata": {
        "id": "e1d239f3"
      },
      "source": [
        "### Pooled Variance\n",
        "\n",
        "**Pooled variance** and **pooled standard error** are used in statistics when you are comparing two or more samples and assume that the populations they came from have the **same variance**.\n",
        "\n",
        "### Pooled Variance\n",
        "\n",
        "Pooled variance (also called combined, composite, or overall variance) is a single, weighted estimate of this common variance. It is calculated by \"pooling\" or combining the variance data from both samples to get a more robust estimate of the assumed shared population variance.\n",
        "\n",
        "***\n",
        "\n",
        "### Pooled Standard Error\n",
        "\n",
        "The **standard error of a sample** is simply the standard deviation of that sample, indicating how spread out the data is from the mean.\n",
        "\n",
        "The **pooled standard error** is a measure that accounts for the variances of the two samples and assumes they are equal. It is called \"pooled\" because it uses the combined data from both samples to calculate a more precise estimate of the standard error for the difference between the two sample means.\n",
        "\n",
        "The only conceptual difference between standard deviation and standard error is one of context:\n",
        "* We refer to **standard deviations** when talking about a whole population.\n",
        "* We refer to **standard errors** when talking about a sample."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39137882",
      "metadata": {
        "id": "39137882"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e78de06e",
      "metadata": {
        "id": "e78de06e"
      },
      "source": [
        "### Discrete Metrics (Binomial Metrics)\n",
        "\n",
        "Only two values are possible (0 or 1)\n",
        "\n",
        "* Click-Through-Rate: Does a user click after seeing something\n",
        "* Conversion Rate: Does a user become a customer after seeing something\n",
        "* Click-Through-Probability: The probability a user clicks on the next step\n",
        "* Bounce Rate: Percentage of people that land on your page and then leave"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83bc499a",
      "metadata": {
        "id": "83bc499a"
      },
      "source": [
        "### Continuous Metrics\n",
        "\n",
        "* Average Revenue Per User\n",
        "* Average Session Duration\n",
        "* Average Order Value"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4f61638",
      "metadata": {
        "id": "b4f61638"
      },
      "source": [
        "### Binomial Distribution\n",
        "\n",
        "Successes and Fails\n",
        "\n",
        "Formula:<br />\n",
        "$P(x: n,p) = \\binom {n}{x} p^x (1 - p)^{(n-x)}$\n",
        "\n",
        "* n trials\n",
        "* x successes\n",
        "\n",
        "According to StatisticsHowTo (2022):\n",
        "\n",
        "> A binomial distribution can be thought of as simply the probability of a SUCCESS or FAILURE outcome in an experiment or survey that is repeated multiple times. The binomial is a type of distribution that has two possible outcomes (the prefix “bi” means two, or twice). For example, a coin toss has only two possible outcomes: heads or tails and taking a test could have two possible outcomes: pass or fail.\n",
        "\n",
        "https://www.statisticshowto.com/probability-and-statistics/binomial-theorem/binomial-distribution-formula/\n",
        "\n",
        "Stephanie Glen. \"Binomial Distribution: Formula, What it is, How to use it\" From StatisticsHowTo.com: Elementary Statistics for the rest of us! https://www.statisticshowto.com/probability-and-statistics/binomial-theorem/binomial-distribution-formula/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e46b7610",
      "metadata": {
        "id": "e46b7610"
      },
      "outputs": [],
      "source": [
        "# # define success - getting heads; A fair coins is flipped 10 times. What is the probability of getting 5 heads?\n",
        "# # vs getting 5 heads in a row 1/2^5\n",
        "# from scipy import stats\n",
        "\n",
        "# stats.binom.pmf(5, 10, .5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cf4a0d2",
      "metadata": {
        "id": "3cf4a0d2"
      },
      "source": [
        "### Normal Distribution\n",
        "\n",
        "Compare the formula for the normal distribution as shown below\n",
        "\n",
        "$\n",
        "\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\large{e^\\frac{-(x-\\mu)^2}{2\\sigma^2}}\n",
        "$\n",
        "\n",
        "with the python code below\n",
        "\n",
        "1/(np.sqrt(2 * np.pi * sigma&ast;&ast;2)) * np.exp( - (x - mu)&ast;&ast;2 / (2 * sigma&ast;&ast;2))\n",
        "\n",
        "Let's break the code down:\n",
        "* x = our set of numbers\n",
        "* mu = mean\n",
        "* sigma&ast;&ast;2 (sigma squared) = variance of x (sigma = std)\n",
        "* exp = exponential\n",
        "* 1 is our numerator\n",
        "* np.sqrt(2 * np.pi * sigma&ast;&ast;2) = $\\sqrt{2\\pi\\sigma^2}$ NOTE: the two asterisk designate a power such as squared\n",
        "* np.exp( - (x - mu)&ast;&ast;2 / (2 * sigma&ast;&ast;2)) = $\\large{e^\\frac{-(x-\\mu)^2}{2\\sigma^2}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "\n",
        "\"In an A/B test comparing two versions of a website landing page, we observed that the average time spent on the new version (treatment) was 2 minutes with a standard deviation of 30 seconds. The control version had an average time spent of 1.8 minutes with a standard deviation of 45 seconds.  Assuming the time spent on each page is normally distributed, is the difference in average time spent between the two versions statistically significant at a 95% confidence level?\"\n",
        "\n",
        "**Why this can be answered with a normal distribution:**\n",
        "\n",
        "* **Assumption of Normality:** The question explicitly states that the time spent on each page is normally distributed. This is a key assumption for using a z-test or t-test, which are based on the normal distribution.\n",
        "* **Comparing Means:** The question focuses on comparing the average time spent, which is a continuous variable. Tests of means (like the z-test or t-test) are appropriate for comparing continuous data that is normally distributed.\n",
        "* **Confidence Level:** The question specifies a 95% confidence level, which directly relates to the critical value used in the normal distribution to determine statistical significance.\n",
        "\n",
        "**How to Answer the Question:**\n",
        "\n",
        "1. **Calculate the difference in means:** 2 minutes (treatment) - 1.8 minutes (control) = 0.2 minutes.\n",
        "2. **Calculate the standard error of the difference:** This will depend on whether you're assuming equal variances or unequal variances between the groups. The formula involves the standard deviations and sample sizes of both groups.\n",
        "3. **Calculate the test statistic (z or t):** Divide the difference in means by the standard error.\n",
        "4. **Determine the p-value:** Use the test statistic and the normal distribution (or t-distribution if sample sizes are small) to find the p-value.\n",
        "5. **Compare the p-value to alpha (0.05):** If the p-value is less than 0.05, the difference is statistically significant at the 95% confidence level.\n",
        "\n",
        "Here's how to use the cumulative distribution function (CDF) and percent point function (PPF).\n",
        "\n",
        "**Scenario:**\n",
        "\n",
        "Recall that we're comparing the average time spent on two versions of a website landing page:\n",
        "\n",
        "* Treatment: Average time = 2 minutes, Standard Deviation = 30 seconds\n",
        "* Control: Average time = 1.8 minutes, Standard Deviation = 45 seconds\n",
        "\n",
        "We want to determine if the difference in average time spent is statistically significant at a 95% confidence level, assuming the time spent on each page is normally distributed.\n",
        "\n",
        "**Using CDF and PPF**\n",
        "\n",
        "Here's how you can use the CDF and PPF in this scenario:\n",
        "\n",
        "1. **Calculate the Test Statistic**\n",
        "\n",
        "   - First, you'd calculate the test statistic (z or t) as mentioned. This involves calculating the difference in means and the standard error of the difference. Let's assume you've done that and obtained a z-score of `z_stat`.\n",
        "\n",
        "2. **P-value using CDF**\n",
        "\n",
        "   - To find the p-value, you can use the CDF of the standard normal distribution (`scipy.stats.norm.cdf`).\n",
        "   - For a two-tailed test:\n",
        "     - `p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))`\n",
        "     - This calculates the area in both tails of the distribution beyond the absolute value of your test statistic.\n",
        "\n",
        "3. **Critical Value using PPF**\n",
        "\n",
        "   - Alternatively, you can find the critical value (z_critical) for a 95% confidence level using the PPF (`scipy.stats.norm.ppf`).\n",
        "   - For a two-tailed test:\n",
        "     - `alpha = 0.05`\n",
        "     - `z_critical = stats.norm.ppf(1 - alpha/2)`\n",
        "\n",
        "4. **Decision**\n",
        "\n",
        "   - Compare the p-value to alpha (0.05) or compare the absolute value of the test statistic (`abs(z_stat)`) to the critical value (`z_critical`).\n",
        "   - If `p_value < alpha` or `abs(z_stat) > z_critical`, you reject the null hypothesis and conclude that the difference in average time spent is statistically significant.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "The CDF and PPF are powerful tools in hypothesis testing. The CDF helps calculate the p-value, while the PPF helps determine the critical value. Both approaches can lead you to the same conclusion about whether to reject or fail to reject the null hypothesis."
      ],
      "metadata": {
        "id": "XjaqbOnuSITz"
      },
      "id": "XjaqbOnuSITz"
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# from scipy.stats import norm\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # --- Word Problem Scenario ---\n",
        "# # A certain type of electronic component has a lifespan (in hours) that is\n",
        "# # normally distributed with:\n",
        "# MEAN = 5000  # hours (μ)\n",
        "# STD_DEV = 500  # hours (σ)\n",
        "\n",
        "# # --- PART 1: Find the Percentage (CDF) from a Standard Deviation (Z-score) ---\n",
        "\n",
        "# # Question: What percentage of components are expected to fail AFTER 6000 hours?\n",
        "\n",
        "\n",
        "# # 1. Convert the raw value (X) to a Z-score (standard deviations from the mean)\n",
        "\n",
        "# # print(f\"Z-score for X = {X1} hours: Z = {Z1:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pic7krcB8fWY"
      },
      "id": "pic7krcB8fWY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Use the CDF to find the probability of being LESS than X1\n",
        "# CDF(Z) gives P(X < x)\n",
        "\n",
        "\n",
        "# print(f\"P(X < {X1} hours) = {P_less_than_X1:.4f}\")"
      ],
      "metadata": {
        "id": "yLavUvxQ94JQ"
      },
      "id": "yLavUvxQ94JQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Find the probability of being GREATER than X1\n",
        "# P(X > x) = 1 - P(X < x)\n",
        "\n",
        "\n",
        "# print(f\"Probability (P > {X1} hours): {P_greater_than_X1:.4f}\")\n",
        "# print(f\"Answer: {Percentage_X1:.2f}% of components will fail after 6000 hours.\\n\")"
      ],
      "metadata": {
        "id": "zmc43c4e99wc"
      },
      "id": "zmc43c4e99wc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- PART 2: Find the Standard Deviation (PPF) from a Percentage (Percentile) ---\n",
        "\n",
        "# # Question: The manufacturer wants to offer a warranty covering the shortest\n",
        "# # 10% of component lifespans. What lifespan (X) marks this threshold?\n",
        "# PERCENTILE = 0.10  # This is the 10th percentile\n",
        "\n",
        "# # 1. Use the PPF (Inverse CDF) to find the Z-score corresponding to the 10th percentile.\n",
        "# # PPF(P) gives the Z-score where P(X < x) = P\n",
        "\n",
        "# # print(f\"Z-score for the {PERCENTILE*100:.0f}th percentile: Z = {Z2:.2f}\")\n"
      ],
      "metadata": {
        "id": "28eM58Mx-TeS"
      },
      "id": "28eM58Mx-TeS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Convert the Z-score back to the raw value (X)\n",
        "# Formula: X = MEAN + (Z * STD_DEV)\n",
        "\n",
        "\n",
        "# print(f\"Lifespan (X) for the {PERCENTILE*100:.0f}th percentile: {X2:.2f} hours\")\n",
        "# print(f\"Answer: The warranty threshold should be set at {X2:.0f} hours.\\n\")"
      ],
      "metadata": {
        "id": "v5phUzP_-a9a"
      },
      "id": "v5phUzP_-a9a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- Visualization (Optional) ---\n",
        "# x = np.linspace(MEAN - 3 * STD_DEV, MEAN + 3 * STD_DEV, 100)\n",
        "# pdf = norm.pdf(x, MEAN, STD_DEV)\n",
        "\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# plt.plot(x, pdf, color='black')\n",
        "\n",
        "# # Highlight P > 6000 (Z > +2.0)\n",
        "# x_fill_cdf = np.linspace(X1, MEAN + 3 * STD_DEV)\n",
        "# plt.fill_between(x_fill_cdf, norm.pdf(x_fill_cdf, MEAN, STD_DEV), color='red', alpha=0.5, label=f'P(X > 6000) = {Percentage_X1:.2f}%')\n",
        "# plt.axvline(X1, color='red', linestyle='--', linewidth=1)\n",
        "\n",
        "# # Highlight P < X2 (P < 10%)\n",
        "# x_fill_ppf = np.linspace(MEAN - 3 * STD_DEV, X2)\n",
        "# plt.fill_between(x_fill_ppf, norm.pdf(x_fill_ppf, MEAN, STD_DEV), color='blue', alpha=0.5, label=f'P(X < {X2:.0f}) = {PERCENTILE*100:.0f}%')\n",
        "# plt.axvline(X2, color='blue', linestyle='--', linewidth=1)\n",
        "\n",
        "\n",
        "# plt.title('Normal Distribution: CDF (Red) and PPF (Blue) Examples')\n",
        "# plt.xlabel(r'Lifespan (Hours), $\\mu={MEAN}, \\sigma={STD_DEV}$')\n",
        "# plt.ylabel('Probability Density')\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "iGfSgI1D-W6v"
      },
      "id": "iGfSgI1D-W6v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "283490ea",
      "metadata": {
        "id": "283490ea"
      },
      "source": [
        "### Conversion Rates\n",
        "\n",
        "* Conversion rate: Conversion rates are calculated by simply taking the number of conversions and dividing that by the number of total ad interactions that can be tracked to a conversion during the same time period. For example, if you had 50 conversions from 1,000 interactions, your conversion rate would be 5%, since 50 ÷ 1,000 = 5%. https://support.google.com/google-ads/answer/2684489?hl=en\n",
        "* Baseline conversion rate: Current conversion rate represented as a percentage\n",
        "* A conversion can refer to any desired action that you want the user to take. This can include anything from a click on a button to making a purchase and becoming a customer. Websites and apps often have multiple conversion goals, and each will have its own conversion rate.\n",
        "\n",
        "https://www.optimizely.com/optimization-glossary/conversion-rate/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3ea6331",
      "metadata": {
        "id": "e3ea6331"
      },
      "source": [
        "### One Tail vs Two Tail AB Tests in Terms of CVR\n",
        "\n",
        "* Both Control and Treatment have equal conversion rates:\n",
        "    * null $Control (A) = Treatment (B)$\n",
        "    * alt $Control (A) \\ne Treatment (B)$\n",
        "* Treatment group's conversion rate is no better than the Control group's and could be worse:\n",
        "    * null $Control (A) \\geq Treatment (B)$\n",
        "    * alt $Control (A) \\lt Treatment (B)$\n",
        "* One tail tests look for an improvement in customer experience\n",
        "\n",
        "https://dominicsando.medium.com/why-two-sided-testing-is-reducing-your-a-b-testing-programs-impact-by-25-11d72276446a"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa98f5f5",
      "metadata": {
        "id": "fa98f5f5"
      },
      "source": [
        "### Lift\n",
        "\n",
        "Lift indicates if the treatment is better than the control. Lift is a percentage of how the treatment compares to the null hypothesis (control group).\n",
        "\n",
        "Formula:\n",
        "* (test - control) / control (* 100)\n",
        "\n",
        "https://henrykpano.medium.com/a-b-testing-calculating-lift-rate-of-a-test-3d071514deb4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d75f02f",
      "metadata": {
        "id": "8d75f02f"
      },
      "source": [
        "## Designing an A/B Test for an E-commerce Product Page\n",
        "\n",
        "**Scenario:**\n",
        "\n",
        "Imagine you're on the product team of an online e-commerce store. The UX designer has created a new version of the product page, aiming to increase the conversion rate (the percentage of users who purchase a product). The current conversion rate is 13%, and the team wants to see if the new design can increase it to 15%.\n",
        "\n",
        "**1. Formulate Hypotheses**\n",
        "\n",
        "* We'll use a two-tailed test since the new design could perform better or worse than the current one.\n",
        "* **Null Hypothesis (H0):** The conversion rate of the new design is the same as the old design (p = p0).\n",
        "* **Alternative Hypothesis (Ha):** The conversion rate of the new design is different from the old design (p ≠ p0).\n",
        "\n",
        "**2. Set Significance Level and Confidence Level**\n",
        "\n",
        "* **Alpha (α):** 0.05. This means there's a 5% chance of incorrectly rejecting the null hypothesis (false positive).\n",
        "* **Confidence Level:** 95%. This means we want to be 95% confident that our results accurately reflect the true difference in conversion rates.\n",
        "\n",
        "**3. Define Groups and Variables**\n",
        "\n",
        "* **Control Group:** Users who see the old product page design.\n",
        "* **Experimental Group:** Users who see the new product page design.\n",
        "* **Independent Variable:** The design of the product page (old vs. new).\n",
        "* **Dependent Variable:** The conversion rate (whether the user buys the product).\n",
        "\n",
        "**Why Two Groups?**\n",
        "\n",
        "Having both a control and experimental group allows us to control for external factors (like seasonality) that could influence the results. By comparing the two groups, we can isolate the effect of the design change on the conversion rate.\n",
        "\n",
        "**4. Determine Sample Size with Power Analysis**\n",
        "\n",
        "* To ensure reliable results, we'll use a power analysis to determine the necessary sample size for each group.\n",
        "* **Power (1 - β):** 0.8 (80%). This is the probability of detecting a real difference in conversion rates if one exists.\n",
        "* **Alpha (α):** 0.05. Alpha (α) determines the significance level of your statistical test and represents the maximum risk you are willing to take of making a Type I error (a false positive).\n",
        "* **Effect Size:** The difference between the current conversion rate (13%) and the desired conversion rate (15%).\n",
        "* **Sample Size:** The minimum number of units (e.g., users, visitors, clicks, or transactions) required for each group (Control Group A and Variant Group B) to ensure that the experiment's results are reliable and statistically valid."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7c107d4",
      "metadata": {
        "id": "b7c107d4"
      },
      "source": [
        "### Test of Proportions\n",
        "\n",
        "$z = \\frac{\\hat{p} - p}{\\sqrt{\\frac{p (1 - p)}{n}}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "598d782a"
      },
      "outputs": [],
      "source": [
        "# # https://www.kaggle.com/datasets/zhangluyuan/ab-testing?select=ab_data.csv\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import scipy.stats as stats\n",
        "# from scipy import stats\n",
        "# from statsmodels.stats.proportion import proportions_ztest, proportion_confint\n",
        "# import statsmodels.stats.api as sms\n",
        "# from math import ceil\n",
        "\n",
        "# df = pd.read_csv('https://raw.githubusercontent.com/gitmystuff/Datasets/main/ab_data.csv')\n",
        "# print(df.shape)\n",
        "# print(df.head())\n",
        "# print()\n",
        "\n",
        "# effect_size = sms.proportion_effectsize(0.13, 0.15)\n",
        "# # effect_size = sms.proportion_effectsize(0.116, 0.136)\n",
        "# sample_size = sms.NormalIndPower().solve_power(\n",
        "#     effect_size, # minimum detectable effect\n",
        "#     power=0.8,\n",
        "#     alpha=0.05\n",
        "#     )\n",
        "\n",
        "# sample_size = int(sample_size)\n",
        "# print('Effect Size: ', effect_size)\n",
        "# print('Sample Size: ', sample_size)\n",
        "# print()\n",
        "\n",
        "# # delete repeat users\n",
        "# session_counts = df['user_id'].value_counts(ascending=False)\n",
        "# repeat_users = session_counts[session_counts > 1].count()\n",
        "# print(f'Repeat users: {repeat_users}')\n",
        "\n",
        "# users_to_drop = session_counts[session_counts > 1].index\n",
        "# df = df[~df['user_id'].isin(users_to_drop)]\n",
        "# print(df.shape)\n",
        "# print()\n",
        "\n",
        "# # create comparison groups\n",
        "# control_sample = df[df['group'] == 'control'].sample(n=round(sample_size), random_state=42)\n",
        "# treatment_sample = df[df['group'] == 'treatment'].sample(n=round(sample_size), random_state=42)\n",
        "\n",
        "# ab_test = pd.concat([control_sample, treatment_sample], axis=0)\n",
        "# ab_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# # print(ab_test.head())\n",
        "# # print()\n",
        "# print(ab_test['group'].value_counts())\n",
        "# print()\n",
        "\n",
        "# # conversion rates\n",
        "# conversion_rates = ab_test.groupby('group')['converted']\n",
        "# std_p = lambda x: np.std(x, ddof=0) # standard deviation\n",
        "# se_p = lambda x: stats.sem(x, ddof=0) # standard error\n",
        "\n",
        "# # conversion_rates = conversion_rates.agg([np.mean, std_p, se_p])\n",
        "# # conversion_rates.columns = ['conversion_rate', 'std_deviation', 'std_error']\n",
        "# # conversion_rates.style.format('{:.3f}')\n",
        "\n",
        "# # test of proportions; never use t\n",
        "# control_group = ab_test[ab_test['group'] == 'control']['converted']\n",
        "# treatment_group = ab_test[ab_test['group'] == 'treatment']['converted']\n",
        "# n_con = control_group.count()\n",
        "# n_treat = treatment_group.count()\n",
        "# successes = [control_group.sum(), treatment_group.sum()]\n",
        "# n_obs = [n_con, n_treat]\n",
        "\n",
        "# _, pval = proportions_ztest(successes, nobs=n_obs)\n",
        "# (lower_con, lower_treat), (upper_con, upper_treat) = proportion_confint(successes, nobs=n_obs, alpha=0.05)\n",
        "\n",
        "# print(f'p-value: {pval:.3f}')\n",
        "# print(f'control group ci (95%): [{lower_con:.3f} - {upper_con:.3f}]')\n",
        "# print(f'treatment group ci (95%): [{lower_treat:.3f} - {upper_treat:.3f}]')"
      ],
      "id": "598d782a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# effect_size = sms.proportion_effectsize(0.13, 0.15)\n",
        "# effect_size = sms.proportion_effectsize(0.116, 0.136)\n",
        "```"
      ],
      "metadata": {
        "id": "sVEsPf1BPIE1"
      },
      "id": "sVEsPf1BPIE1"
    },
    {
      "cell_type": "markdown",
      "id": "79f93620",
      "metadata": {
        "id": "79f93620"
      },
      "source": [
        "### Conclusions\n",
        "\n",
        "* compare the p-value to $\\alpha$ (0.05)\n",
        "* check treatment ci upper bound - is it more than the desired rate?\n",
        "* significant, statistically significant, practically significant"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complete the Following with New Numbers\n",
        "\n",
        "Interpret the outcome regarding the treatment's effect and the new design's viability.\n",
        "\n",
        "* Is it smaller than $\\alpha$ (0.05)?\n",
        "* What is the treatment upper CI compared to the desired output?\n",
        "* Is it significant? Statitistically significant? Practically significant?\n",
        "* Does the new design work?"
      ],
      "metadata": {
        "id": "n87VzY9lUTVJ"
      },
      "id": "n87VzY9lUTVJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Core A/B Testing Concepts\n",
        "\n",
        "* **A/B Test** - A randomized experiment comparing two versions (A = control, B = variant) to measure the effect on a key metric.\n",
        "* **Control Group** - The group that experiences the original version or baseline condition.\n",
        "* **Treatment (Variant) Group** - The group that receives the changed version to test its effect.\n",
        "* **Randomization** - Assignment of users to control or treatment groups by chance, ensuring comparability.\n",
        "* **Randomization Unit** - The level at which random assignment is applied.\n",
        "* **Interference** - When treatment effects spill over across units, violating independence.\n",
        "* **Experiment Design** - The structured plan for assigning treatments, collecting data, and analyzing results.\n",
        "* **Experimentation Platform** - Software system managing randomization, tracking, metrics, and analysis.\n",
        "\n",
        "---\n",
        "\n",
        "## Descriptive Statistics and Population\n",
        "\n",
        "* **Population** - The entire set of individuals or items of interest in a study.\n",
        "* **Sample** - A subset of the population used to draw conclusions about the whole.\n",
        "* **Sample Mean** - The arithmetic average of values in a sample, used to estimate the population mean.\n",
        "* **Sample Variability** - The degree to which values in a sample differ from each other and from the mean.\n",
        "\n",
        "---\n",
        "\n",
        "## Hypothesis, Metrics, and Evaluation\n",
        "\n",
        "* **Null Hypothesis** - The default assumption that there is **no effect or difference** between groups.\n",
        "* **Key Metrics** - The primary performance measures used to evaluate the success of an experiment.\n",
        "* **Overall Evaluation Criteria (OEC)** - The agreed-upon primary metric that determines success.\n",
        "* **Guardrail Metrics** - Secondary metrics monitored to ensure no harm to critical aspects of the business.\n",
        "* **Lifetime Value (LTV)** - The predicted total revenue or profit a customer will generate over their relationship with a business.\n",
        "* **Objectives and Key Results (OKR)** - A goal-setting framework linking broad objectives to measurable outcomes.\n",
        "\n",
        "---\n",
        "\n",
        "## Statistical Concepts and Errors\n",
        "\n",
        "* **Sample Size / Power Analysis** - Calculation of how many users are needed to detect a given effect with high probability (power).\n",
        "* **Minimum Detectable Effect (MDE)** - The smallest effect size a test is designed to detect with adequate power.\n",
        "* **Effect Size** - The magnitude of the difference in outcomes between groups.\n",
        "* **Confidence Interval (CI)** - A range of plausible values for the effect size, often at 95% confidence.\n",
        "* **Confidence Level** - The probability that a confidence interval captures the true population parameter.\n",
        "* **Margin of Error** - The maximum expected difference between the sample estimate and the true population value.\n",
        "* **p-Value** - The probability of observing the data (or more extreme) if the null hypothesis of “no difference” is true.\n",
        "* **Statistical Significance** - A determination that observed results are unlikely due to chance, given a threshold ($\\alpha$).\n",
        "* **Statistical Power** - Probability that the test will detect a true effect when it exists.\n",
        "* **Type I Error (False Positive)** - Concluding there is an effect when none exists. Incorrectly rejecting a true null hypothesis.\n",
        "* **Type II Error (False Negative)** - Failing to detect a true effect. Failing to reject a false null hypothesis.\n",
        "* **Practical Significance** - Whether a statistically significant effect is large enough to matter in practice.\n",
        "\n",
        "---\n",
        "\n",
        "## Experimental Design Variants\n",
        "\n",
        "* **A/A Test** - Test where both groups see the same version, used to check randomization and instrumentation.\n",
        "* **A/B/n Test** - Extension of A/B testing to multiple variants (n > 2).\n",
        "* **Multivariate Test (MVT)** - Tests multiple factors and their interactions simultaneously.\n",
        "* **Bandit Algorithm** - Adaptive alternative to A/B testing that dynamically allocates more traffic to better-performing variants.\n",
        "* **Crossover Design** - Participants switch between conditions to control for individual differences.\n",
        "\n",
        "---\n",
        "\n",
        "## Tests Commonly Used in A/B Analysis\n",
        "\n",
        "* **Two-Proportion z-Test** - Compares two independent proportions, such as conversion rates.\n",
        "* **Chi-Square Test of Independence** - Compares observed vs. expected frequencies in a contingency table.\n",
        "* **Fisher’s Exact Test** - Exact test for independence in small 2x2 tables.\n",
        "* **t-Test (Two-Sample)** - Tests whether the means of two groups differ.\n",
        "* **Welch’s t-Test** - Adjusted t-test that does not assume equal variance.\n",
        "* **ANOVA (Analysis of Variance)** - Compares means across 3+ groups (A/B/n testing).\n",
        "* **Sequential Testing / SPRT** - Allows tests to be monitored continuously with stopping rules.\n",
        "\n",
        "---\n",
        "\n",
        "## Advanced and Related Terms\n",
        "\n",
        "* **Multiple Testing / Bonferroni Correction** - Adjustments to control false positives when running many simultaneous tests.\n",
        "* **False Discovery Rate (FDR)** - Expected proportion of false positives among declared significant results.\n",
        "* **Bayesian A/B Testing** - Uses Bayesian inference to estimate posterior probabilities of variant superiority.\n",
        "* **Uplift Modeling** - Predicts differential treatment effects at the individual level.\n",
        "* **Interim Analysis / Peeking Problem** - Looking at test results before the planned stopping time, inflating false positives.\n",
        "\n",
        "---\n",
        "\n",
        "## Threats to Experiment Validity\n",
        "\n",
        "* **Novelty Effect** - A temporary change in user behavior when first exposed to a new feature.\n",
        "* **Primary Effect** - The stable, long-term impact of a treatment after novelty fades.\n",
        "* **Seasonality** - Natural fluctuations in user behavior due to time of year or external cycles.\n",
        "* **Day of the Week** - Systematic differences in behavior between weekdays and weekends.\n",
        "* **Sample Size and Duration** - The number of participants and time needed for adequate power and stable results."
      ],
      "metadata": {
        "id": "5g_eJ4Lf0pV7"
      },
      "id": "5g_eJ4Lf0pV7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comprehensive Guide to Statistical Tests\n",
        "\n",
        "## Tests of Normality\n",
        "\n",
        "* **Shapiro–Wilk Test**\n",
        "  Definition: Tests whether sample data are drawn from a normally distributed population.\n",
        "  Scenario: Before applying a t-test on exam scores, you check if the distribution of scores approximates normality.\n",
        "\n",
        "* **Kolmogorov–Smirnov Test (with Lilliefors correction)**\n",
        "  Definition: Compares sample distribution with a reference normal distribution.\n",
        "  Scenario: A quality-control engineer tests if machine-part diameters follow normal distribution assumptions required for tolerance models.\n",
        "\n",
        "* **Anderson–Darling Test**\n",
        "  Definition: Emphasizes tails of the distribution when testing for normality.\n",
        "  Scenario: In finance, you want to check if daily returns are normally distributed, focusing especially on outliers in tails.\n",
        "\n",
        "---\n",
        "\n",
        "## Tests of Equal Variance (Homogeneity of Variance)\n",
        "\n",
        "* **Levene’s Test**\n",
        "  Definition: Tests if multiple groups have equal variances.\n",
        "  Scenario: Comparing math test variability across three teaching methods.\n",
        "\n",
        "* **Bartlett’s Test**\n",
        "  Definition: Sensitive test for homogeneity of variances, assumes normality.\n",
        "  Scenario: Testing whether experimental groups in a chemistry trial have equal measurement precision.\n",
        "\n",
        "* **Brown–Forsythe Test**\n",
        "  Definition: Robust test for equality of variances using median instead of mean.\n",
        "  Scenario: Applied in psychology to compare mood ratings variance across treatments with non-normal data.\n",
        "\n",
        "---\n",
        "\n",
        "## Tests of Means\n",
        "\n",
        "* **One-Sample t-Test**\n",
        "  Definition: Compares sample mean against a known or hypothesized population mean.\n",
        "  Scenario: Checking if the average daily commute time in your city differs from the national average of 30 minutes.\n",
        "\n",
        "* **Independent Samples t-Test**\n",
        "  Definition: Compares means between two independent groups.\n",
        "  Scenario: Evaluating whether two different fertilizers produce different average crop yields.\n",
        "\n",
        "* **Paired Samples t-Test**\n",
        "  Definition: Compares means from the same group at two time points.\n",
        "  Scenario: Testing weight of patients before and after a 6-week exercise program.\n",
        "\n",
        "* **Welch’s t-Test**\n",
        "  Definition: Adjusted t-test when groups have unequal variances.\n",
        "  Scenario: Comparing salaries of two professions with different sample sizes and variances.\n",
        "\n",
        "---\n",
        "\n",
        "## Tests of Proportions\n",
        "\n",
        "* **Chi-Square Test of Independence**\n",
        "  Definition: Assesses whether two categorical variables are associated.\n",
        "  Scenario: Testing if voting preference is related to gender in a survey.\n",
        "\n",
        "* **Chi-Square Goodness-of-Fit Test**\n",
        "  Definition: Compares observed frequency distribution with expected distribution.\n",
        "  Scenario: Determining if die rolls follow uniform distribution.\n",
        "\n",
        "* **Fisher’s Exact Test**\n",
        "  Definition: Exact test for small sample categorical data.\n",
        "  Scenario: In a clinical trial with 20 patients, testing if a treatment group has different recovery rates than control.\n",
        "\n",
        "* **One-Proportion z-Test**\n",
        "  Definition: Tests if a sample proportion equals a hypothesized proportion.\n",
        "  Scenario: Testing if 60% of voters favor a candidate vs. the claimed 50%.\n",
        "\n",
        "* **Two-Proportion z-Test**\n",
        "  Definition: Compares proportions between two groups.\n",
        "  Scenario: Comparing click-through rates between two website designs.\n",
        "\n",
        "---\n",
        "\n",
        "## ANOVA and Related\n",
        "\n",
        "* **One-Way ANOVA**\n",
        "  Definition: Compares means across 3+ groups using variance analysis.\n",
        "  Scenario: Testing average exam scores across four different teaching methods.\n",
        "\n",
        "* **Two-Way ANOVA**\n",
        "  Definition: Examines effects of two independent factors on one dependent variable, with or without interaction.\n",
        "  Scenario: Testing effects of diet type and exercise plan on weight loss.\n",
        "\n",
        "* **Repeated Measures ANOVA**\n",
        "  Definition: Compares means of the same subjects across multiple conditions or time points.\n",
        "  Scenario: Measuring cognitive performance of students at three different times of day.\n",
        "\n",
        "* **MANOVA (Multivariate ANOVA)**\n",
        "  Definition: Tests differences in group means across multiple dependent variables simultaneously.\n",
        "  Scenario: Studying impact of medication on both blood pressure and cholesterol.\n",
        "\n",
        "---\n",
        "\n",
        "## Regression and Model Fit\n",
        "\n",
        "* **F-Test for Overall Regression**\n",
        "  Definition: Tests whether regression model explains significant variance in outcome.\n",
        "  Scenario: Testing whether advertising spend, price, and seasonality together predict sales.\n",
        "\n",
        "* **Durbin–Watson Test**\n",
        "  Definition: Tests for autocorrelation in regression residuals.\n",
        "  Scenario: Checking for serial correlation in stock return predictions.\n",
        "\n",
        "* **Hosmer–Lemeshow Test**\n",
        "  Definition: Goodness-of-fit test for logistic regression.\n",
        "  Scenario: Evaluating whether predicted probabilities of disease match observed outcomes in patient groups.\n",
        "\n",
        "---\n",
        "\n",
        "## Nonparametric Tests\n",
        "\n",
        "* **Mann–Whitney U Test**\n",
        "  Definition: Nonparametric alternative to independent t-test.\n",
        "  Scenario: Comparing satisfaction scores between two restaurants when data are ordinal.\n",
        "\n",
        "* **Wilcoxon Signed-Rank Test**\n",
        "  Definition: Nonparametric alternative to paired t-test.\n",
        "  Scenario: Comparing pain levels before and after therapy on a 1–10 scale.\n",
        "\n",
        "* **Kruskal–Wallis Test**\n",
        "  Definition: Nonparametric alternative to one-way ANOVA.\n",
        "  Scenario: Comparing customer service ratings across three call centers.\n",
        "\n",
        "* **Friedman Test**\n",
        "  Definition: Nonparametric alternative to repeated-measures ANOVA.\n",
        "  Scenario: Comparing reaction times under three lighting conditions for the same subjects.\n",
        "\n",
        "---\n",
        "\n",
        "## Resampling and Experimental Design\n",
        "\n",
        "* **Bootstrap Methods**\n",
        "  Definition: Resampling with replacement to estimate uncertainty and confidence intervals.\n",
        "  Scenario: Estimating confidence interval of mean salary in a dataset too small for classical inference.\n",
        "\n",
        "* **Permutation Tests**\n",
        "  Definition: Randomly reshuffling labels to test null hypothesis.\n",
        "  Scenario: Testing if observed difference in test scores between groups could arise by chance.\n",
        "\n",
        "* **A/B Testing (Randomized Controlled Trials)**\n",
        "  Definition: Compares two (or more) variations by random assignment to detect effect on a metric.\n",
        "  Scenario: Online retailer tests two checkout page designs to see which yields higher purchase completion.\n"
      ],
      "metadata": {
        "id": "nmiMcLWi1L6Z"
      },
      "id": "nmiMcLWi1L6Z"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}