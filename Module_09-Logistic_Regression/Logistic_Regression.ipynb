{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC5502/blob/main/Module_09-Logistic_Regression/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14d5dfc8",
      "metadata": {
        "id": "14d5dfc8"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1148ca0d",
      "metadata": {
        "id": "1148ca0d"
      },
      "source": [
        "## Binary Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression\n",
        "\n",
        "$y = \\beta_0 + \\beta_1X_1 + ... + \\beta_nX_n$"
      ],
      "metadata": {
        "id": "tFge15HfAmNe"
      },
      "id": "tFge15HfAmNe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9a5dfee",
      "metadata": {
        "id": "c9a5dfee"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.datasets import make_regression\n",
        "\n",
        "# X, y = make_regression(n_samples=1000, n_features=5)\n",
        "# df = pd.DataFrame(data=X)\n",
        "# print(df.head())\n",
        "# df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"68-95-99.7 rule,\" also known as the empirical rule, is a statistical guideline for normal distributions, stating that approximately 68% of data falls within one standard deviation of the mean, 95% within two, and 99.7% within three."
      ],
      "metadata": {
        "id": "iIxuWM0xAi7l"
      },
      "id": "iIxuWM0xAi7l"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the lower and upper bounds\n",
        "# lower_bound = -3\n",
        "# upper_bound = 3\n",
        "\n",
        "# # Iterate through each column (variable) and calculate the percentage\n",
        "# for col in df.columns:\n",
        "#     # Create a boolean mask for values within the specified range for the current column\n",
        "#     within_range = (df[col] >= lower_bound) & (df[col] <= upper_bound)\n",
        "\n",
        "#     # Calculate the percentage of values within the range for the current column\n",
        "#     percent_within_range = within_range.sum() / len(df) * 100\n",
        "\n",
        "#     # Display the result for the current column\n",
        "#     print(f\"Variable {col}: {percent_within_range:.2f}% of values are between -3 and 3\")"
      ],
      "metadata": {
        "id": "TIhgK06_ANUB"
      },
      "id": "TIhgK06_ANUB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Logistic Regression\n",
        "\n",
        "The reason logistic regression has the word \"regression\" in its name is due to its **historical and statistical relationship to linear regression**.\n",
        "\n",
        "Here's a breakdown of why:\n",
        "\n",
        "* **Modeling a Relationship:** Like linear regression, logistic regression aims to **model the relationship** between a set of predictor variables (independent variables) and an outcome variable (dependent variable). In linear regression, the outcome variable is continuous. In logistic regression, the outcome variable is **binary** (or sometimes ordinal or multinomial, but the core concept starts with binary).\n",
        "\n",
        "* **Linear Combination of Predictors:** At its heart, logistic regression uses a **linear combination of the predictor variables** to estimate the outcome. This is the same fundamental structure as in linear regression:\n",
        "\n",
        "    ```\n",
        "    Linear Regression:  y = β₀ + β₁x₁ + β₂x₂ + ... + ε\n",
        "    Logistic Regression (in the logit space): log(p / (1 - p)) = β₀ + β₁x₁ + β₂x₂ + ...\n",
        "    ```\n",
        "\n",
        "    Notice the right-hand side of both equations is a linear combination of the predictors and their coefficients (the β values).\n",
        "\n",
        "* **Predicting a Value (Indirectly):** While linear regression directly predicts the value of the continuous outcome, logistic regression **indirectly** predicts the probability of the binary outcome belonging to a specific category (usually coded as 1). This probability is then used for classification.\n",
        "\n",
        "* **Mathematical Foundation:** Many of the statistical concepts and techniques used in logistic regression, such as maximum likelihood estimation for finding the coefficients and hypothesis testing for their significance, have their roots in the broader field of regression analysis.\n",
        "\n",
        "**Think of it this way:**\n",
        "\n",
        "Logistic regression takes the familiar linear regression framework and applies a **transformation (the logit function)** to the outcome variable (the probability of success). This transformation allows us to model a binary outcome using a linear combination of predictors while ensuring the predicted probabilities stay within the valid range of 0 to 1.\n",
        "\n",
        "**In summary, the \"regression\" part of \"logistic regression\" comes from:**\n",
        "\n",
        "* The use of a linear combination of predictor variables.\n",
        "* The goal of modeling the relationship between predictors and an outcome.\n",
        "* The shared statistical underpinnings with linear regression.\n",
        "\n",
        "Even though the outcome variable is categorical, the underlying mechanism involves a regression-like approach to model the factors influencing the probability of that category. It's a bit of a historical artifact and reflects the evolution of statistical modeling techniques."
      ],
      "metadata": {
        "id": "j9ECJoyTqjUa"
      },
      "id": "j9ECJoyTqjUa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The term \"linear combination\" refers to a specific way of combining a set of variables (or vectors) using scalar multiplication and addition.\n",
        "\n",
        "Here's a breakdown of what makes it \"linear\":\n",
        "\n",
        "**The Components:**\n",
        "\n",
        "* **Variables (or Vectors):** In the context of regression (both linear and logistic), these are your predictor variables (e.g., age, income, education level). Let's represent them generally as `x₁, x₂, x₃, ... , xₚ`.\n",
        "* **Coefficients (or Scalars):** These are the values that multiply each of the variables. In a regression model, these are the \"beta\" coefficients (e.g., `β₀, β₁, β₂, ... , βₚ`).\n",
        "* **Addition:** The results of the scalar multiplications are then added together.\n",
        "\n",
        "**The Definition of a Linear Combination:**\n",
        "\n",
        "A linear combination of the variables `x₁, x₂, ... , xₚ` with coefficients `c₁, c₂, ... , cₚ` is an expression of the form:\n",
        "\n",
        "```\n",
        "c₁x₁ + c₂x₂ + c₃x₃ + ... + cₚxₚ\n",
        "```\n",
        "\n",
        "**Why \"Linear\"?**\n",
        "\n",
        "The term \"linear\" comes from the fact that each variable is raised to the power of 1 (it's not squared, cubed, or involved in any other non-linear function like logarithms or exponentials *within the combination itself*). The relationship between each individual predictor and the resulting combined value is direct and proportional.\n",
        "\n",
        "**In the context of regression:**\n",
        "\n",
        "* **Linear Regression:** The predicted outcome `y` is modeled as a linear combination of the predictor variables:\n",
        "   ```\n",
        "   y = β₀ + β₁x₁ + β₂x₂ + ... + βₚxₚ + ε\n",
        "   ```\n",
        "   Here, `β₀` is the intercept (a constant term, which can be thought of as the coefficient of a variable that is always 1), and `β₁, β₂, ..., βₚ` are the coefficients associated with each predictor variable `x₁, x₂, ..., xₚ`.\n",
        "\n",
        "* **Logistic Regression (in the logit space):** The log-odds of the outcome are modeled as a linear combination of the predictor variables:\n",
        "   ```\n",
        "   log(p / (1 - p)) = β₀ + β₁x₁ + β₂x₂ + ... + βₚxₚ\n",
        "   ```\n",
        "   Again, the right-hand side is a linear combination of the predictors.\n",
        "\n",
        "**What \"Linear Combination\" is NOT:**\n",
        "\n",
        "A combination would be non-linear if it involved terms like:\n",
        "\n",
        "* `x₁²`\n",
        "* `√x₂`\n",
        "* `log(x₃)`\n",
        "* `x₁ * x₂` (interaction terms, while sometimes included in regression models, make the overall model non-linear in the original predictors)\n",
        "\n",
        "**Why is this important in regression?**\n",
        "\n",
        "The linearity of the combination simplifies the mathematical properties of the model and makes it easier to interpret the individual effects of the predictor variables (assuming no interaction terms). The coefficient associated with a predictor directly indicates the change in the outcome (or the log-odds of the outcome in logistic regression) for a one-unit change in that predictor, holding other predictors constant.\n",
        "\n",
        "So, in essence, \"linear combination\" simply means that you're adding together the predictor variables, each multiplied by a constant coefficient, without any more complex, non-linear transformations applied *to the predictors themselves within that combination*."
      ],
      "metadata": {
        "id": "63zaZMDDr1xx"
      },
      "id": "63zaZMDDr1xx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interaction terms **can contribute to multicollinearity** in a regression model (both linear and logistic). Here's why:\n",
        "\n",
        "**What is Multicollinearity?**\n",
        "\n",
        "Multicollinearity refers to a high degree of correlation between two or more predictor variables in a regression model. This can cause problems in estimating the individual effects of each predictor, leading to unstable and unreliable coefficient estimates.\n",
        "\n",
        "**How Interaction Terms Introduce Multicollinearity:**\n",
        "\n",
        "Interaction terms are created by multiplying two or more existing predictor variables together (e.g., `x₁ * x₂`). This inherently creates a relationship between the interaction term and its constituent main effect terms:\n",
        "\n",
        "1.  **Relationship with Main Effects:** The interaction term `x₁ * x₂` will naturally be correlated with both `x₁` and `x₂`.\n",
        "    * If `x₁` increases, `x₁ * x₂` will also tend to increase (assuming `x₂` is not always zero or negative).\n",
        "    * Similarly, if `x₂` increases, `x₁ * x₂` will also tend to increase.\n",
        "\n",
        "2.  **Mathematical Dependency:** The interaction term is mathematically derived from the main effects, creating a direct dependency. If you know the values of `x₁` and `x₂`, you automatically know the value of `x₁ * x₂`. This strong linear relationship is the essence of multicollinearity.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Imagine you are modeling the effect of study time (`hours`) and prior knowledge (`knowledge_score`) on exam performance. You also include an interaction term `hours * knowledge_score`.\n",
        "\n",
        "* It's likely that `hours` and `hours * knowledge_score` will be correlated. Students who study more might also have higher values for the interaction term.\n",
        "* Similarly, `knowledge_score` and `hours * knowledge_score` will likely be correlated. Students with higher prior knowledge will tend to have higher values for the interaction term.\n",
        "\n",
        "**Consequences of Multicollinearity with Interaction Terms:**\n",
        "\n",
        "The consequences are the same as with any other source of multicollinearity:\n",
        "\n",
        "* **Unstable Coefficient Estimates:** The standard errors of the coefficients for the main effects and the interaction term will tend to be inflated. This makes it harder to determine if the individual effects are statistically significant (you might fail to reject the null hypothesis even if there is a true effect).\n",
        "* **Difficulty in Interpretation:** It becomes challenging to isolate the individual effects of the main variables because their impact is intertwined with the interaction effect. For example, it's harder to say what the \"pure\" effect of `hours` is when you also have an interaction with `knowledge_score`.\n",
        "* **Inflated Variance of Coefficients:** The variance of the estimated coefficients increases, making the estimates less precise.\n",
        "\n",
        "**How to Address Multicollinearity with Interaction Terms:**\n",
        "\n",
        "* **Centering Predictors:** One common technique is to center the main effect variables before creating the interaction term. Centering involves subtracting the mean of each variable from its values. This can often reduce the correlation between the main effects and their interaction term. For example, instead of using `hours` and `knowledge_score`, you would use `(hours - mean(hours))` and `(knowledge_score - mean(knowledge_score))` to create the interaction term.\n",
        "* **Careful Interpretation:** Be cautious when interpreting the individual coefficients of the main effects in the presence of significant interaction terms. The effect of one main variable depends on the level of the other interacting variable.\n",
        "* **Consider the Research Question:** Ensure that the interaction term is theoretically justified and crucial for your research question. If it's not essential, you might consider removing it.\n",
        "* **Variance Inflation Factor (VIF):** Calculate the VIF for all predictors, including the interaction term. High VIF values (typically above 5 or 10) indicate a potential multicollinearity problem.\n",
        "* **Combine or Remove Variables (with caution):** If multicollinearity is severe and the interaction term isn't critical, you might consider combining related variables or removing one of the highly correlated predictors. However, removing an interaction term can change the interpretation of the main effects.\n",
        "\n",
        "**In conclusion, interaction terms are a common source of multicollinearity because they are mathematically derived from and inherently related to their constituent main effect variables. It's important to be aware of this potential issue and take steps to mitigate its impact on your regression analysis.**"
      ],
      "metadata": {
        "id": "51R1CSSPsCBJ"
      },
      "id": "51R1CSSPsCBJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make Regression with a Binary Class"
      ],
      "metadata": {
        "id": "5xC3OOenEMhX"
      },
      "id": "5xC3OOenEMhX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2978d9d0",
      "metadata": {
        "scrolled": false,
        "id": "2978d9d0"
      },
      "outputs": [],
      "source": [
        "# # create and train the model\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.datasets import make_regression\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# X, y = make_regression(n_samples=50, n_features=1, noise=10)\n",
        "# df = pd.DataFrame(data=X, columns=['x'])\n",
        "# df['y'] = y\n",
        "# print(df.head())\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# df[['x', 'y']] = scaler.fit_transform(df[['x', 'y']])\n",
        "# print('\\n Dataframe after scaling')\n",
        "# print(df.head())\n",
        "\n",
        "# # decision threshold = .5\n",
        "# df['class'] = df['y'].apply(lambda x: 0 if x <= 0 else 1)\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df.drop(['y'], axis=1), df['y'], test_size=.2, random_state=42)\n",
        "\n",
        "# model = LinearRegression()\n",
        "# model.fit(X_train, y_train)\n",
        "\n",
        "# # test set prediction results\n",
        "# predictions = model.predict(X_test)\n",
        "# print('\\nx, y scatter plot with line of best fit as per our line equation')\n",
        "# print(f'y = {model.intercept_} + {model.coef_[0]}X')\n",
        "\n",
        "# plt.scatter(X_train['x'], y_train, label='data')\n",
        "# plt.plot(X_train['x'], model.intercept_ + model.coef_[0] * X_train['x'], color='red', label='fitted line')\n",
        "# # plt.axhline(0.5, color='green', linestyle='dashed', label='threshold')\n",
        "# plt.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3595b5e",
      "metadata": {
        "id": "d3595b5e"
      },
      "outputs": [],
      "source": [
        "# # does a line of best fit over a binary dependent variable make sense?\n",
        "# plt.scatter(X_train['x'], X_train['class'], label='data')\n",
        "# plt.plot(X_train['x'], model.intercept_ + model.coef_[0] * X_train['x'], color='red', label='fitted line')\n",
        "# plt.axhline(.5, color='green', linestyle='dashed', label='threshold')\n",
        "# # plt.axvline(0, color='gray', label='y=0')\n",
        "# plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a120381",
      "metadata": {
        "id": "5a120381"
      },
      "source": [
        "### The Sigmoid\n",
        "\n",
        "Sigmoid curves are common in statistics as cumulative distribution functions (which go from 0 to 1), such as the integrals of the logistic density, the normal density, and Student's t probability density functions. The logistic sigmoid function is invertible, and its inverse is the logit function.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Sigmoid_function\n",
        "\n",
        "Sigmoidal growth is a modification of exponential growth in which the percent change gets smaller as the population approaches the carrying capacity.\n",
        "\n",
        "http://matcmath.org/textbooks/quantitativereasoning/sigmoidal-growth/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "261a96c5",
      "metadata": {
        "id": "261a96c5"
      },
      "outputs": [],
      "source": [
        "# # what we'd really like\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import scipy.stats as stats\n",
        "\n",
        "# # df with x and class\n",
        "# print(df.drop('y', axis=1).head())\n",
        "\n",
        "# x = np.linspace(-2, 2, 100)\n",
        "# s = .3 # scale parameter\n",
        "# print('\\nAn S curve that is bound by 1 and 0')\n",
        "# plt.scatter(X_train['x'], X_train['class'], label='data')\n",
        "# plt.plot(x, 1 / (1 + np.exp(-(np.mean(x)/s + (1/s)*x))), color='red', label='sigmoid')\n",
        "# plt.axhline(.5, color='green', linestyle='dashed', label='threshold')\n",
        "# plt.axvline(0, color='gray', alpha=0.5)\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26f380ec",
      "metadata": {
        "id": "26f380ec"
      },
      "source": [
        "### Probability\n",
        "\n",
        "Probability = observed outcomes/possible outcomes\n",
        "```\n",
        "P(y=1 | β₀ + βᵢΧᵢ) = f(β₀ + βᵢΧᵢ)\n",
        "```\n",
        "The probability of y = 1 is determined by a threshold and in this equation, x is unbounded, $(-\\infty, +\\infty)$, on the x axis, but y is either 0 or 1, and not very informative in regards to x and our threshold. With binary classification we want to show a better relationship with x.\n",
        "\n",
        "### What is this Thing Called Threshold?\n",
        "\n",
        "Logistic regression returns a probability. You can use the returned probability \"as is\" (for example, the probability that the user will click on this ad is 0.00023) or convert the returned probability to a binary value (for example, this email is spam).\n",
        "\n",
        "A logistic regression model that returns 0.9995 for a particular email message is predicting that it is very likely to be spam. Conversely, another email message with a prediction score of 0.0003 on that same logistic regression model is very likely not spam. However, what about an email message with a prediction score of 0.6? In order to map a logistic regression value to a binary category, you must define a classification threshold (also called the decision threshold). A value above that threshold indicates \"spam\"; a value below indicates \"not spam.\" It is tempting to assume that the classification threshold should always be 0.5, but thresholds are problem-dependent, and are therefore values that you must tune.\n",
        "\n",
        "https://developers.google.com/machine-learning/crash-course/classification/thresholding\n",
        "\n",
        "### Formula\n",
        "\n",
        "$P(y=1 \\mid X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\dots + \\beta_nX_n)}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d229df66",
      "metadata": {
        "id": "d229df66"
      },
      "source": [
        "**Review our line of best fit given x and y**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# # Sample data where y is continuous and follows the line of best fit with some noise\n",
        "# np.random.seed(42)\n",
        "# num_samples = 100\n",
        "# X_train = pd.DataFrame({'x': np.random.randn(num_samples) * 2})  # Wider range for x\n",
        "# true_slope = 0.8\n",
        "# true_intercept = 0.2\n",
        "# noise = np.random.randn(num_samples) * 0.5  # Adjust noise for desired scatter\n",
        "# y_train = true_intercept + true_slope * X_train['x'] + noise\n",
        "\n",
        "# # Fit a linear regression model\n",
        "# model = LinearRegression()\n",
        "# model.fit(X_train, y_train)\n",
        "\n",
        "# print('\\nx, y scatter plot with line of best fit as per our line equation')\n",
        "# print(f'y = {model.intercept_:.2f} + {model.coef_[0]:.2f}X')\n",
        "\n",
        "# plt.scatter(X_train['x'], y_train, label='data')\n",
        "# plt.plot(X_train['x'], model.intercept_ + model.coef_[0] * X_train['x'], color='red', label='fitted line')\n",
        "# plt.axhline(0.5, color='yellow', linestyle='dashed', label='threshold')\n",
        "\n",
        "# # Find the x-intercept of the fitted line and the threshold (y=0.5)\n",
        "# x_intercept = (0.5 - model.intercept_) / model.coef_[0] if model.coef_[0] != 0 else np.nan\n",
        "\n",
        "# # Define the sigmoid function\n",
        "# def sigmoid(x, center, slope):\n",
        "#     return 1 / (1 + np.exp(-slope * (x - center)))\n",
        "\n",
        "# # Generate x values for plotting the sigmoid\n",
        "# x_min = X_train['x'].min() - 0.5\n",
        "# x_max = X_train['x'].max() + 0.5\n",
        "# x_sigmoid = np.linspace(x_min, x_max, 100)\n",
        "\n",
        "# # Adjust the slope (s) for visual fit\n",
        "# s = 2  # Experiment with this value\n",
        "\n",
        "# # Plot the sigmoid centered at the intersection\n",
        "# if not np.isnan(x_intercept):\n",
        "#     plt.plot(x_sigmoid, sigmoid(x_sigmoid, x_intercept, s), color='green', label='sigmoid')\n",
        "#     plt.scatter(x_intercept, 0.5, color='purple', marker='o', label='intersection/sigmoid center')\n",
        "# else:\n",
        "#     plt.plot(x_sigmoid, [0.5] * len(x_sigmoid), color='green', linestyle='--', label='sigmoid (no intersection)')\n",
        "\n",
        "# plt.xlabel('x')\n",
        "# plt.ylabel('y')\n",
        "# plt.legend()\n",
        "# plt.title('Scatter Plot with Continuous Data, Fitted Line, Threshold, and Centered Sigmoid')\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "HQCO94LiGiBa"
      },
      "id": "HQCO94LiGiBa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "35b645fe",
      "metadata": {
        "id": "35b645fe"
      },
      "source": [
        "### Odds\n",
        "\n",
        "1.  **Model the probability (p) using the sigmoid function:**\n",
        "    ```\n",
        "    p = 1 / (1 + e^(-(β₀ + β₁X + ... + βₚXₚ)))\n",
        "    ```\n",
        "\n",
        "2.  **Calculate the odds:**\n",
        "    ```\n",
        "    Odds = p / (1 - p)\n",
        "    ```\n",
        "    Substituting the formula for p:\n",
        "    ```\n",
        "    Odds = [1 / (1 + e^(-(β₀ + β₁X)))] / [1 - 1 / (1 + e^(-(β₀ + β₁X)))]\n",
        "    Odds = e^(β₀ + β₁X + ... + βₚXₚ)\n",
        "    ```\n",
        "\n",
        "3.  **Take the natural logarithm of the odds (the log-odds or logit):**\n",
        "    ```\n",
        "    ln(Odds) = ln(p / (1 - p)) = β₀ + β₁X + ... + βₚXₚ\n",
        "    ```\n",
        "\n",
        "**Therefore, the formula for the odds itself is simply:**\n",
        "\n",
        "**Odds = p / (1 - p)**\n",
        "\n",
        "The equation `p / (1 - p) = β₀ + β₁X` represents a model where the odds are directly linearly related to the predictor, which is not the standard formulation of logistic regression. The log-odds are what are modeled linearly.\n",
        "\n",
        "Odds are positive so we have a range of 0, $+ \\infty$ so we use log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13ce34c8",
      "metadata": {
        "id": "13ce34c8"
      },
      "outputs": [],
      "source": [
        "# # https://www.statisticshowto.com/log-odds/\n",
        "# import pandas as pd\n",
        "# import math\n",
        "\n",
        "# print('Defining odds based on a probability represented by a decimals')\n",
        "# d = {'Probs': [.1, .2, .3, .4, .5, .6, .7, .8, .9]}\n",
        "# probs = pd.DataFrame(d)\n",
        "# probs['Odds'] = probs['Probs']/(1 - probs['Probs'])\n",
        "# probs.head(9)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4f74cd1",
      "metadata": {
        "id": "e4f74cd1"
      },
      "source": [
        "### Log Odds\n",
        "\n",
        "We use the log of odds so that we can get to $(-\\infty, +\\infty)$\n",
        "\n",
        "$\\large{log(\\frac{p}{1 - p}}) = \\beta_0 + \\beta_i X_i$\n",
        "\n",
        "Now our range is unrestricted $- \\infty, + \\infty$ where the odds for and odds against meet at 0 with potential symmetrical range 0 to infinity. See images at https://towardsdatascience.com/https-towardsdatascience-com-what-and-why-of-log-odds-64ba988bf704\n",
        "\n",
        "In binomial logistic regression, the dependent variable takes only two values, which can be coded 0 and 1. So, it can't be that $Y=b0+b1X$ because that could result in any value. The logit transformation solves this problem.\n",
        "\n",
        "We take the odds to make the value continuous. We take the odds ratio to get a parameter estimate and we take the log of that ratio to make the variable range from negative infinity to infinity and be symmetric around 0 instead of 1.\n",
        "\n",
        "Peter Flom (https://stats.stackexchange.com/users/686/peter-flom), Why $\\log(\\frac{p}{1-p}) = b_0+b_1x $ in Logistic regression, URL (version: 2017-09-25): https://stats.stackexchange.com/q/304844"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3d33c98",
      "metadata": {
        "id": "a3d33c98"
      },
      "outputs": [],
      "source": [
        "# # log odds example\n",
        "# import math\n",
        "\n",
        "# p = .2\n",
        "# odds = p/(1-p)\n",
        "# print('.2/.8 =', odds)\n",
        "# print('log(.25) =', math.log(odds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef2ce66a",
      "metadata": {
        "id": "ef2ce66a"
      },
      "outputs": [],
      "source": [
        "# # add log odds to our probs dataframe\n",
        "# probs['Log Odds'] = probs['Odds'].apply(lambda x: math.log(x))\n",
        "# probs.head(9)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From Lines to Likelihood\n",
        "\n",
        "**Linear regression** is a powerful tool for modeling the relationship between a continuous outcome variable and one or more predictor variables. We've explored how to find the best-fitting line (or hyperplane) to our data using the equation:\n",
        "\n",
        "```\n",
        "y = β₀ + β₁Χ₁ + β₂Χ₂ + ... + βᵢΧᵢ + ε\n",
        "```\n",
        "\n",
        "Here, `y` is our continuous outcome, `xᵢ` are our predictors, `βᵢ` are the coefficients that quantify the relationship, and `ε` represents the error term. A key characteristic of linear regression is that the predicted outcome `y` can take on any real number – positive, negative, or zero, extending to infinity in both directions.\n",
        "\n",
        "**The Challenge of Binary Outcomes**\n",
        "\n",
        "Now, let's consider a different type of outcome variable: one that is **binary**. This means the outcome can only take on one of two values, such as:\n",
        "\n",
        "* Success or Failure\n",
        "* Yes or No\n",
        "* 0 or 1\n",
        "\n",
        "Our goal is to model the **probability** of one of these outcomes occurring, say `P(y = 1)`.\n",
        "\n",
        "**The Naive Linear Approach**\n",
        "\n",
        "Let's begin by trying to make both sides of the following regression equation make sense:\n",
        "\n",
        "```\n",
        "P(y = 1) | β₀ + βᵢΧᵢ\n",
        "```\n",
        "\n",
        "**Probabilities must always be between 0 and 1 (inclusive)**. However, the output of the linear equation $β₀ + β_iX_i$, just like the `y` in standard linear regression, can take on any real number. This means our \"predicted probability\" could easily fall below 0 or rise above 1, which are not valid probability values.\n",
        "\n",
        "**The Need for a Transformation: Introducing the Link Function**\n",
        "\n",
        "This limitation highlights the need for a way to transform the unbounded output of our linear combination of predictors into a value that is always neatly confined within the 0 to 1 range. This is where the concept of a **link function** comes into play. In logistic regression, the link function we use involves the **odds** and the **logit**.\n",
        "\n",
        "**Step 1: Understanding the Odds**\n",
        "\n",
        "Before we get to the logit, let's first understand **odds**. The odds of an event occurring are defined as the ratio of the probability of the event occurring to the probability of it not occurring:\n",
        "\n",
        "```\n",
        "Odds = P(y = 1) / P(y = 0) = p / (1 - p)\n",
        "```\n",
        "\n",
        "The odds have a range from 0 (when the probability is 0) to infinity (as the probability approaches 1). While bounded below by zero, they are still unbounded above, making them not ideal for direct linear modeling.\n",
        "\n",
        "**Step 2: The Logit Transformation**\n",
        "\n",
        "To address the unbounded nature of the odds, we apply the **logit transformation**, which is the natural logarithm of the odds:\n",
        "\n",
        "```\n",
        "logit(p) = ln(p / (1 - p))\n",
        "```\n",
        "\n",
        "The beauty of the logit function is that it transforms the range of the odds ([0, ∞)) to the entire real number line (-∞, ∞). This unbounded range now perfectly aligns with the possible output of our linear combination of predictors.\n",
        "\n",
        "**Step 3: Modeling the Logit Linearly**\n",
        "\n",
        "The core assumption of logistic regression is that the **logit of the probability** is linearly related to the predictor variables:\n",
        "\n",
        "```\n",
        "ln(p / (1 - p)) = β₀ + βᵢΧᵢ\n",
        "```\n",
        "\n",
        "Here, we see the familiar linear combination of predictors on the right-hand side. We are essentially using a linear model to predict the *transformed* probability (the log-odds).\n",
        "\n",
        "**Step 4: Deriving the Logistic Function**\n",
        "\n",
        "Our ultimate goal is to model the probability `p` directly. To do this, we need to reverse the logit transformation. Let's follow the steps:\n",
        "\n",
        "1.  **Exponentiate both sides:**\n",
        "    ```\n",
        "    e^[ln(p / (1 - p))] = e^(β₀ + βᵢΧᵢ)\n",
        "    ```\n",
        "\n",
        "2.  **Simplify the left side:**\n",
        "    ```\n",
        "    p / (1 - p) = e^(β₀ + βᵢΧᵢ)\n",
        "    ```\n",
        "\n",
        "3.  **Solve for p:**\n",
        "    ```\n",
        "    p = (1 - p) * e^(β₀ + βᵢΧᵢ)\n",
        "    p = 1 * e^(β₀ + βᵢΧᵢ) - p * e^(β₀ + βᵢΧᵢ)\n",
        "    p + p * e^(β₀ + βᵢΧᵢ) = e^(β₀ + βᵢΧᵢ)\n",
        "    p * (1 + e^(β₀ + βᵢΧᵢ)) = e^(β₀ + βᵢΧᵢ)\n",
        "    p = e^(β₀ + βᵢΧᵢ) / (1 + e^(β₀ + βᵢΧᵢ))\n",
        "    ```\n",
        "    or\n",
        "\n",
        "    ```\n",
        "    Let x = β₀ + βᵢΧᵢ\n",
        "\n",
        "    p = (1 - p) * e^x\n",
        "    p = e^x - p * e^x\n",
        "    p + p * e^x = e^x\n",
        "    p * (1 + e^x) = e^x\n",
        "    p = e^x / (1 + e^x)\n",
        "    ```\n",
        "\n",
        "4.  **Divide numerator and denominator by `e^(β₀ + βᵢΧᵢ)`:**\n",
        "    ```\n",
        "    p = [e^(β₀ + βᵢΧᵢ) / e^(β₀ + βᵢΧᵢ)] / [(1 / e^(β₀ + βᵢΧᵢ)) + (e^(β₀ + βᵢΧᵢ) / e^(β₀ + βᵢΧᵢ))]\n",
        "    ```\n",
        "    or\n",
        "    ```\n",
        "    p = [e^x / e^x] / [(1 / e^x) + (e^x / e^x)]\n",
        "    ```\n",
        "\n",
        "5.  **Simplify to the Logistic Function:**\n",
        "    ```\n",
        "    p = 1 / (e^(-(β₀ + βᵢΧᵢ)) + 1)\n",
        "    ```\n",
        "    or\n",
        "    ```\n",
        "    p = 1 / (e^(-(x)) + 1)\n",
        "    ```\n",
        "\n",
        "    Which is commonly written as:\n",
        "\n",
        "    ```\n",
        "    p = 1 / (1 + e^(-(β₀ + βᵢΧᵢ)))\n",
        "    ```\n",
        "    or\n",
        "    ```\n",
        "    p = 1 / (1 + e^(-x))\n",
        "    ```\n",
        "\n",
        "This final equation is the **logistic function**. Notice how it takes the linear combination of predictors (`β₀ + βᵢΧᵢ`) and transforms it through an exponential and a final division to produce a value of `p` that is always guaranteed to be between 0 and 1.\n",
        "\n",
        "$p = \\large{\\frac{1}{1 + e^{-x}}}$\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "Logistic regression, while used for binary outcomes, still retains the fundamental concept of a linear relationship between the predictors and a transformed version of the outcome (the log-odds). The logistic function acts as a crucial link, ensuring that our predicted probabilities are meaningful and within the valid range. By understanding this derivation, we can appreciate how logistic regression bridges the gap between linear modeling and the world of binary outcomes."
      ],
      "metadata": {
        "id": "GEAiPeVdxhRG"
      },
      "id": "GEAiPeVdxhRG"
    },
    {
      "cell_type": "markdown",
      "id": "a9ab21a9",
      "metadata": {
        "id": "a9ab21a9"
      },
      "source": [
        "## Make Classification Example with 2 Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c95d264",
      "metadata": {
        "id": "8c95d264"
      },
      "outputs": [],
      "source": [
        "# # Create dataset\n",
        "# import pandas as pd\n",
        "# from sklearn.datasets import make_classification\n",
        "\n",
        "# n = 1000 # number of observations\n",
        "# f = 2 # number of features\n",
        "# inform = 2 # number of meaningful features\n",
        "\n",
        "# # unpack X and y\n",
        "# features, binary_class = make_classification(n_samples=n, n_features=f,\n",
        "#                                              n_informative=inform, n_redundant=0,\n",
        "#                                              n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# # Create a dataframe of the features and add the binary class (label, output)\n",
        "# df = pd.DataFrame(features)\n",
        "# df.columns = ['Feature_1', 'Feature_2']\n",
        "# df['Class'] = binary_class\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff77ac97",
      "metadata": {
        "id": "ff77ac97"
      },
      "source": [
        "### Logistic Model\n",
        "\n",
        "In statistics, the logistic model (or logit model) is a statistical model that models the probability of an event taking place by having the log-odds for the event be a linear combination of one or more independent variables. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (the coefficients in the linear combination).\n",
        "\n",
        "* $p = \\large{\\frac{1}{1 + e^{-(\\beta_0 + \\beta_ix_i)}}}$\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Logistic_regression\n",
        "* https://en.m.wikipedia.org/wiki/Logistic_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a10fed6",
      "metadata": {
        "id": "3a10fed6"
      },
      "outputs": [],
      "source": [
        "# # get our parameters\n",
        "# from statsmodels.formula.api import logit\n",
        "\n",
        "# model = logit(formula='Class ~ Feature_1 + Feature_2', data=df).fit()\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1da9328a",
      "metadata": {
        "id": "1da9328a"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import math\n",
        "\n",
        "# print(model.params)\n",
        "# def logistic_function(row):\n",
        "#     b0 = model.params.Intercept\n",
        "#     b1 = model.params.Feature_1\n",
        "#     b2 = model.params.Feature_2\n",
        "\n",
        "#     x1 = row['Feature_1']\n",
        "#     x2 = row['Feature_2']\n",
        "\n",
        "#     return 1 / (1 + np.exp(-(b0+(b1*x1)+(b2*x2))))\n",
        "\n",
        "# df['p'] = df.apply(logistic_function, axis=1)\n",
        "# df['Predicted'] =  df['p'].apply(lambda x: 0 if x < .5 else 1)\n",
        "# df['Odds'] = df['p']/(1 - df['p'])\n",
        "# df['Log Odds'] = df['Odds'].apply(lambda x: math.log(x))\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dac3a137",
      "metadata": {
        "id": "dac3a137"
      },
      "source": [
        "### Logit Function\n",
        "\n",
        "A Logit function, the inverse of the logistic sigmoid, also known as the log-odds function, is a function that represents probability values from 0 to 1, and negative infinity to infinity.\n",
        "\n",
        "* https://deepai.org/machine-learning-glossary-and-terms/logit\n",
        "* https://en.m.wikipedia.org/wiki/Logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dce74501",
      "metadata": {
        "id": "dce74501"
      },
      "outputs": [],
      "source": [
        "# # logit function - log odds over p\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# df.plot.scatter(x='p', y='Log Odds', label='Logit Function', c=df['Log Odds'])\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8777d0fd",
      "metadata": {
        "id": "8777d0fd"
      },
      "source": [
        "### Logit vs Sigmoid\n",
        "\n",
        "The inverse of the logit curve is the inverse-logit or sigmoid function. The sigmoid function transforms the numbers ( -∞ to +∞ ) back to values between 0 and 1.\n",
        "\n",
        "* https://ajaytech.co/python-logistic-regression/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1848068d",
      "metadata": {
        "id": "1848068d"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# df.plot.scatter(x='Log Odds', y='p', marker='.', s=.5, label='Sigmoid Function', c=df['Log Odds'])\n",
        "# plt.axhline(y=.5, color='red', linestyle='dashed', label='Threshold')\n",
        "# plt.axvline(x=0, color='green', linestyle='dashed', label='Sigmoid Midpoint')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b713fcc",
      "metadata": {
        "id": "6b713fcc"
      },
      "source": [
        "* Logistic regression algorithm: $f(x) = \\large{\\frac{1}{1 + e^{-x}}}$\n",
        "* x = $\\beta_0 + \\beta_1X_1 + \\beta_2X_2$\n",
        "* If $x > 0$ then $\\frac{1}{1 + e^{-x}} > .5$\n",
        "* If $x < 0$ then $\\frac{1}{1 + e^{-x}} < .5$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a337a642",
      "metadata": {
        "id": "a337a642"
      },
      "source": [
        "## The Confusion Matrix\n",
        "\n",
        "Suppose we have some tabular data and each observation is labeled either a 0 or 1. Our goal is to create a model and predict whether new data will be labelled a 0 or a 1.\n",
        "\n",
        "https://www.researchgate.net/profile/Amr-Alfayoumy/publication/350588424/figure/fig6/AS:1008141403693073@1617371049997/A-confusion-matrix-as-represented-in-sklearn.ppm\n",
        "\n",
        "**True Positive (TP)**: An outcome that is labeled 1 and in reality is a 1\n",
        "\n",
        "**False Positive (FP)**: An outcome that labeled 1 and in reality is a 0\n",
        "\n",
        "**True Negative (TN)**: An outcome that is labeled 0 and in reality is a 0\n",
        "\n",
        "**False Negative (FN)**: An outcome that is labeled 0 and in reality is a 1\n",
        "\n",
        "**Accuracy**: (TP + TN) / (TP + FP + TN + FN)\n",
        "\n",
        "**Precision**: TP / (TP + FP), a measure of quality\n",
        "\n",
        "**Recall**: TP / (TP + FN), a measure of quantity\n",
        "\n",
        "### Type I and Type II Errors\n",
        "\n",
        "https://towardsdatascience.com/statistics-for-data-scientists-f2456d26c5a5\n",
        "\n",
        "Type I Error means rejecting the True null hypothesis and accepting the alternate hypothesis. It is a false positive. A smoke detector detecting smoke when there is no smoke.\n",
        "Type II Error means accepting the null hypothesis when an alternate hypothesis is true. It is a false negative. When fire alarm fails to detect fire.\n",
        "\n",
        "### Alpha and Beta Level\n",
        "\n",
        "Alpha level is also known as the significance level, α. It is the probability of making the wrong decision,i.e., rejecting the null hypothesis when it is true. Low alpha levels mean that we need strong evidence to be able to reject the null hypothesis. This means that alpha level of 1% requires more statistical evidence than 5%.\n",
        "\n",
        "Why is 0.05 (5%) used as significance level so commonly?\n",
        "\n",
        "The smaller the alpha level, the little is the chance to reject True Null hypothesis. And, also the smaller is the chance to reject the False null hypothesis, because of the tiny area. The more you try to avoid Type I Error, the more you are likely to make a Type II Error.\n",
        "\n",
        "A confidence level is 1 - α, which means accepting the null hypothesis when it is True.\n",
        "Beta level means the probability of making a Type II Error, i.e., rejecting the Alternate hypothesis when it is True.\n",
        "\n",
        "### Confustion Matrix\n",
        "\n",
        "In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix)... It is a special kind of contingency table, with two dimensions (\"actual\" and \"predicted\"), and identical sets of \"classes\" in both dimensions (each combination of dimension and class is a variable in the contingency table). They provide our True Negatives, False Positives, False Negatives, and True Positives, from which we derive Accuracy, Precision, and Recall.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Confusion_matrix\n",
        "\n",
        "**Note**: Because of the of the output sklearn provides, we will structure our table different from what Wikipedia says."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08213710",
      "metadata": {
        "id": "08213710"
      },
      "source": [
        "Confused? Please be aware of the distinction. In this course we will be using the sklearn matrix. There is a way to change the rows and columns to match Wikipedia's matrix by adding labels: confusion_matrix(y_test, predictions, labels=\\[1, 0\\])."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e88e170c",
      "metadata": {
        "id": "e88e170c"
      },
      "outputs": [],
      "source": [
        "# # Create dataset\n",
        "# n = 1000 # number of observations\n",
        "# f = 2 # number of features\n",
        "# inform = 2 # number of meaningful features\n",
        "# features, binary_class = make_classification(n_samples=n, n_features=f,\n",
        "#                                              n_informative=inform, n_redundant=0,\n",
        "#                                              n_clusters_per_class=1, random_state=13)\n",
        "\n",
        "# # Create a dataframe of the features and add the binary class (label, output)\n",
        "# df = pd.DataFrame(features)\n",
        "# df.columns = ['Feature_1', 'Feature_2']\n",
        "# df['Target'] = binary_class\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a61283f",
      "metadata": {
        "id": "6a61283f"
      },
      "outputs": [],
      "source": [
        "# # X_train, X_test split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df.drop('Target', axis=1), df['Target'], test_size=0.20)\n",
        "\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# sc = StandardScaler()\n",
        "# X_train = sc.fit_transform(X_train)\n",
        "# X_test = sc.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66005c9b",
      "metadata": {
        "id": "66005c9b"
      },
      "outputs": [],
      "source": [
        "# # Create model\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# model = LogisticRegression(solver='liblinear')\n",
        "# model.fit(X_train,y_train)\n",
        "# predictions = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66b7e244",
      "metadata": {
        "id": "66b7e244"
      },
      "source": [
        "**The Solver (liblinear)**: Solvers are used to minimize the loss function which in turn provides our best coefficients. Please see the documentation to learn about the five different solvers.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d6c34b5",
      "metadata": {
        "id": "0d6c34b5"
      },
      "outputs": [],
      "source": [
        "# # View confusion matrix\n",
        "# from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, classification_report\n",
        "\n",
        "# tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
        "# print('Confusion Matrix')\n",
        "# print(confusion_matrix(y_test, predictions))\n",
        "# print()\n",
        "# print('Here is what the numbers represent from the sklLearn output')\n",
        "# print('''\n",
        "#                  predicted\n",
        "#                    |  0  |  1\n",
        "#            --------------------\n",
        "#            class 0 | TN  |  FP\n",
        "#   actual   --------------------\n",
        "#            class 1 | FN  |  TP\n",
        "# ''')\n",
        "# print()\n",
        "# print('Here is what the numbers represent from the Wikipedia article')\n",
        "# print('''\n",
        "#                  actual\n",
        "#                    |  1  |  0\n",
        "#            --------------------\n",
        "#            class 1 | TP  |  FP\n",
        "# predicted  --------------------\n",
        "#            class 0 | FN  |  TN\n",
        "# ''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7f60915",
      "metadata": {
        "id": "f7f60915"
      },
      "outputs": [],
      "source": [
        "# # Compare with crosstab\n",
        "# pd.crosstab(y_test, predictions, rownames=['Actual'], colnames=['Predicted'],margins=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85920841",
      "metadata": {
        "id": "85920841"
      },
      "outputs": [],
      "source": [
        "# # Compare KDE plots for probabilities and normed probabilities\n",
        "# import seaborn as sns\n",
        "# import scipy\n",
        "\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "# fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5))\n",
        "\n",
        "# probs=model.predict_proba(X_test)[:,1]\n",
        "# sns.kdeplot(probs[y_test==0], shade=False, color='blue', label='Class 0', ax=ax1);\n",
        "# sns.kdeplot(probs[y_test==1], shade=False, color='red', label='Class 1', ax=ax1);\n",
        "# ax1.axvline(x=.5, color='black')\n",
        "# ax1.set_title('Probabilities')\n",
        "# ax1.set(xlabel='Probabilities', ylabel='Probability Density')\n",
        "\n",
        "# mean = probs[y_test==0].mean()\n",
        "# std = probs[y_test==0].std()\n",
        "# x = np.linspace(probs[y_test==0].min(), probs[y_test==0].max(), 100)\n",
        "# y = scipy.stats.norm.pdf(x,mean,std)\n",
        "# ax2.plot(x, y, color='blue', label='Class 0')\n",
        "\n",
        "# mean = probs[y_test==1].mean()\n",
        "# std = probs[y_test==1].std()\n",
        "# x = np.linspace(probs[y_test==1].min(), probs[y_test==1].max(), 100)\n",
        "# y = scipy.stats.norm.pdf(x,mean,std)\n",
        "# ax2.plot(x, y, color='red', label='Class 1')\n",
        "\n",
        "# x_threshold = 0.5\n",
        "# ax2.axvline(x=x_threshold,color='black')\n",
        "\n",
        "# ax2.set_title('Probabilities (Normalized)')\n",
        "# ax2.set(xlabel='Probabilities', ylabel='Probability Density')\n",
        "\n",
        "# ax1.legend(loc='upper center')\n",
        "# ax2.legend(loc='upper center')\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55fa2341",
      "metadata": {
        "id": "55fa2341"
      },
      "outputs": [],
      "source": [
        "# # visualize TN, FN, FP, TP\n",
        "# import math\n",
        "# import matplotlib.patches as mpatches\n",
        "\n",
        "# x_threshold = 0.5\n",
        "# tn, fp, fn, tp = confusion_matrix(y_test,predictions).ravel()\n",
        "\n",
        "# mean_0 = probs[y_test==0].mean()\n",
        "# std_0 = probs[y_test==0].std()\n",
        "# x_0 = np.linspace(probs[y_test==0].min(), probs[y_test==0].max(), 100)\n",
        "# y_0 = scipy.stats.norm.pdf(x_0,mean_0,std_0)\n",
        "\n",
        "# mean_1 = probs[y_test==1].mean()\n",
        "# std_1 = probs[y_test==1].std()\n",
        "# x_1 = np.linspace(probs[y_test==1].min(), probs[y_test==1].max(), 100)\n",
        "# y_1 = scipy.stats.norm.pdf(x_1,mean_1,std_1)\n",
        "\n",
        "# plt.plot(x_0, y_0, color='black')\n",
        "# plt.fill_between(x_0, y_0, color='blue', alpha=1)\n",
        "\n",
        "# plt.plot(x,y, color='black')\n",
        "# plt.fill_between(x_1, y_1, color='red', alpha=1)\n",
        "\n",
        "# ptx_0 = np.linspace(x_threshold, probs[y_test==0].max(), 100)\n",
        "# pty_0 = scipy.stats.norm.pdf(ptx_0,mean_0,std_0)\n",
        "# plt.fill_between(ptx_0, pty_0, color='yellow', alpha=1)\n",
        "\n",
        "# ptx_1 = np.linspace(probs[y_test==1].min(), x_threshold, 100)\n",
        "# pty_1 = scipy.stats.norm.pdf(ptx_1,mean_1,std_1)\n",
        "# plt.fill_between(ptx_1, pty_1, color='green', alpha=1)\n",
        "\n",
        "# tn_patch = mpatches.Patch(color='blue', label='True Negatives (TN): ' + str(tn))\n",
        "# fp_patch = mpatches.Patch(color='yellow', label='False Positives (FP): ' + str(fp))\n",
        "# fn_patch = mpatches.Patch(color='green', label='False Negatives (FN): ' + str(fn))\n",
        "# tp_patch = mpatches.Patch(color='red', label='True Positives (TP): ' + str(tp))\n",
        "# plt.legend(handles=[tn_patch,fp_patch,fn_patch,tp_patch], loc=2)\n",
        "\n",
        "# plt.title('Probability Distributions with Threshold',fontsize=10)\n",
        "# plt.axvline(x=x_threshold,color='black')\n",
        "# plt.xlabel('Value')\n",
        "# plt.ylabel('Density')\n",
        "\n",
        "# plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot directly illustrates the components of a confusion matrix in the context of classification, but we can draw parallels to the historical debate between Neyman-Pearson and Fisher regarding hypothesis testing.\n",
        "\n",
        "**The Core of the Neyman-Pearson vs. Fisher Debate (Simplified for the Connection):**\n",
        "\n",
        "* **Fisher's Approach:** Primarily focused on obtaining a p-value to assess the evidence against a single null hypothesis. The emphasis was on the strength of evidence provided by the data.\n",
        "* **Neyman-Pearson Approach:** Introduced the framework of decision-making between two specific hypotheses (null and alternative) and emphasized the control of error rates in repeated experiments. Key concepts included:\n",
        "    * **Type I Error (False Positive - α):** Rejecting a true null hypothesis.\n",
        "    * **Type II Error (False Negative - β):** Failing to reject a false null hypothesis.\n",
        "    * **Power (1 - β):** The probability of correctly rejecting a false null hypothesis.\n",
        "    * **Setting a Significance Level (α):** Pre-determining the acceptable probability of a Type I error.\n",
        "\n",
        "**Connecting the Plot to the Debate:**\n",
        "\n",
        "1.  **Framing Classification as a Decision:** You can frame the classification task as a decision-making process similar to hypothesis testing:\n",
        "    * **Null Hypothesis (H₀):** The instance belongs to the negative class.\n",
        "    * **Alternative Hypothesis (H₁):** The instance belongs to the positive class.\n",
        "    * **Decision Rule:** The classification threshold acts as the decision rule. If the predicted probability (or score) is above the threshold, we \"reject the null\" and classify as positive.\n",
        "\n",
        "2.  **Mapping Confusion Matrix to Error Types:** Now, directly connect the components of the confusion matrix to the error types in the Neyman-Pearson framework:\n",
        "    * **False Positives (FP):** These are analogous to **Type I errors (α)**. We incorrectly classify a negative instance as positive, which means we \"rejected the true null hypothesis.\" The yellow area in the plot represents the probability or proportion of Type I errors for the chosen threshold.\n",
        "    * **False Negatives (FN):** These are analogous to **Type II errors (β)**. We incorrectly classify a positive instance as negative, which means we \"failed to reject a false null hypothesis.\" The green area in the plot represents the probability or proportion of Type II errors for the chosen threshold.\n",
        "    * **True Positives (TP):** These relate to **Power (1 - β)**. We correctly classify a positive instance as positive, meaning we correctly rejected the false null hypothesis. The red area represents the probability or proportion of correct positive classifications.\n",
        "    * **True Negatives (TN):** These represent correctly failing to reject a true null hypothesis.\n",
        "\n",
        "3.  **The Role of the Threshold (α and β Control):** Emphasize how the choice of the classification threshold directly influences the balance between False Positives (Type I errors) and False Negatives (Type II errors).\n",
        "    * **Lowering the threshold (moving it to the left):** Increases the number of positive predictions, leading to more True Positives but also more False Positives (higher α). It decreases False Negatives (lower β, higher power).\n",
        "    * **Raising the threshold (moving it to the right):** Decreases the number of positive predictions, leading to more True Negatives but also more False Negatives (higher β, lower power). It decreases False Positives (lower α).\n",
        "\n",
        "4.  **Connecting to Fisher's Perspective (Less Direct):** You can briefly mention that Fisher's emphasis on the p-value could be seen as providing a measure of evidence against the null hypothesis for a single instance. However, the Neyman-Pearson framework, as illustrated by the control of error rates (α and β) across many decisions (or classifications), provides a more direct link to the concepts visualized in the confusion matrix and the trade-offs involved in setting a decision threshold.\n",
        "\n",
        "**Example Analogy:**\n",
        "\n",
        "\"Think of classifying emails as spam (positive) or not spam (negative).\n",
        "\n",
        "* A **False Positive** (yellow area) is like classifying a legitimate email as spam (Type I error - rejecting a true 'not spam' hypothesis).\n",
        "* A **False Negative** (green area) is like classifying a spam email as legitimate (Type II error - failing to reject a false 'not spam' hypothesis).\n",
        "\n",
        "Just like in the Neyman-Pearson framework where we set a significance level (α) to control the rate of Type I errors, in classification, we might adjust our threshold to minimize False Positives if the cost of missing a legitimate email is high.\"\n",
        "\n",
        "**Key Takeaway:**\n",
        "\n",
        "By drawing this analogy, you can show that the fundamental considerations of error types and decision thresholds are not unique to formal hypothesis testing but are also central to the practical task of classification. The plot provides a visual and intuitive way to understand these trade-offs in action.\n",
        "\n",
        "While the plot isn't a direct visualization of the p-value concept from Fisher's approach, it beautifully illustrates the core ideas of decision rules and error control that were central to the Neyman-Pearson framework and are fundamental to understanding the performance of classification models."
      ],
      "metadata": {
        "id": "NKNNdrCU7ADz"
      },
      "id": "NKNNdrCU7ADz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GX253zVD9t2Y"
      },
      "outputs": [],
      "source": [
        "# # Compare with crosstab\n",
        "# pd.crosstab(y_test, predictions, rownames=['Actual'], colnames=['Predicted'],margins=True)"
      ],
      "id": "GX253zVD9t2Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Breaking down the values:**\n",
        "\n",
        "* **True Negatives (TN):** 90 (Predicted 0, Actual 0)\n",
        "* **False Positives (FP):** 10 (Predicted 1, Actual 0)\n",
        "* **False Negatives (FN):** 9 (Predicted 0, Actual 1)\n",
        "* **True Positives (TP):** 91 (Predicted 1, Actual 1)\n",
        "\n",
        "**Now we can revisit the 95% and alpha:**\n",
        "\n",
        "* **Total predicted as positive (to the right of the threshold in the plot):** TP + FP = 91 + 10 = 101\n",
        "* **False Positives (yellow area):** FP = 10\n",
        "\n",
        "The proportion of predictions that were positive and were actually negative (False Positives) out of all positive predictions is:\n",
        "\n",
        "FP / (TP + FP) = 10 / 101 ≈ 0.099 or **9.9%**\n",
        "\n",
        "So, your visual estimation was correct – it's nowhere near 95%.\n",
        "\n",
        "**Connecting to the Null Hypothesis and 1 - Alpha:**\n",
        "\n",
        "Let's assume the null hypothesis (H₀) is that an instance belongs to the negative class (Actual = 0).\n",
        "\n",
        "* **Alpha (α) - Probability of a Type I Error (False Positive Rate):** This is the probability of rejecting H₀ when it's true. In our case, it's the probability of predicting positive (1) when the actual class is negative (0). We can estimate this from the confusion matrix as:\n",
        "\n",
        "    `FPR = FP / (TN + FP) = 10 / (90 + 10) = 10 / 100 = 0.10` or **10%**\n",
        "\n",
        "    So, for the chosen threshold, the estimated probability of a Type I error (α) is 10%.\n",
        "\n",
        "* **1 - Alpha (Confidence Level or True Negative Rate - Specificity):** This is the probability of *not* rejecting H₀ when it's true, i.e., correctly predicting negative when the actual class is negative.\n",
        "\n",
        "    `TNR = TN / (TN + FP) = 90 / (90 + 10) = 90 / 100 = 0.90` or **90%**\n",
        "\n",
        "    So, the estimated value of 1 - alpha (or the specificity) is 90%.\n",
        "\n",
        "**Therefore, based on the provided (sort of) confusion matrix:**\n",
        "\n",
        "* The proportion of positive predictions that were actually negative (related to the yellow area) is about 9.9%.\n",
        "* The estimated probability of a Type I error (α) is 10%.\n",
        "* The estimated value of 1 - alpha (Specificity or True Negative Rate) is 90%.\n",
        "\n",
        "The visual impression from the plot aligns with these calculations – the False Positive area (yellow) appears smaller than the True Positive area (red) on the positive prediction side of the threshold."
      ],
      "metadata": {
        "id": "a6uLgQ7f9hi0"
      },
      "id": "a6uLgQ7f9hi0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Problem\n",
        "\n",
        "\n",
        "**The Disease Screening Problem**\n",
        "\n",
        "Imagine a new rapid test is developed to detect a **rare disease** in Denton, Texas. The **prevalence (base rate)** of this disease in the general population is very low: $\\mathbf{0.1\\%}$ (or 1 out of every 1,000 people).\n",
        "\n",
        "A large-scale study was conducted to determine the test's performance metrics:\n",
        "\n",
        "* **Sensitivity (True Positive Rate):** The ability of the test to correctly identify someone *with* the disease is $\\mathbf{91\\%}$.\n",
        "* **Specificity (True Negative Rate):** The ability of the test to correctly identify someone *without* the disease is $\\mathbf{90\\%}$.\n",
        "\n",
        "Now, suppose you go to a clinic in Denton, take this test, and your result comes back **positive**. **What is the actual probability that you have the disease?**\n",
        "\n",
        "***\n",
        "\n",
        "### Solving the Word Problem Using Base Rates (Prevalence)\n",
        "\n",
        "To find the probability of actually having the disease given a positive test result, we need to calculate the **Positive Predictive Value (PPV)**. We'll use a hypothetical population of $\\mathbf{10,000}$ people to make the calculations clear.\n",
        "\n",
        "#### Step 1: Establish the Actual Disease Status\n",
        "\n",
        "We first apply the $\\mathbf{0.1\\%}$ prevalence to the population:\n",
        "\n",
        "$$\\text{Actual Positives (Disease Present)} = 10,000 \\times 0.001 = \\mathbf{10 \\text{ people}}$$\n",
        "\n",
        "$$\\text{Actual Negatives (Disease Absent)} = 10,000 - 10 = \\mathbf{9,990 \\text{ people}}$$\n",
        "\n",
        "***\n",
        "\n",
        "#### Step 2: Calculate True Positives (TP) and False Positives (FP)\n",
        "\n",
        "Next, we see how the test performs on these two groups:\n",
        "\n",
        "#### A. True Positives (TP)\n",
        "\n",
        "These are the people who have the disease *and* test positive (based on $\\mathbf{91\\%}$ Sensitivity):\n",
        "\n",
        "$$\\text{TP} = \\text{Actual Positives} \\times \\text{Sensitivity}$$\n",
        "\n",
        "$$\\text{TP} = 10 \\times 0.91 = \\mathbf{9.1 \\text{ people}}$$\n",
        "\n",
        "##### B. False Positives (FP)\n",
        "\n",
        "These are the people who **do not** have the disease but still test positive. The probability of this error is $1 - \\text{Specificity}$, or $1 - 0.90 = \\mathbf{10\\%}$ (This is the False Positive Rate):\n",
        "\n",
        "$$\\text{FP} = \\text{Actual Negatives} \\times (1 - \\text{Specificity})$$\n",
        "\n",
        "$$\\text{FP} = 9,990 \\times (1 - 0.90)$$\n",
        "\n",
        "$$\\text{FP} = 9,990 \\times 0.10 = \\mathbf{999 \\text{ people}}$$\n",
        "\n",
        "***\n",
        "\n",
        "#### Step 3: Calculate the Positive Predictive Value (PPV)\n",
        "\n",
        "The $\\text{PPV}$ is the probability that you actually have the disease ($\\text{TP}$) out of the total number of positive test results ($\\text{TP} + \\text{FP}$):\n",
        "\n",
        "$$\\text{PPV} = P(\\text{Disease} \\mid \\text{Positive Test}) = \\frac{\\text{True Positives} (\\text{TP})}{\\text{True Positives} (\\text{TP}) + \\text{False Positives} (\\text{FP})}$$\n",
        "\n",
        "$$\\text{PPV} = \\frac{9.1}{9.1 + 999}$$\n",
        "\n",
        "$$\\text{PPV} = \\frac{9.1}{1,008.1}$$\n",
        "\n",
        "$$\\text{PPV} \\approx \\mathbf{0.00902} \\text{ or } \\mathbf{0.902\\%}$$\n",
        "\n",
        "***\n",
        "\n",
        "### Answer and Explanation\n",
        "\n",
        "Despite testing positive, your actual probability of having the disease is only approximately $\\mathbf{0.9\\%}$.\n",
        "\n",
        "This result highlights the critical role of **prevalence (the base rate)** when interpreting test results, especially for rare conditions:\n",
        "\n",
        "* Even though the test is $\\mathbf{90\\%}$ specific (meaning it's quite good at saying \"No\" when someone is healthy), the vast majority of the population (9,990 out of 10,000) are healthy.\n",
        "* This large healthy group, when subjected to the $\\mathbf{10\\%}$ chance of a false alarm, produces $\\mathbf{999}$ **False Positives**.\n",
        "* This number of False Positives ($\\mathbf{999}$) is far greater than the number of actual cases the test correctly identified ($\\mathbf{9.1}$ **True Positives**). Therefore, if you test positive, you are far more likely to be one of the healthy people with a false positive result than one of the few people who actually have the disease.\n"
      ],
      "metadata": {
        "id": "hwytyUGD-bwq"
      },
      "id": "hwytyUGD-bwq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the Base Rate?**\n",
        "\n",
        "The base rate is simply **how common something is in the general population** before we have any specific information or test results. It's the underlying probability of an event occurring. Think of it as the starting point or the prior probability.\n",
        "\n",
        "**The OJ Simpson Trial and Base Rates (A Simplified Explanation):**\n",
        "\n",
        "Yes, the defense in the OJ Simpson trial reportedly used an argument related to base rates. Here's a way to understand it:\n",
        "\n",
        "* **Defense Argument (Simplified):** The defense argued that the base rate of husbands who divorce their wives and then go on to murder them is very low. They suggested that just because OJ Simpson had a history of domestic issues with Nicole Brown Simpson and they had divorced, it didn't automatically mean he was likely to commit murder. They focused on the rarity of that sequence of events.\n",
        "\n",
        "* **Prosecution's (Alleged) Counter-Argument Weakness:** The prosecution, according to this account, didn't effectively counter this by providing the base rate of spouses (regardless of divorce history) being murdered by their current or former partners in situations involving a history of violence.\n",
        "\n",
        "**Why Base Rates Matter (Using a Different Example First):**\n",
        "\n",
        "Imagine a very rare disease that affects only 1 in 10,000 people in Denton. A new test for this disease is developed that is 99% accurate. If you take this test and it comes back positive, what's the probability you actually have the disease?\n",
        "\n",
        "Many people instinctively think it's very high (around 99%). But let's look at the numbers:\n",
        "\n",
        "* **Prevalence (Base Rate):** 1 out of 10,000 people have the disease (0.01%).\n",
        "* **Population (for example):** 10,000 people tested.\n",
        "* **True Positives (Test correctly identifies the sick person):** 1 (99% of 1 is about 1).\n",
        "* **False Positives (Test incorrectly identifies a healthy person as sick):** Out of the 9,999 healthy people, 1% will test positive (0.01 * 9999 ≈ 100).\n",
        "\n",
        "So, you have about 100 people testing positive who are healthy for every 1 person who tests positive and is actually sick. In this case, even with a very accurate test, the probability of actually having the disease after a positive result is quite low because the base rate of the disease is so rare.\n",
        "\n",
        "**Connecting Back to the OJ Simpson Case:**\n",
        "\n",
        "The defense was essentially arguing that the \"base rate\" of a divorced man with a history of domestic issues going on to commit murder was low. They wanted the jury to consider the rarity of that specific sequence of events.\n",
        "\n",
        "The prosecution, to counter this effectively using base rates, might have needed to argue:\n",
        "\n",
        "* What is the base rate of spouses (in general, or with a history of violence) being killed by their partners? This base rate might be higher than the very specific scenario the defense presented.\n",
        "* What is the increased risk of spousal homicide in situations involving a history of domestic violence, regardless of divorce?\n",
        "\n",
        "**In essence, the base rate provides crucial context.** Without knowing how common an event is in the first place, it's easy to misinterpret the significance of specific factors or pieces of evidence. The defense tried to leverage a seemingly low base rate for a very specific scenario, while the prosecution needed to ensure the jury considered the more relevant and potentially higher base rate of spousal homicide in contexts involving violence.\n",
        "\n",
        "The OJ Simpson case highlights the importance of considering the base rate when evaluating probabilities and evidence. Focusing on a very narrow and rare sequence of events (divorce followed by murder by the ex-spouse) might be misleading if the broader context (spousal violence leading to homicide) has a different underlying probability."
      ],
      "metadata": {
        "id": "cdBF1BE__5Ya"
      },
      "id": "cdBF1BE__5Ya"
    },
    {
      "cell_type": "markdown",
      "id": "de7fb3b2",
      "metadata": {
        "id": "de7fb3b2"
      },
      "source": [
        "### Underfitting and Overfitting\n",
        "\n",
        "In statistics, overfitting is \"the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably\". An overfitted model is a statistical model that contains more parameters than can be justified by the data.\n",
        "\n",
        "<img src='https://docs.aws.amazon.com/images/machine-learning/latest/dg/images/mlconcepts_image5.png' alt='underfitting overfitting' />\n",
        "\n",
        "https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html\n",
        "\n",
        "If you're overfitting, or you are getting great training scores and poor  test scores, you might be overfitting so try removing the lesser performing features. The model is just memorizing the training data.\n",
        "\n",
        "Underfitting occurs when a statistical model cannot adequately capture the underlying structure of the data. An under-fitted model is a model where some parameters or terms that would appear in a correctly specified model are missing.[2] Under-fitting would occur, for example, when fitting a linear model to non-linear data. Such a model will tend to have poor predictive performance.\n",
        "\n",
        "If you're underfitting, or you are getting poor training scores and test scores, you might be underfitting so try adding more data or more features.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "958ec329",
      "metadata": {
        "id": "958ec329"
      },
      "source": [
        "### Bias Variance Tradeoff\n",
        "\n",
        "In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the parameter estimates across samples can be reduced by increasing the bias in the estimated parameters. The bias–variance dilemma or bias–variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\n",
        "\n",
        "Please become familiar with this image:\n",
        "\n",
        "<img src='https://www.kdnuggets.com/wp-content/uploads/bias-and-variance.jpg' alt='bias variance tradeoff' />\n",
        "\n",
        "Please read the article responsible for the image:\n",
        "\n",
        "https://www.kdnuggets.com/2016/08/bias-variance-tradeoff-overview.html\n",
        "\n",
        "#### A Formula\n",
        "\n",
        "$E[(\\hat{y} - y)^2] = E[(\\hat{y} - E[\\hat{y})^2] + (E[\\hat{y}] - y)^2$\n",
        "<br />MSE = Variance + Bias^2\n",
        "* where y = given y, (ground truth)\n",
        "* $\\hat{y}$ = prediction\n",
        "* $E[\\hat{y}]$ = expected value or average of predictions\n",
        "* error = $\\frac{fp + fn}{total}$\n",
        "* Bias = error in training\n",
        "* Variance = range of predictions, how much an estimate of the target function will change if different data was is\n",
        "\n",
        "<table width='50%' style='margin-right: 100%'>\n",
        "    <tr>\n",
        "        <td>Training Error</td>\n",
        "        <td>1%</td>\n",
        "        <td>15%</td>\n",
        "        <td>16%</td>\n",
        "        <td>1%</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Testing Error</td>\n",
        "        <td>15%</td>\n",
        "        <td>16%</td>\n",
        "        <td>33%</td>\n",
        "        <td>1%</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td></td>\n",
        "        <td>lo bias</td>\n",
        "        <td>hi bias</td>\n",
        "        <td>hi bias</td>\n",
        "        <td>lo bias</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td></td>\n",
        "        <td>hi var</td>\n",
        "        <td>lo var</td>\n",
        "        <td>hi var</td>\n",
        "        <td>lo var</td>\n",
        "    </tr>\n",
        "</table>\n",
        "\n",
        "https://www.mastersindatascience.org/learning/difference-between-bias-and-variance/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6a4f31b",
      "metadata": {
        "id": "a6a4f31b"
      },
      "source": [
        "### The Classification Report for Imbalanced Data Sets\n",
        "\n",
        "Both the titanic and cancer datasets weren't as balanced as they could be but to appreciate the implications of an imbalanced dataset, consider a fraudualent credit card dataset where there may be a 1 (for fraudulent activity) per 1000 0s of normal activity. We wouldn't need to build a predictive model to make correct predictions 99% of the time. The data itself lets us do that. We need metrics that tell us that the model we build is learning about the relevant data and that it predicts what it is suppose to predict. This gets back to our Precision Recall Tradeoff.\n",
        "\n",
        "**Precision**: The sum of observations correctly classified as positive out of the total observations classified as positive. From the formula, tp / (tp + fp), we can see we want to mimimize fp (false positives), or type I errors, which is sounding an alarm when there is no fire.\n",
        "\n",
        "**Recall**: The sum of observations correctly classified as positive out of the total actual positives. From the forumula, tp / (tp + fn), we can see we want to minimize fn (false negatives), or type II errors, which is not sounding an alarm when there is a fire."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3691916",
      "metadata": {
        "id": "e3691916"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# print(confusion_matrix(y_test, predictions))\n",
        "# print(classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "240381d0",
      "metadata": {
        "id": "240381d0"
      },
      "source": [
        "* precision 0: 114 / 137\n",
        "* precision 1: 66 / 86\n",
        "* recall 0: 114 / 134\n",
        "* recall 1: 66 / 89\n",
        "\n",
        "We know about accuracy, precision, and recall and we've talked about f1-score as being the harmonic mean of precision and recall (f1 score = 2 * (precision * recall) / (precision + recall)).\n",
        "\n",
        "* support: the number of observations labeled 0 or 1 (remember the pie chart we made of the target?)\n",
        "* macro avg: the average between the two classes (e.g. (prec 0 + prec 1) / 2\n",
        "* weighted avg: the average between the two classes taking into account the number of their observations (e.g. ((prec0 * score0) + (prec 1 * score1)) / total)\n",
        "\n",
        "A classification report provides a little more information by providing metrics for each class. For example, for class 0, our recall is higher than our precision, which tells us that there are fewer false negatives than false positives in class 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2de2d1c",
      "metadata": {
        "id": "e2de2d1c"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "\n",
        "# print((.83 + .77) / 2)\n",
        "# wgt_exmpl = ((.83 * 134) + (.77 * 89)) / (134 + 89)\n",
        "# print(np.round(wgt_exmpl, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b04dcf2",
      "metadata": {
        "id": "8b04dcf2"
      },
      "source": [
        "### Precision Recall Tradeoff\n",
        "\n",
        "In pattern recognition, information retrieval and classification (machine learning), precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of relevant instances that were retrieved. Both precision and recall are therefore based on relevance... Accuracy can be a misleading metric for imbalanced data sets. Consider a sample with 95 negative and 5 positive values. Classifying all values as negative in this case gives 0.95 accuracy score.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Precision_and_recall\n",
        "\n",
        "In cases of imbalanced data, precision, recall, and f1 score become useful metrics. Precision tells us the proportion of data that was predicted was actually predicted correctly. Recall, also known as the True Positive Rate, is the probability that an actual positive will be predicted to be positive.\n",
        "\n",
        "Precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of relevant instances that were retrieved. Both precision and recall are therefore based on relevance.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Precision_and_recall\n",
        "\n",
        "<img src='https://miro.medium.com/v2/resize:fit:640/format:webp/1*GM10sFuFBAbhwBt2scuP1g.png' alt='precision recall tradeoff' />\n",
        "\n",
        "https://datascience-george.medium.com/the-precision-recall-trade-off-aa295faba140\n",
        "\n",
        "The following is a visualization of our true negatives, false positives, false negatives, and true positives from the data set generated with make_classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd34a890",
      "metadata": {
        "scrolled": true,
        "id": "bd34a890"
      },
      "outputs": [],
      "source": [
        "# tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
        "# print('Confusion Matrix')\n",
        "# print()\n",
        "# print(f'''\n",
        "#            survived predicted\n",
        "#                    |  0   |  1\n",
        "#            -----------------------  -----------------------------------------\n",
        "#            no      | {tn}  |  {fp}     fpr (type I error)  | tnr\n",
        "#   actual   -----------------------  -----------------------------------------\n",
        "#            yes     | {fn}   |  {tp}     tpr (recall)        | fnr (type II error)\n",
        "\n",
        "#                     for   | precision\n",
        "#            ----------------------------\n",
        "#                     npv   | fdr\n",
        "\n",
        "# ''')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11e9c355",
      "metadata": {
        "id": "11e9c355"
      },
      "source": [
        "### Metrics\n",
        "\n",
        "* tn = pred 0 actual 0\n",
        "* fp = pred 1 actual 0\n",
        "* fn = pred 0 actual 1\n",
        "* tp = pred 1 actual 1\n",
        "* acc(uracy) = $\\frac{tn + tp}{total}$\n",
        "* error = $\\frac{fp + fn}{total}$\n",
        "* prev(alence) = $\\frac{fn + tp}{total}$\n",
        "* queue = $\\frac{fp + tp}{total}$\n",
        "* tpr = $\\frac{tp}{tp + fn}$\n",
        "    * true positive rate\n",
        "    * recall\n",
        "    * sensitivity\n",
        "    * prob of detection\n",
        "    * 1 - fnr\n",
        "* fnr = $\\frac{fn}{tp + fn}$\n",
        "    * false negative rate\n",
        "    * type II error\n",
        "    * 1 - tpr\n",
        "* tnr = $\\frac{tn}{tn + fp}$\n",
        "    * true negative rate\n",
        "    * specificity\n",
        "    * 1 - fpr\n",
        "* fpr = $\\frac{fp}{tn + fp}$\n",
        "    * false positive rate\n",
        "    * type I error\n",
        "    * fall out\n",
        "    * prob of false claim\n",
        "    * 1 - tnr\n",
        "* ppv = $\\frac{tp}{tp + fp}$\n",
        "    * positive predicted value\n",
        "    * precision\n",
        "    * 1 - fdr\n",
        "* fdr = $\\frac{fp}{tp + fp}$\n",
        "    * false discovery rate\n",
        "    * 1 - ppv\n",
        "* npv = $\\frac{tn}{tn + fn}$\n",
        "    * negative predicted value\n",
        "    * 1 - for\n",
        "* for = $\\frac{fn}{tn + fn}$\n",
        "    * false omission rate\n",
        "    * 1 - npv\n",
        "* liklihood ratio+ (lr+) = $\\frac{tpr}{fpr}$\n",
        "    * roc\n",
        "* liklihood ratio- (lr-) = $\\frac{fnr}{tnr}$\n",
        "* diagnostic odds ratio = $\\frac{lr+}{lr-}$\n",
        "* f1 score = 2 * $\\frac{precision-recall}{precision+recall}$\n",
        "* Youden's J = sensitivity + specificity - 1 = tpr - fpr\n",
        "* Matthew's Correlation Coefficient = $\\frac{(tp*tn)-(fp*tp)}{\\sqrt{(tp+fp)(tp+fn)(tn+fp)(tn+fn)}}$\n",
        "  "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}