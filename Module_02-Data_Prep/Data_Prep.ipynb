{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOaUE1YEdt73ABP7850volE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC5502/blob/main/Module_02-Data_Prep/Data_Prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Prep\n"
      ],
      "metadata": {
        "id": "4SUvtSLX7Zcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "* Colab - get notebook from gitmystuff DTSC5502 repository\n",
        "* Save a Copy in Drive\n",
        "* Remove Copy of\n",
        "* Edit your name\n",
        "* Clean up Colab Notebooks folder\n",
        "* Submit shared link"
      ],
      "metadata": {
        "id": "tv86CANY3_Xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Prep Introduction\n",
        "\n",
        "* Types and characteristics of data\n",
        "* Duplicates\n",
        "* Constants\n",
        "* Quasi-Constants\n",
        "* Missing data\n",
        "* Outliers\n",
        "* Datetime\n",
        "* Feature creation\n",
        "* Discretization\n",
        "* Categorical encoding\n",
        "* Variable transformation\n",
        "* Scaling\n",
        "\n",
        "When performing data preparation, the order of operations is crucial for a robust and reproducible machine learning pipeline. While there isn't one single \"best\" order that fits every scenario, a logical and widely accepted sequence follows a general flow from broad data quality checks to more specific feature transformations. This ensures that early steps, like removing bad data, don't get skewed by later transformations.\n",
        "\n",
        "The general best-practice order is:\n",
        "\n",
        "1.  **Remove Bad Data and Redundancy:** Begin by identifying and removing data that's problematic at a fundamental level.\n",
        "    * **Duplicates**: Removing duplicate rows is the first step. They can bias a model and inflate performance metrics.\n",
        "    * **Constants**: Columns with only one unique value (constants) provide no information for a model to learn from. Removing them early reduces dimensionality and simplifies your dataset.\n",
        "    * **Quasi-Constants**: Similar to constants, these features have very little variance (e.g., 99% of values are the same). It's generally a good idea to remove them as they offer minimal predictive power.\n",
        "\n",
        "2.  **Handle Missing Data and Outliers:** Once your data is free of redundant rows and features, you can address quality issues within the remaining columns.\n",
        "    * **Missing Data**: Decide on a strategy for handling missing values. This can involve dropping rows or columns with too much missing data, or **imputing** values using methods like the mean, median, or more advanced techniques.\n",
        "    * **Outliers**: After addressing missing data, identify and handle outliers. Outliers can heavily influence the mean and other statistics, so it's important to deal with them before transformations or scaling.\n",
        "\n",
        "3.  **Perform Feature Engineering:** This is a creative and domain-specific step where you can extract more information from your existing features.\n",
        "    * **Datetime**: Extracting meaningful features from datetime columns, such as month, day of the week, or holiday flags, is a common and powerful technique. This should be done before more generic transformations.\n",
        "    * **Feature Creation**: This is the core of feature engineering, where you create new variables based on your domain knowledge. For example, creating a `BMI` feature from `height` and `weight`.\n",
        "\n",
        "4.  **Transform Variables and Features:** With a clean and feature-rich dataset, you can now prepare the variables for the specific requirements of your chosen model.\n",
        "    * **Discretization**: If needed, convert continuous numerical features into discrete bins. This is often used for algorithms that benefit from binned data, like certain tree-based models.\n",
        "    * **Categorical Encoding**: Convert all categorical features (which may have been `object` or `category` data types) into a numerical format. This is a non-negotiable step for most machine learning algorithms. The choice between one-hot encoding, label encoding, etc., depends on the nature of the data.\n",
        "\n",
        "5.  **Scale Features:** The final step before training a model is often scaling, which adjusts the range of numerical features.\n",
        "    * **Scaling**: This is especially important for distance-based algorithms like **K-nearest Neighbors** and models that use gradient descent, such as **Linear Regression** and **Neural Networks**. Common techniques are **Normalization** (Min-Max Scaling) and **Standardization** (Z-Score Scaling). This should be done **after** all other transformations to prevent data leakage and ensure the scaling is based on the final, prepared feature values.\n"
      ],
      "metadata": {
        "id": "p43IcKZ2H4TU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Data"
      ],
      "metadata": {
        "id": "RpWcbBh6rAKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seed the Project"
      ],
      "metadata": {
        "id": "FMVaAmuqAWhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def generate_user_seed():\n",
        "    # Get current time in nanoseconds (more granular)\n",
        "    nanoseconds = time.time_ns()\n",
        "\n",
        "    # Add a small random component to further reduce collision chances\n",
        "    random_component = random.randint(0, 1000)  # Adjust range as needed\n",
        "\n",
        "    # Combine them (XOR is a good way to mix values)\n",
        "    seed = nanoseconds ^ random_component\n",
        "\n",
        "    # Ensure the seed is within the valid range for numpy's seed\n",
        "    seed = seed % (2**32)  # Modulo to keep it within 32-bit range\n",
        "\n",
        "    return seed\n",
        "\n",
        "user_seed = generate_user_seed()\n",
        "print(user_seed)\n",
        "random_state = np.random.seed(user_seed)"
      ],
      "metadata": {
        "id": "YUMcsfNhXbRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Faker"
      ],
      "metadata": {
        "id": "_j1l2Lr8_bS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Faker"
      ],
      "metadata": {
        "id": "hViR5zhSGxq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_names=[\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"]"
      ],
      "metadata": {
        "id": "Qpa4mbcgKoEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create demographic data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "fake = Faker()\n",
        "\n",
        "output = []\n",
        "for x in range(1000):\n",
        "  binary = np.random.choice(['binary_1', 'binary_2'], p=[0.5, 0.5])\n",
        "  output.append({\n",
        "        'binary': binary, # sex assigned at birth\n",
        "        'given_name': fake.first_name_female() if binary=='binary_1' else fake.first_name_male(),\n",
        "        'datetime': fake.date_time_this_decade(),\n",
        "        'surname': fake.last_name(),\n",
        "        'date_of_birth': fake.date_of_birth(),\n",
        "        'day_of_week': fake.day_of_week(),\n",
        "        'phone_number': fake.phone_number(),\n",
        "        'email': fake.email(),\n",
        "        'address': fake.address(),\n",
        "        'city': fake.city(),\n",
        "        'state': np.random.choice(state_names),\n",
        "        'zipcode': fake.zipcode(),\n",
        "        })\n",
        "\n",
        "demographics = pd.DataFrame(output)\n",
        "demographics.head()"
      ],
      "metadata": {
        "id": "HqvkXvuWKbXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_address_v2(text):\n",
        "  \"\"\"\n",
        "  Parses an address string into street address, city, state, and zipcode.\n",
        "  This version handles potential variations in the input format.\n",
        "\n",
        "  Args:\n",
        "    text: The address string to parse.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary containing the parsed address components.\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    # Split the input into lines\n",
        "    lines = text.strip().split('\\n')\n",
        "\n",
        "    # Extract the street address from the first line\n",
        "    street_address = lines[0].strip()\n",
        "\n",
        "    # Extract the city, state, and zipcode from the second line\n",
        "    city_state_zip = lines[1].strip().split(',')\n",
        "    city = city_state_zip[0].strip()\n",
        "    state_zip = city_state_zip[1].strip().split()\n",
        "    state = state_zip[0].strip()\n",
        "    zipcode = state_zip[1].strip()\n",
        "\n",
        "    return {\n",
        "        'street_address': street_address,\n",
        "        'city': city,\n",
        "        'state': state,\n",
        "        'zipcode': zipcode\n",
        "    }\n",
        "\n",
        "  except (IndexError, ValueError):\n",
        "    return None\n",
        "\n",
        "# Example usage\n",
        "text = \"80974 Jeffrey Mountains\\nWest Benjamin, IL 82801\"\n",
        "address = parse_address_v2(text)\n",
        "\n",
        "if address:\n",
        "  print(address)\n",
        "else:\n",
        "  print(\"Could not parse the address.\")"
      ],
      "metadata": {
        "id": "ZraWvumX7WkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the parse_address function to each row of the 'address' column\n",
        "demographics[['nu_address', 'nu_city', 'nu_state', 'nu_zipcode']] = demographics['address'].apply(lambda x: pd.Series(parse_address_v2(x)))\n",
        "\n",
        "demographics.head()"
      ],
      "metadata": {
        "id": "dXjBEBKa7Ccq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop address, city, state, zipcode and rename nu_ etc.\n",
        "demographics.drop(['address', 'city', 'state', 'zipcode'], axis=1, inplace=True)\n",
        "demographics.rename(columns={'nu_address': 'address', 'nu_city': 'city', 'nu_state': 'state', 'nu_zipcode': 'zipcode'}, inplace=True)\n",
        "demographics.head()"
      ],
      "metadata": {
        "id": "x4Hz9Qpw71M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Independent Variable Correlated with Class"
      ],
      "metadata": {
        "id": "4A99Q0IknJCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def generate_feature(df, class_col, coeff, intercept):\n",
        "    \"\"\"\n",
        "    Generates normally distributed feature data for a logistic regression model.\n",
        "\n",
        "    Args:\n",
        "        df: The pandas DataFrame containing the class column.\n",
        "        class_col: The name of the class column (containing 0s and 1s).\n",
        "        coeff: The coefficient for the feature in the logistic regression model.\n",
        "        intercept: The intercept of the logistic regression model.\n",
        "\n",
        "    Returns:\n",
        "        A pandas Series containing the generated feature data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate probabilities based on the class\n",
        "    probs = np.random.rand(len(df))  # Initial random probabilities\n",
        "    probs = np.where(df[class_col] == 1, probs * 0.8 + 0.2, probs * 0.8)  # Adjust for class\n",
        "\n",
        "    # Apply the inverse logit (logit) function\n",
        "    logits = np.log(probs / (1 - probs))\n",
        "\n",
        "    # Calculate the feature values\n",
        "    feature_values = (logits - intercept) / coeff\n",
        "\n",
        "    return pd.Series(feature_values)\n",
        "\n"
      ],
      "metadata": {
        "id": "n3VmXvwBnRr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make Classification"
      ],
      "metadata": {
        "id": "kmOFYI7T_eoP"
      }
    },
    {
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def make_linear_y(row):\n",
        "  model = LogisticRegression()\n",
        "  model.fit(X, y)\n",
        "  coefficients = model.coef_\n",
        "  intercept = model.intercept_\n",
        "  f_of_x = intercept + coefficients[0][0]*row['informative_1'] + coefficients[0][1]*row['informative_2']\n",
        "  # print(f_of_x[0])\n",
        "  return f_of_x[0]\n",
        "\n",
        "# Set n_informative and n_redundant to values that sum to less than n_features\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, random_state=random_state)\n",
        "df = pd.DataFrame(X, columns=['informative_1', 'informative_2'])\n",
        "df = pd.concat([demographics, df], axis=1).reset_index(drop=True)\n",
        "\n",
        "df['target'] = df.apply(make_linear_y, axis=1) # an independent variable\n",
        "df['class'] = y # the dependent variable\n",
        "df['corr_feature_class'] = generate_feature(df, 'class', 0.5, -1)\n",
        "df.head()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "1UYr8YXrJkX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automation Functions\n",
        "\n",
        "1. gen_null(series, perc)\n",
        "2. gen_quasi_constants(primary_label, variation_percentage=.2, replace=False)\n",
        "3. gen_normal_data(mu=0, std=1, size=len(df))\n",
        "4. gen_uniform_data(size=len(df))\n",
        "5. gen_multivariate_normal_data(mean=[0, 0], cov=[[1, 0], [0, 1]], size=len(df))\n",
        "6. gen_correlated_normal_series(original_series, target_correlation, size=len(df))\n",
        "7. gen_correlated_uniform_series(original_series, correlation_coefficient=0, size=len(df))\n",
        "8. gen_outliers(mean=0, std_dev=1, size=len(df), outlier_percentage=0.1, outlier_magnitude=3)\n",
        "9. gen_standard_scaling(mean=50, std_dev=10, size=len(df), scale_factor=1000)\n",
        "10. gen_minmax_scaling(mean=50, std_dev=10, size=len(df), range_factor=10)\n",
        "11. random_choice_data(choices, size)"
      ],
      "metadata": {
        "id": "FlnsQrSN8FLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "\n",
        "def gen_null(series, perc):\n",
        "  \"\"\"\n",
        "  Introduces null values (np.nan) into a list based on a specified percentage.\n",
        "\n",
        "  Args:\n",
        "      var: The variable to modify.\n",
        "      perc: The percentage of values to replace with nulls (0-100).\n",
        "\n",
        "  Returns:\n",
        "      The modified variable with null.\n",
        "  \"\"\"\n",
        "  var = series.copy()\n",
        "  num_nulls = int(len(var) * (perc / 100))\n",
        "  indices_to_replace = np.random.choice(len(var), num_nulls, replace=False)\n",
        "\n",
        "  for idx in indices_to_replace:\n",
        "      var[idx] = np.nan\n",
        "\n",
        "  return var\n",
        "\n",
        "def gen_quasi_constants(primary_label, variation_percentage=.2, size=len(df)):\n",
        "  \"\"\"\n",
        "  Generates quasi-constant labels for a Series, with a small percentage of variation.\n",
        "\n",
        "  Args:\n",
        "      primary_label: The main label to use for most values.\n",
        "      variation_percentage: The percentage of labels to vary (0-100).\n",
        "\n",
        "  Returns:\n",
        "      A new Series containing the quasi-constant labels.\n",
        "  \"\"\"\n",
        "\n",
        "  series = pd.Series(np.full(size, primary_label))\n",
        "  num_variations = int(size * (variation_percentage / 100))\n",
        "  variation_indices = np.random.choice(series.index, num_variations, replace=False)\n",
        "  primary_label = primary_label + '_0'\n",
        "  variation1 = primary_label + '_1'\n",
        "  variation2 = primary_label + '_2'\n",
        "\n",
        "  labels = pd.Series([primary_label] * len(series), index=series.index)\n",
        "  labels.loc[variation_indices] = np.random.choice([variation1, variation2], size=num_variations)  # Adjust variations as needed\n",
        "\n",
        "  return labels\n",
        "\n",
        "def gen_normal_data(mu=0, std=1, size=len(df)):\n",
        "  \"\"\"\n",
        "  Generates a normal dataset given the mean and standard deviation\n",
        "\n",
        "  Args:\n",
        "        mu: The mean of the normal distribution.\n",
        "        std: The standard deviation of the normal distribution.\n",
        "        size: The number of data points to generate.\n",
        "\n",
        "  Returns:\n",
        "        A normally distributed series.\n",
        "  \"\"\"\n",
        "  return np.random.normal(mu, std, size)\n",
        "\n",
        "def gen_uniform_data(size=len(df)):\n",
        "  \"\"\"\n",
        "  Generates a uniform dataset\n",
        "\n",
        "  Args:\n",
        "        size: The number of data points to generate.\n",
        "\n",
        "  Returns:\n",
        "        A uniform distributed series.\n",
        "  \"\"\"\n",
        "  return np.random.uniform(size=size)\n",
        "\n",
        "def gen_multivariate_normal_data(mean=[0, 0], cov=[[1, 0], [0, 1]], size=len(df)):\n",
        "  \"\"\"\n",
        "  Generates two datasets with a multivariate normal distribution given the mean and covariance matrix\n",
        "\n",
        "  Args:\n",
        "        mean: The mean of each of the datasets.\n",
        "        cov: The covariance matrix of the datasets.\n",
        "        size: The number of data points to generate.\n",
        "\n",
        "  Returns:\n",
        "        Two correlated series.\n",
        "  \"\"\"\n",
        "  ds1, ds2 = np.random.multivariate_normal(mean, cov, size, tol=1e-6).T # ds = dataset\n",
        "  return ds1, ds2\n",
        "\n",
        "def gen_correlated_normal_series(original_series, target_correlation, size=len(df)):\n",
        "  \"\"\"\n",
        "  Generates a correlated series based on a given series.\n",
        "\n",
        "  This function takes an original series as input and generates a new series\n",
        "  that is correlated with the original series. The correlation between the\n",
        "  original and generated series is approximately equal to the specified\n",
        "  target correlation.\n",
        "\n",
        "  The generated series is created by linearly transforming the original series\n",
        "  and adding Gaussian noise with an adjusted standard deviation to achieve the\n",
        "  desired correlation.\n",
        "\n",
        "  Args:\n",
        "      original_series (numpy.ndarray): The original series.\n",
        "      target_correlation (float): The desired Pearson correlation coefficient\n",
        "          between the original and generated series.\n",
        "\n",
        "  Returns:\n",
        "      numpy.ndarray: The generated correlated series.\n",
        "  \"\"\"\n",
        "  return np.mean(original_series) + target_correlation * (original_series - np.mean(original_series)) \\\n",
        "  +  np.random.normal(0, np.sqrt(1 - target_correlation**2) * np.std(original_series), len(original_series))\n",
        "  \"\"\"\n",
        "  Explanation\n",
        "\n",
        "  This one-liner leverages the properties of linear transformations and normal distributions to generate a correlated series.\n",
        "\n",
        "  It first centers the original_series by subtracting its mean.\n",
        "  It then scales this centered series by the target_correlation.\n",
        "  Finally, it adds Gaussian noise with a standard deviation adjusted to ensure the overall correlation matches the target_correlation.\n",
        "  \"\"\"\n",
        "\n",
        "def gen_correlated_uniform_series(original_series, correlation_coefficient=0, size=len(df)):\n",
        "  \"\"\"\n",
        "  Work in progress\n",
        "\n",
        "  Generates a new series correlated with the given series based on the specified correlation coefficient,\n",
        "  using rank correlation to ensure the generated series follows a uniform distribution.\n",
        "\n",
        "  Args:\n",
        "      original_series (numpy.ndarray or list): The original series.\n",
        "      correlation_coefficient (float): The desired correlation coefficient between the original and generated series.\n",
        "      size: The number of data points to generate.\n",
        "\n",
        "  Returns:\n",
        "      The generated correlated series with a uniform distribution.\n",
        "  \"\"\"\n",
        "  z_scores = (original_series - np.mean(original_series)) / np.std(original_series)\n",
        "  correlation_coefficient=.7\n",
        "  return norm.cdf(correlation_coefficient * norm.ppf(np.random.uniform(size=size)) + np.sqrt(1 - correlation_coefficient**2) * z_scores)\n",
        "\n",
        "def pearson_r_func(x, y, y_mean, y_std, desired_r):\n",
        "    x_mean = np.mean(x)\n",
        "    x_std = np.std(x)\n",
        "    numerator = np.sum((x - x_mean) * (y - y_mean))\n",
        "    denominator = x_std * y_std * len(x)\n",
        "    calculated_r = numerator / denominator\n",
        "    return (calculated_r - desired_r)**2  # Minimize the squared difference\n",
        "\n",
        "def minimize_r(original_series, target_correlation, size=len(df)):\n",
        "    y = original_series\n",
        "    y_mean = np.mean(y)\n",
        "    y_std = np.std(y)\n",
        "    desired_r = target_correlation\n",
        "\n",
        "    # Initial guess for x values\n",
        "    x0 = np.random.uniform(size=len(original_series))\n",
        "\n",
        "    # Solve for x\n",
        "    result = minimize(pearson_r_func, x0, args=(y, y_mean, y_std, desired_r))\n",
        "\n",
        "    if result.success:\n",
        "        x_solution = result.x\n",
        "        # print(\"Solution for x:\", x_solution)\n",
        "        return x_solution\n",
        "    else:\n",
        "        print(\"Optimization failed.\")\n",
        "\n",
        "def gen_outliers(mean=0, std_dev=1, size=len(df), outlier_percentage=0.1, outlier_magnitude=3):\n",
        "    \"\"\"\n",
        "    Generates a normal distribution with outliers.\n",
        "\n",
        "    Args:\n",
        "        mean (float): The mean of the normal distribution.\n",
        "        std_dev (float): The standard deviation of the normal distribution.\n",
        "        size (int): The number of samples to generate.\n",
        "        outlier_percentage (float): The percentage of outliers to introduce (between 0 and 1).\n",
        "        outlier_magnitude (float): The magnitude by which outliers deviate from the mean.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The generated data with outliers.\n",
        "    \"\"\"\n",
        "    data = np.random.normal(mean, std_dev, size)\n",
        "    num_outliers = int(size * outlier_percentage)\n",
        "    outlier_indices = np.random.choice(size, num_outliers, replace=False)\n",
        "    for index in outlier_indices:\n",
        "        if np.random.rand() < 0.5:\n",
        "            data[index] += outlier_magnitude\n",
        "        else:\n",
        "            data[index] -= outlier_magnitude\n",
        "\n",
        "    return data\n",
        "\n",
        "def gen_standard_scaling(mean=50, std_dev=10, size=len(df), scale_factor=1000):\n",
        "  \"\"\"\n",
        "  Generates data with a specified mean and standard deviation, then scales it by a factor to create a distribution needing scaling.\n",
        "\n",
        "  Args:\n",
        "      mean (float): The mean of the original distribution.\n",
        "      std_dev (float): The standard deviation of the original distribution.\n",
        "      size (int): The number of samples to generate.\n",
        "      scale_factor (float): The factor by which to scale the original distribution.\n",
        "\n",
        "  Returns:\n",
        "      numpy.ndarray: The generated data needing scaling.\n",
        "  \"\"\"\n",
        "  original_data = np.random.normal(mean, std_dev, size)\n",
        "  return original_data * scale_factor\n",
        "\n",
        "def gen_minmax_scaling(mean=50, std_dev=10, size=len(df), range_factor=10):\n",
        "  \"\"\"\n",
        "  Generates data with a specified mean and standard deviation, then scales and shifts it to create a distribution needing MinMax scaling.\n",
        "\n",
        "  Args:\n",
        "      mean (float): The mean of the original distribution.\n",
        "      std_dev (float): The standard deviation of the original distribution.\n",
        "      size (int): The number of samples to generate.\n",
        "      range_factor (float): The factor to expand the range of the original distribution.\n",
        "\n",
        "  Returns:\n",
        "      numpy.ndarray: The generated data needing scaling.\n",
        "  \"\"\"\n",
        "\n",
        "  # Generate the original data\n",
        "  original_data = np.random.normal(mean, std_dev, size)\n",
        "\n",
        "  # Expand the range of the data\n",
        "  min_val = np.min(original_data)\n",
        "  max_val = np.max(original_data)\n",
        "  return (original_data - min_val) * range_factor + min_val\n",
        "\n",
        "def random_choice_data(choices, size):\n",
        "  \"\"\"\n",
        "  Generates a new series correlated with the given series based on the specified correlation coefficient,\n",
        "  using rank correlation to ensure the generated series follows a uniform distribution.\n",
        "\n",
        "  Args:\n",
        "      original_series (numpy.ndarray or list): The original series.\n",
        "      correlation_coefficient (float): The desired correlation coefficient between the original and generated series.\n",
        "\n",
        "  Returns:\n",
        "      numpy.ndarray: The generated correlated series with a uniform distribution.\n",
        "  \"\"\"\n",
        "  return np.random.choice(choices, size=size)\n"
      ],
      "metadata": {
        "id": "JWOa9CmWQQ3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# categorical variables with little correlation to sales\n",
        "df['random choice 2'] = random_choice_data(['Rand Choice 1', 'Rand Choice 2'], size=len(df))\n",
        "df['random choice 4'] = random_choice_data(['North', 'South', 'East', 'West'], size=len(df))\n",
        "df['random choice 7'] = random_choice_data(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], size=len(df))\n",
        "\n",
        "# categorical random choices with random # of labels\n",
        "num_labels = np.random.randint(3, 5)\n",
        "df[f'random label num {num_labels}'] = random_choice_data([f'label num lo {i}' for i in range(1, num_labels + 1)], size=len(df))\n",
        "\n",
        "num_labels = np.random.randint(10, 15)\n",
        "df[f'random label num {num_labels}'] = random_choice_data([f'label num hi {i}' for i in range(1, num_labels + 1)], size=len(df))"
      ],
      "metadata": {
        "id": "OwAolbPsxTxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# categorical variables correlated with target\n",
        "df['pd qcut1'] = pd.qcut(df['target'], 2, labels=['Low', 'High']) # bi label\n",
        "df['pd qcut2'] = pd.qcut(df['target'], 4, labels=['Q1', 'Q2', 'Q3', 'Q4']) # 4 labels\n",
        "\n",
        "quantiles = [0, 0.1, 0.2, 0.4, 0.6, 0.8, 1]\n",
        "df['pd qcut3'] = pd.qcut(df['target'], quantiles, labels=['G1', 'G2', 'G3', 'G4', 'G5', 'G6']) # 6 labels"
      ],
      "metadata": {
        "id": "oiR1SK-QR7Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate four numerical normally distributed continuous features that have a correlation greater than absolute value of .5 with each other\n",
        "# gen_multivariate_normal_data(mean=[0, 0], cov=[[1, 0], [0, 1]], size=len(df))\n",
        "df['multicollinearity 1'], df['multicollinearity 2'] = gen_multivariate_normal_data(mean=[0, 0], cov=[[1, .7], [.7, 1]], size=len(df))\n",
        "df['multicollinearity 3'], df['multicollinearity 4'] = gen_multivariate_normal_data(mean=[0, 0], cov=[[1, .9], [.9, 1]], size=len(df))"
      ],
      "metadata": {
        "id": "Q6QTLoR2uPkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate two normally distributed features that are correlated with the target\n",
        "# gen_correlated_normal_series(original_series, target_correlation, size=len(df))\n",
        "df['correlated w target 1'] = gen_correlated_normal_series(df['target'], target_correlation=.5)\n",
        "df['correlated w target 2'] = gen_correlated_normal_series(df['target'], target_correlation=.7)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "EyUCReGZlSSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate two uniformly distributed features that are correlated with the target\n",
        "# gen_correlated_uniform_series(original_series, correlation_coefficient=0, size=len(df))\n",
        "df['uniform corr 1'] = gen_correlated_uniform_series(df['target'])\n",
        "df['uniform corr 2'] = gen_correlated_uniform_series(df['target'])\n",
        "\n",
        "print(df[['uniform corr 1', 'uniform corr 2', 'target']].corr())\n",
        "df[['uniform corr 1', 'uniform corr 2']].hist();"
      ],
      "metadata": {
        "id": "nEQxB18Juh8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create two features that are duplicates of other features\n",
        "df['duplicate_1'] = df['informative_1']\n",
        "df['duplicate_2'] = df['informative_2']"
      ],
      "metadata": {
        "id": "Vx1_vq2FIxan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create two numerical features with outliers\n",
        "df['outliers 1'] = gen_outliers(mean=0, std_dev=1, size=len(df), outlier_percentage=0.1, outlier_magnitude=3)\n",
        "df['outliers 2'] = gen_outliers(mean=3, std_dev=2, size=len(df), outlier_percentage=0.2, outlier_magnitude=2)"
      ],
      "metadata": {
        "id": "wJGVW0OMJKCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a numerical feature that needs standard scaling\n",
        "df['standard scaling'] = gen_standard_scaling()"
      ],
      "metadata": {
        "id": "oNXKVbukJUX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a numerical feature that needs min max scaling\n",
        "df['min max scaling'] = gen_minmax_scaling()"
      ],
      "metadata": {
        "id": "YffL2FIjJZBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate null values\n",
        "for col in df.drop(['class', 'informative_1', 'informative_2', 'target', 'duplicate_1', 'duplicate_2'], axis=1).columns:\n",
        "    df[col] = gen_null(df[col], np.random.choice([0, 5, 10, 20, 30, 50], size=1).item())"
      ],
      "metadata": {
        "id": "BSB0O7puJmJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create two features that have constant values\n",
        "df['constant_1'] = 'constant_value'\n",
        "df['constant_2'] = 'constant_value'"
      ],
      "metadata": {
        "id": "2sfkZqmYIjDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create two features with semi constant values\n",
        "df['semi_constant_1'] = gen_quasi_constants('q_const', variation_percentage = 1)\n",
        "df['semi_constant_2'] = gen_quasi_constants('q_const', variation_percentage = 1)"
      ],
      "metadata": {
        "id": "gWSpRTvfIp1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())  # check your work"
      ],
      "metadata": {
        "id": "nQ8w6T9WYLf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add duplicates\n",
        "dupes = df.loc[0:9]\n",
        "df = pd.concat([df, dupes], axis=0)\n",
        "\n",
        "# shuffle all columns\n",
        "# df = df.sample(frac=1).reset_index(drop=True)\n",
        "# df = df.sample(frac=1, axis=1)\n",
        "\n",
        "# shuffle selected columns\n",
        "demographic_columns = demographics.columns\n",
        "remaining_columns = [col for col in df.columns if col not in demographic_columns]\n",
        "# print(remaining_columns)\n",
        "np.random.shuffle(remaining_columns)\n",
        "\n",
        "# Reassemble the DataFrame with the shuffled columns\n",
        "df = df[list(demographic_columns) + list(remaining_columns)]\n",
        "\n",
        "# move target to the end of the list\n",
        "class_var = 'class'\n",
        "df = df[df.drop('class', axis=1).columns.tolist() + [class_var]]\n",
        "\n",
        "print(df.shape)\n",
        "print(df.info())\n",
        "df.head()"
      ],
      "metadata": {
        "id": "CiFb0YwCXupw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('simulated_data.csv', index=False)"
      ],
      "metadata": {
        "id": "Legtg7PmvNor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Drive"
      ],
      "metadata": {
        "id": "g_XqWl1r3w5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Mount Drive\n",
        "# import shutil\n",
        "\n",
        "# # Source file path (within your Drive)\n",
        "# source_file = '/content/filename'\n",
        "\n",
        "# # Destination path (root of your Drive)\n",
        "# destination_path = '/content/drive/MyDrive/filepath/filename'\n",
        "\n",
        "# # Copy the file\n",
        "# shutil.copy(source_file, destination_path)"
      ],
      "metadata": {
        "id": "aUUjViiUOEcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Prep Process"
      ],
      "metadata": {
        "id": "K3nv3YmhSHkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "nWWZXj_t7wkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Along"
      ],
      "metadata": {
        "id": "23NaFEzJVxm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code along\n"
      ],
      "metadata": {
        "id": "GER6wToI7wkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make It Stick - Characteristics of Data"
      ],
      "metadata": {
        "id": "5oJjLkWfVNe8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types and Characteristics of Data\n",
        "\n",
        "**Numerical Data**\n",
        "\n",
        "This refers to columns with a numerical data type, typically integers (`int64`) or floating-point numbers (`float64`). These data represent quantities and are used for mathematical operations.\n",
        "\n",
        "* **Characteristics**:\n",
        "    * **Continuous**: Can take any value within a given range (e.g., height, temperature, price).\n",
        "    * **Discrete**: Can only take specific, distinct values (e.g., number of children, counts).\n",
        "    * **Order**: The values have a meaningful order and can be compared (e.g., 10 is greater than 5).\n",
        "    * **Mathematical Operations**: Can be used for calculations like mean, standard deviation, and regression analysis.\n",
        "\n",
        "**Object/String Data**\n",
        "\n",
        "This refers to columns with an `object` data type, which in pandas often means they contain text or strings. These are non-numeric and don't have inherent mathematical properties.\n",
        "\n",
        "* **Characteristics**:\n",
        "    * **Nominal**: Categories without a meaningful order (e.g., colors like \"red,\" \"blue,\" \"green\").\n",
        "    * **Textual**: Contains free-form text, which might require natural language processing (NLP) techniques for analysis.\n",
        "    * **High Cardinality**: Often contain a large number of unique values, making them difficult to use directly in many machine learning models without preprocessing.\n",
        "\n",
        "**Categorical Data**\n",
        "\n",
        "This refers to columns with a `category` data type. A `category` type is a pandas-specific data type that is more memory-efficient than `object` for columns with a limited number of unique values.\n",
        "\n",
        "* **Characteristics**:\n",
        "    * **Ordinal**: Categories with a meaningful order (e.g., \"low,\" \"medium,\" \"high\").\n",
        "    * **Nominal**: Categories without a meaningful order (as with `df_object`).\n",
        "    * **Memory Efficient**: Internally stores an integer representation of each category, which is more efficient for storage and computation.\n",
        "\n",
        "**Categorical Features**\n",
        "\n",
        "This is a combined group of columns from both the `category` and `object` data types. These columns represent features that are **not** numerical and will likely need to be converted to a numerical format using encoding techniques (like **One-Hot Encoding** or **Label Encoding**) before being used in most machine learning models.\n",
        "\n",
        "* **Characteristics**:\n",
        "    * **Mixed Types**: A collection of both `category` and `object` data types.\n",
        "    * **Preprocessing Required**: These features cannot be directly used in most machine learning algorithms. They must be preprocessed to a numerical representation.\n",
        "    * **High-level Representation**: This group represents all the columns in a dataset that are qualitative rather than quantitative."
      ],
      "metadata": {
        "id": "nMZvDVF51DcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_numerical = df.select_dtypes(include='number').columns\n",
        "# df_object = df.select_dtypes(include=['object']).columns\n",
        "# df_discreet = df.select_dtypes(include=['category']).columns\n",
        "# df_categorical_features = df.select_dtypes(include=['category', 'object']).columns\n",
        "# print(\"Numerical Data:\")\n",
        "# print(df_numerical)\n",
        "# print(\"/nObject Data:\")\n",
        "# print(df_object)\n",
        "# print(\"/nDiscreet Data:\")\n",
        "# print(df_discreet)\n",
        "# print(\"/nCategorical Data:\")\n",
        "# print(df_categorical_features"
      ],
      "metadata": {
        "id": "pVRA3GCN73QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Row Duplicates"
      ],
      "metadata": {
        "id": "YIpg3h9QFswd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# len(df[df.duplicated(keep=False)])"
      ],
      "metadata": {
        "id": "Pf0GiiFiFwNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Column Duplicates"
      ],
      "metadata": {
        "id": "Putqz0nUF-sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### By Code"
      ],
      "metadata": {
        "id": "ec5O8WX7M9oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# duplicate_features = []\n",
        "# for i in range(0, len(df.columns)):\n",
        "#     orig = df.columns[i]\n",
        "\n",
        "#     for dupe in df.columns[i + 1:]:\n",
        "#         if df[orig].equals(df[dupe]):\n",
        "#             duplicate_features.append(dupe)\n",
        "\n",
        "# duplicate_features"
      ],
      "metadata": {
        "id": "7qKp-5peGBcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### By Colab AI"
      ],
      "metadata": {
        "id": "aiXKz6GxM_rR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Along"
      ],
      "metadata": {
        "id": "GixwWVK9WG2y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8stGMhnZNDqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants"
      ],
      "metadata": {
        "id": "Xw5iD_SG-3TI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### By Code"
      ],
      "metadata": {
        "id": "dJASGoZJElY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# constant_features = [\n",
        "#     feat for feat in df.columns if len(df[feat].unique()) == 1\n",
        "# ]\n",
        "\n",
        "# constant_features"
      ],
      "metadata": {
        "id": "nuB6RAHuDvGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### By Colab AI"
      ],
      "metadata": {
        "id": "cr75jttJEnWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Along"
      ],
      "metadata": {
        "id": "tRl0tuFYWJfe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EIncU2q4Eg1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quasi-Constants"
      ],
      "metadata": {
        "id": "I2e8lgeTEtuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# quasi_consts = []\n",
        "# thresh = .95\n",
        "# for val in df.columns.sort_values():\n",
        "#     if (len(df[val].unique()) < 3 and max(df[val].value_counts(normalize=True)) > thresh):\n",
        "#         quasi_consts.append(val)\n",
        "\n",
        "# quasi_consts"
      ],
      "metadata": {
        "id": "7qA2qwEWFUjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missing Values (Imputation)"
      ],
      "metadata": {
        "id": "DaK5E7BEHrdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # what happens to the mean and median with skewed data\n",
        "# import numpy as np\n",
        "# from scipy.stats import skewnorm, norm\n",
        "# from scipy import stats\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# fig, ax = plt.subplots(1, 3, figsize=(14, 3))\n",
        "# skew = 20\n",
        "# n = 100000\n",
        "\n",
        "# r = skewnorm.rvs(skew, loc=0, scale=10, size=n)\n",
        "# ax[0].hist(r, bins=50, density=True, alpha=0.4)\n",
        "# ax[0].axvline(x=np.median(r), color='green', label='Median')\n",
        "# ax[0].axvline(x=np.mean(r).round(2), color='red', label='Mean')\n",
        "# ax[0].legend()\n",
        "# print(f'right skew data mean: {np.mean(r).round(2)}, median: {np.median(r).round(2)}')\n",
        "\n",
        "# l = skewnorm.rvs(-skew, loc=0, scale=10, size=n)\n",
        "# ax[2].hist(l, bins=50, density=True, alpha=0.4);\n",
        "# ax[2].axvline(x=np.mean(l).round(2), color='red', label='Mean')\n",
        "# ax[2].axvline(x=np.median(l), color='green', label='Median')\n",
        "# ax[2].legend()\n",
        "# print(f'left skew data mean: {np.mean(l).round(2)}, median: {np.median(l).round(2)}')\n",
        "\n",
        "# n = norm.rvs(loc=0, scale=1, size=n)\n",
        "# ax[1].hist(n, bins=50, density=True, alpha=0.4);\n",
        "# ax[1].axvline(x=np.mean(n).round(2), color='red', linewidth=3, label='Mean')\n",
        "# ax[1].axvline(x=np.median(n), color='green', label='Median')\n",
        "# ax[1].legend()\n",
        "# print(f'normal data mean: {np.mean(n).round(2)}, median: {np.median(n).round(2)}')"
      ],
      "metadata": {
        "id": "SXPp9Oqbqg9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_categorical_features = df.select_dtypes(include=['category', 'object']).columns\n",
        "# dfx = df.copy()\n",
        "# for feat in df.columns[df.isnull().sum() > 1]:\n",
        "#   if feat in df_categorical_features:\n",
        "#     dfx[feat] = df[feat].fillna(df[feat].mode()[0])\n",
        "#   else:\n",
        "#     if abs(df[feat].skew()) < .8:\n",
        "#       dfx[feat] = df[feat].fillna(round(df[feat].mean(), 2))\n",
        "#     else:\n",
        "#       dfx[feat] = df[feat].fillna(df[feat].median())\n",
        "\n",
        "# dfx"
      ],
      "metadata": {
        "id": "aHO2VEMIHqbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outliers"
      ],
      "metadata": {
        "id": "qzuW_Dp-h0hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # code along\n",
        "# df.boxplot(column=['outliers 1']);"
      ],
      "metadata": {
        "id": "Wvt9mNhQvX2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Along"
      ],
      "metadata": {
        "id": "YeKCmMJs4YJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code along\n"
      ],
      "metadata": {
        "id": "ZwA0vezIyGsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# code along\n"
      ],
      "metadata": {
        "id": "C71eDEM3JOc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# code along\n"
      ],
      "metadata": {
        "id": "RR5jTPPP7ThD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datetime"
      ],
      "metadata": {
        "id": "9XexLNUgPwi2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Along"
      ],
      "metadata": {
        "id": "ibX2_MW_2HeR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uqffzqcD2K1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Creation"
      ],
      "metadata": {
        "id": "3ZaEp01sx2-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # create a new variable by combining two variables\n",
        "# df['scaling_combined'] = df['standard scaling'] + df['min max scaling']\n",
        "# df.drop(['standard scaling', 'min max scaling'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "l4TygZ-hx6PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discretization"
      ],
      "metadata": {
        "id": "Ppg8LVqXP7Xs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Along"
      ],
      "metadata": {
        "id": "K-9lOY5j2cQD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5B1kUXQt2fWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28c3f668"
      },
      "source": [
        "## Categorical Encoding\n",
        "* Sklearn One Hot Encoding\n",
        "* Dummy Trap\n",
        "* Pandas get_dummies\n",
        "* Labelizer\n",
        "* Weight of Evidence\n",
        "* Frequency Encoding\n",
        "\n",
        "### Categorical Data\n",
        "* Nominal (Cat or Dog)\n",
        "* Ordinal (Grades)\n",
        "* Works better for limited labels in a category\n",
        "* Engineer features with many labels\n",
        "\n",
        "### Multicollinearity\n",
        "* Predictors need to be independent of each other\n",
        "* https://www.theanalysisfactor.com/multicollinearity-explained-visually/\n",
        "* https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/\n",
        "* Cats_and_Dogs = [Cat, Dog, Dog, Cat, Cat, Dog]\n",
        "* Cats = [1, 0, 0, 1, 1, 0]\n",
        "* Dogs = [0, 1, 1, 0, 0, 1]\n",
        "\n",
        "### Beware of Mismatch in Training and Test\n",
        "\n",
        "* Some labels in the train set don't show up in the test set\n",
        "\n",
        "https://towardsdatascience.com/beware-of-the-dummy-variable-trap-in-pandas-727e8e6b8bde"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1beccfa2"
      },
      "source": [
        "## One Hot Encoder Explanation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e2451d6"
      },
      "outputs": [],
      "source": [
        "# # sklearn OneHotEncoder\n",
        "# # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
        "# # https://stackoverflow.com/questions/50473381/scikit-learns-labelbinarizer-vs-onehotencoder\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# pets = ['dog', 'cat', 'cat', 'dog', 'turtle', 'cat', 'cat', 'turtle', 'dog', 'cat']\n",
        "# print('cat = 0; dog = 1; turtle = 2')\n",
        "# le = LabelEncoder()\n",
        "# int_values = le.fit_transform(pets)\n",
        "# print('Pets:', pets)\n",
        "# print('Label Encoder:', int_values)\n",
        "# int_values = int_values.reshape(len(int_values), 1)\n",
        "# print(pd.Series(pets))\n",
        "\n",
        "# ohe = OneHotEncoder(sparse_output=False)\n",
        "# ohe = ohe.fit_transform(int_values)\n",
        "# print('One Hot Encoder:\\n', ohe)\n",
        "\n",
        "# lb = LabelBinarizer()\n",
        "# print('Label Binarizer:\\n', lb.fit_transform(int_values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a558af81"
      },
      "outputs": [],
      "source": [
        "# pets = pd.DataFrame(pd.Series(pets), columns=['Pets'])\n",
        "# pets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3af38b01"
      },
      "outputs": [],
      "source": [
        "# ohe = OneHotEncoder(sparse_output=False)\n",
        "# ohe_pets = ohe.fit_transform(pets)\n",
        "# pets_df = pd.DataFrame(ohe_pets, columns=ohe.get_feature_names_out(['Pets']))\n",
        "# pets_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dimensionality\n",
        "\n",
        "One-hot encoding creates a new binary column for each unique category in a categorical feature. The main problem with this is the **curse of dimensionality**, especially for features with many unique values (high cardinality).\n",
        "\n",
        "This process rapidly increases the number of columns in your dataset, leading to several issues:\n",
        "\n",
        "* **Increased Memory and Computational Costs**: The dataset becomes much larger, requiring more memory and computational power to store and process, which slows down model training.\n",
        "* **Sparse Data**: The new columns are mostly zeros, creating a **sparse matrix** where only one value is a \"1\" in each row for that set of features. This can be inefficient for many algorithms that don't handle sparsity well.\n",
        "* **Overfitting**: With a high number of features and potentially limited data, models can easily learn the noise and idiosyncrasies of the training data rather than the underlying patterns, leading to poor performance on new, unseen data."
      ],
      "metadata": {
        "id": "-2T9cgDu8s34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make It Stick - Dummy Trap"
      ],
      "metadata": {
        "id": "c0CMkEBtVgzw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eaf9f4e"
      },
      "source": [
        "### Dummy Trap\n",
        "\n",
        "The Dummy Variable Trap occurs when two or more dummy variables created by one-hot encoding are highly correlated (multi-collinear). This means that one variable can be predicted from the others, making it difficult to interpret predicted coefficient variables in regression models. In other words, the individual effect of the dummy variables on the prediction model can not be interpreted well because of multicollinearity.\n",
        "\n",
        "https://www.learndatasci.com/glossary/dummy-variable-trap/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **dummy trap** is a problem that occurs in regression analysis when a categorical variable with multiple categories is encoded into dummy variables. It creates a situation of **perfect multicollinearity**, where one dummy variable can be perfectly predicted from a linear combination of the other dummy variables.\n",
        "\n",
        "**How It Happens**\n",
        "\n",
        "When you use a technique like **one-hot encoding** to convert a categorical variable (e.g., \"City\" with categories \"New York,\" \"Chicago,\" and \"Boston\") into numerical data, you create a new binary column for each category. For any given data point, one of these new columns will be a `1` and the rest will be `0`s.\n",
        "\n",
        "* **Example**: If you have three cities, you create three dummy variables: `is_New_York`, `is_Chicago`, and `is_Boston`. A row for a person in New York would have `is_New_York` = 1, `is_Chicago` = 0, and `is_Boston` = 0.\n",
        "\n",
        "The trap arises because there's a **redundant variable**. If you know the values for `is_New_York` and `is_Chicago`, you can perfectly predict the value for `is_Boston`: if `is_New_York` is 0 and `is_Chicago` is 0, then `is_Boston` must be 1. This linear relationship between the variables violates a key assumption of many regression models (like linear regression), making it impossible to uniquely calculate the coefficients for each variable.\n",
        "\n",
        "### The Solution\n",
        "\n",
        "To avoid the dummy trap, you must **drop one of the dummy variables**. This is also known as using \"dummy encoding\" instead of one-hot encoding. By having only $k-1$ dummy variables for a categorical variable with $k$ categories, you eliminate the perfect multicollinearity. The dropped category becomes the **baseline** or **reference category**, and the coefficients of the remaining dummy variables are interpreted in comparison to that baseline."
      ],
      "metadata": {
        "id": "e2pwIGwFWjQC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f408874"
      },
      "outputs": [],
      "source": [
        "# pets_df.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlation\n",
        "\n",
        "Correlation is a statistical measure in data science that quantifies the **linear relationship** between two variables. It helps you understand if and how two variables change together. This relationship is measured by the **correlation coefficient**, a value typically denoted by **r** that ranges from -1 to +1.\n",
        "\n",
        "**Types of Correlation**\n",
        "\n",
        "* **Positive Correlation (r > 0):** As one variable increases, the other also tends to increase. For example, the more hours a student studies, the higher their exam score tends to be.\n",
        "* **Negative Correlation (r < 0):** As one variable increases, the other tends to decrease. For example, as the temperature rises, the sales of hot coffee might decrease.\n",
        "* **No Correlation (r ≈ 0):** There is no apparent linear relationship between the two variables. For instance, the number of books you read has no correlation with the color of your car.\n",
        "\n",
        "\n",
        "**Key Aspects**\n",
        "\n",
        "* **Strength:** The closer the absolute value of the correlation coefficient is to 1, the stronger the linear relationship. A value of 1 or -1 indicates a perfect linear relationship. The closer the value is to 0, the weaker the relationship.\n",
        "* **Direction:** The sign of the coefficient (+ or -) indicates the direction of the relationship.\n",
        "* **Correlation does not imply causation:** This is a crucial point in data science. Just because two variables are correlated doesn't mean one causes the other. There could be a third, unobserved variable (a confounding variable) influencing both, or the relationship could be a mere coincidence."
      ],
      "metadata": {
        "id": "LA6WVjH64v7g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ec8b974"
      },
      "outputs": [],
      "source": [
        "# ohe = OneHotEncoder(drop='first', sparse_output=False)\n",
        "# ohe_pets = ohe.fit_transform(pets)\n",
        "# pets_df = pd.DataFrame(ohe_pets, columns=ohe.get_feature_names_out(['Pets']))\n",
        "# pets_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2271c79c"
      },
      "outputs": [],
      "source": [
        "# pets_df.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Day of Week Encoding\n",
        "\n",
        "* https://mikulskibartosz.name/time-in-machine-learning\n",
        "\n",
        "The best way to encode days of the week for machine learning depends on whether the model can recognize and use the inherent **cyclical** nature of the data. The two most effective methods are **one-hot encoding** and **sinusoidal (or cyclical) encoding**.\n",
        "\n",
        "**One-Hot Encoding**\n",
        "One-hot encoding is a straightforward method that creates a new binary column for each day of the week. This is an excellent choice for models that don't inherently handle cyclical data well, such as linear regression and tree-based models (e.g., Random Forest, Gradient Boosting).\n",
        "\n",
        "* **How it works**: A feature like \"Day of Week\" with values like 'Monday', 'Tuesday', etc., is transformed into 7 new columns: `is_Monday`, `is_Tuesday`, etc. For a given row, the column corresponding to the correct day will have a value of 1, and all other new columns will be 0.\n",
        "* **Pros**:\n",
        "    * Simple and easy to implement.\n",
        "    * Works with virtually all machine learning models.\n",
        "    * Prevents the model from incorrectly assuming an ordinal relationship (e.g., thinking that Saturday is \"less than\" Sunday).\n",
        "* **Cons**:\n",
        "    * Can create a large number of features, especially for data with high cardinality.\n",
        "    * Doesn't capture the cyclical relationship of the days (e.g., Sunday is closer to Monday than to Wednesday).\n",
        "\n",
        "**Sinusoidal (Cyclical) Encoding**\n",
        "\n",
        "This method is ideal for models that can leverage the continuous, cyclical nature of the data, such as neural networks. It transforms the day of the week into two new features using sine and cosine functions.\n",
        "\n",
        "* **How it works**: Each day is mapped to an angle on a circle. Monday might be 0 degrees, Tuesday 51.4 degrees ($360/7$), and so on, with Sunday wrapping back around to Monday. The sine and cosine of these angles are then used as two new features. The formulas are:\n",
        "    * $Sine\\_Feature = sin(\\frac{2 \\pi \\times day\\_of\\_week}{7})$\n",
        "    * $Cosine\\_Feature = cos(\\frac{2 \\pi \\times day\\_of\\_week}{7})$\n",
        "    \n",
        "    This creates a continuous representation where the distance between days is preserved, and the cyclical nature is explicitly encoded.\n",
        "* **Pros**:\n",
        "    * Explicitly captures the cyclical relationship, which can improve model performance.\n",
        "    * Creates only two new features, regardless of the number of days, reducing dimensionality.\n",
        "* **Cons**:\n",
        "    * Can be more complex to implement than one-hot encoding.\n",
        "    * The model must be able to effectively use these continuous features.\n",
        "    * Less interpretable than one-hot encoding.\n",
        "\n",
        "**Recommendation**\n",
        "For a majority of cases, **one-hot encoding** is a safe and reliable choice, especially for tree-based models and simpler linear models. It ensures no false ordinal relationships are introduced. However, if you are using a neural network or a model that benefits from continuous, cyclical features, **sinusoidal encoding** is the better and more elegant solution as it directly encodes the underlying structure of the data."
      ],
      "metadata": {
        "id": "6yia75mH8p43"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "084c15ac"
      },
      "source": [
        "### Get Dummies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "576cb600"
      },
      "outputs": [],
      "source": [
        "# # using pandas get_dummies\n",
        "# import pandas as pd\n",
        "\n",
        "# dummy_example = pd.get_dummies(df, drop_first=True)\n",
        "# print(dummy_example.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45820551"
      },
      "source": [
        "### One Hot Encoding Alternatives\n",
        "\n",
        "For features with many labels\n",
        "\n",
        "* https://medium.com/analytics-vidhya/stop-one-hot-encoding-your-categorical-variables-bbb0fba89809\n",
        "* https://medium.com/swlh/stop-one-hot-encoding-your-categorical-features-avoid-curse-of-dimensionality-16743c32cea4\n",
        "* https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02 (frequency and mean encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38db741f"
      },
      "source": [
        "### Encoding Order\n",
        "\n",
        "* Bilabel Mapping (2 labels)\n",
        "* Frequency (5+ labels)\n",
        "* One Hot Encoding (3 - 5 labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cat_features = []\n",
        "# for feat in df.select_dtypes(include=['object', 'category']):\n",
        "#   if len(df[feat].value_counts()) < 3:\n",
        "#     df[feat] = df[feat].map({df[feat].value_counts().index[0]: 0, df[feat].value_counts().index[1]: 1})\n",
        "#     df[feat] = df[feat].astype(int)\n",
        "#   elif 2 < len(df[feat].value_counts()) < 6:\n",
        "#     cat_features.append(feat)\n",
        "#   elif len(df[feat].value_counts()) > 5:\n",
        "#     freq = df.groupby(feat, observed=False).size()/len(df)\n",
        "#     df[feat] = df[feat].map(freq)\n",
        "\n",
        "# ohe = OneHotEncoder(categories='auto', drop='first', sparse_output=False, handle_unknown='ignore')\n",
        "# ohe_df = ohe.fit_transform(df[cat_features])\n",
        "# ohe_df = pd.DataFrame(ohe_df, columns=ohe.get_feature_names_out(cat_features))\n",
        "# df.index = df.index\n",
        "# df = df.join(ohe_df)\n",
        "# df.drop(cat_features, axis=1, inplace=True)\n",
        "# df"
      ],
      "metadata": {
        "id": "nPdE_QawKTMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Titanic Complete"
      ],
      "metadata": {
        "id": "k4dMEWJHH5Ln"
      }
    }
  ]
}