{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtN07PDZKIKuarBP9sgMxn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC5502/blob/main/Module_08-Linear_Regression/GLMs_Splines_GAMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GLMs, Splines, and GAMs\n",
        "\n",
        "by Your Name"
      ],
      "metadata": {
        "id": "_Ow_-87VQChE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions\n",
        "\n",
        "* Carefully review the material\n",
        "* Make sure all cells are uncommented appropriately and executed without errors\n",
        "* Answer or respond to questions and text prompts when asked to do so\n",
        "* Share this link so that anyone with link can view and submit the shared link in Canvas"
      ],
      "metadata": {
        "id": "uYuMLLS0u1Pn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "* Colab - get notebook from gitmystuff DTSC5502 repository\n",
        "* Save a Copy in Drive\n",
        "* Remove Copy of\n",
        "* Edit your name\n",
        "* Submit shared link in Canvas\n",
        "\n",
        "### Overview\n",
        "\n",
        "* Logarithms and Euler's Number\n",
        "* Functions\n",
        "  * Line\n",
        "  * Quadratic\n",
        "  * Exponential\n",
        "  * Log Base e (Natural)\n",
        "  * Sine\n",
        "  * Cosine\n",
        "  * Tangent\n",
        "  * Hyperbolic\n",
        "* Distributions\n",
        "  * Normal\n",
        "  * Binomial\n",
        "  * Poisson\n",
        "  * Gamma\n",
        "* S-Curve and CDF\n",
        "* Generalized Linear Models\n",
        "* Linear and Logistic Regression Formulas\n",
        "* Assumptions\n",
        "* Polynomial Regression\n",
        "* Step Functions\n",
        "* Basis Functions\n",
        "* Local Regression\n",
        "* Regression Splines\n",
        "* Smoothing Splines\n",
        "* Generalized Additive Models"
      ],
      "metadata": {
        "id": "BnBqTANqu1mM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video Reviews\n",
        "\n",
        "Quant Psych\n",
        "* Understanding GLM - https://www.youtube.com/watch?v=SqN-qlQOM5A\n",
        "* GLM Pt 1 - https://www.youtube.com/watch?v=ZWnM-yPUXlA\n",
        "* GLM Pt 2 - https://www.youtube.com/watch?v=29Droau_6DM\n",
        "* Mixed Models - https://www.youtube.com/watch?v=5tOifM51ZOk\n",
        "\n",
        "\n",
        "Stanford\n",
        "* 1.1 First 4 minutes - https://www.youtube.com/watch?v=LvySJGj-88U&list=PLoROMvodv4rPP6braWoRt5UCXYZ71GZIQ\n",
        "GLM\n",
        "https://www.youtube.com/playlist?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e\n",
        "\n",
        "* 4.8\n",
        "* 7.2 - 7.4\n",
        "* Splines - 7.2, 7.3,\n",
        "* GAM - 7.4\n",
        "* Python: Polynomial Regression... - https://www.youtube.com/watch?v=_0omkfNiU2c&list=PLoROMvodv4rPP6braWoRt5UCXYZ71GZIQ&index=56&pp=iAQB\n",
        "* Python: Splines - https://www.youtube.com/watch?v=C9h-o6AfNX0&list=PLoROMvodv4rPP6braWoRt5UCXYZ71GZIQ&index=57&pp=iAQB\n",
        "* Python: GAMs - https://www.youtube.com/watch?v=hQx84r2maaE&list=PLoROMvodv4rPP6braWoRt5UCXYZ71GZIQ&index=58&pp=iAQB\n",
        "\n",
        "* for linear regression we focus on coefficients and gams we focus on fitted functions\n",
        "* Partial dependence plot - The partial dependence plot (short PDP or PD plot) shows the marginal effect one or two features have on the predicted outcome of a machine learning model (J. H. Friedman 200130). A partial dependence plot can show whether the relationship between the target and a feature is linear, monotonic or more complex. https://christophm.github.io/interpretable-ml-book/pdp.html"
      ],
      "metadata": {
        "id": "Owt7-mCLCv00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concepts vs Facts\n",
        "\n",
        "Concepts are an efficient way to store knowledge. Once you have abstracted the common features of a concept such as chair, you will recognize objects you have never seen before as a type of chair. This is because you hold in memory a prototypic representation of the concept - an abstraction of \"chairness.\" Unfortunately, facts must each be held in memory individually. This is because they have no common group features. By definition, a fact is a unique piece of information that must be individually held in memory to be known. Compared to concepts, facts are a much less efficient form of knowledge.\n",
        "\n",
        "Developing Technical Training: A Structured Approach for Developing Classroom and Computer-Based Instructional Materials 3rd Edition by Ruth Clark https://www.amazon.com/Developing-Technical-Training-Computer-based-Instructional/dp/0787988464"
      ],
      "metadata": {
        "id": "FfULYXS-yX1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions\n",
        "\n",
        "* https://github.com/gitmystuff/DSChunks/blob/main/Functions.ipynb"
      ],
      "metadata": {
        "id": "iO75yctdeU15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distributions"
      ],
      "metadata": {
        "id": "0Bv98X0YXQBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Is the Distribution Normal?\n",
        "\n",
        "* The Shapiro–Wilk test is a test for normality. It was published in 1965 by Samuel Sanford Shapiro and Martin Wilk.\n",
        "* https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6676026/#:~:text=The%20purpose%20of%20the%20t,essential%20in%20the%20t%2Dtest\n",
        "* https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test\n",
        "*If the p-value of the test is greater than α = .05, then the data is assumed to be normally distributed."
      ],
      "metadata": {
        "id": "s3-nXUkx7_bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # https://levelup.gitconnected.com/probability-distributions-using-scipy-58fdab53d7ac\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# import scipy.stats as stats\n",
        "\n",
        "# # normal\n",
        "# n = 100\n",
        "# x = np.linspace(0, 10, n)\n",
        "# y = stats.norm.pdf(x, 5, 1)\n",
        "# dist = stats.norm().rvs(1000)\n",
        "# plt.plot(x, y, label=f'normal: {round(stats.shapiro(dist).pvalue,2)}')\n",
        "\n",
        "# # binom\n",
        "# x = np.arange(0,11)\n",
        "# y = stats.binom(n=len(x), p=.3).pmf(x)\n",
        "# dist = stats.binom(n=len(x), p=.3).rvs(1000)\n",
        "# plt.plot(x, y, label=f'binom: {round(stats.shapiro(dist).pvalue,2)}')\n",
        "\n",
        "\n",
        "# # poisson\n",
        "# x = np.arange(0,11)\n",
        "# y = stats.poisson(mu=2).pmf(x)\n",
        "# dist = stats.poisson(mu=2).rvs(1000)\n",
        "# plt.plot(x, y, label=f'poisson: {round(stats.shapiro(y).pvalue,2)}')\n",
        "\n",
        "# plt.legend();"
      ],
      "metadata": {
        "id": "rHXacSjB8Brc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from numpy.lib import bincount\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib.ticker as ticker\n",
        "# import seaborn as sns\n",
        "# import scipy.stats as stats\n",
        "\n",
        "# n = 100\n",
        "# fig, ax = plt.subplots()\n",
        "# ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "# # normal\n",
        "# norm = np.random.normal(2, 1, size=n)\n",
        "# sns.kdeplot(norm, label=f'normal: {round(stats.shapiro(norm).pvalue,2)}', ax=ax)\n",
        "\n",
        "# # binomial\n",
        "# bino = np.random.binomial(10, .2, size=n)\n",
        "# sns.kdeplot(bino, label=f'binomial: {round(stats.shapiro(bino).pvalue,2)}', ax=ax)\n",
        "\n",
        "# # poisson\n",
        "# pois = np.random.poisson(2, n)\n",
        "# sns.kdeplot(pois, label=f'poisson: {round(stats.shapiro(pois).pvalue,2)}', ax=ax)\n",
        "\n",
        "# # gamma\n",
        "# gamma = np.random.gamma(3, 1, n)\n",
        "# sns.kdeplot(gamma, label=f'gamma: {round(stats.shapiro(gamma).pvalue,2)}', ax=ax)\n",
        "\n",
        "# plt.legend();"
      ],
      "metadata": {
        "id": "TKHCA06UMhtQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Probability Density Functions for Non-Normal Distributions\n",
        "\n",
        "* When a distribution is normal, we can perform significance tests, find confidence intervals, and use p-values, power, alpha, z-scores, etc\n",
        "* When not normal, the **normal** pdf won't work, so we use the appropirate function - e.g., for a Poisson distribution use the Poisson pdf (stats.poisson.pdf) (cdf, ppf, pmf, etc)\n",
        "* The probabilities associated with extreme values on a normal distribution will probably not be the same for other distributions (beta, poisson, gamma, etc)"
      ],
      "metadata": {
        "id": "IZlb5U_YwFAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practical Examples of Distributions\n",
        "\n",
        "* Normal: natural phenomena of one variate\n",
        "* Binomial: counting, gambling\n",
        "* Poisson: events happening in a period of time\n",
        "* Gamma: time to failure in mechanics, reliability in a piece of technology"
      ],
      "metadata": {
        "id": "w-aKAabxVsgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Derivative (Brief)\n",
        "\n",
        "* First\n",
        "* Second\n"
      ],
      "metadata": {
        "id": "GRir3VWGvZmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine you're driving a car. Let's break down how first and second derivatives relate to your trip:\n",
        "\n",
        "**1. First Derivative: Velocity (Speed and Direction)**\n",
        "\n",
        "* **What it tells you:** How fast your position is changing at a specific moment and in what direction (are you going forwards or backwards?).\n",
        "* **Example:** If your first derivative is 30 mph, you're moving forward at 30 mph. If it's -20 mph, you're moving backward at 20 mph.\n",
        "* **In a graph:** The first derivative represents the *slope* of the line tangent to the curve at any point. A steep uphill slope means a large positive first derivative (high speed in the positive direction). A flat line means a first derivative of zero (no change in position/stopped).\n",
        "\n",
        "**2. Second Derivative: Acceleration**\n",
        "\n",
        "* **What it tells you:** How fast your *velocity* is changing. Are you speeding up, slowing down, or staying at a constant speed?\n",
        "* **Example:** If your second derivative is 5 mph per second, you're increasing your speed by 5 mph every second. If it's -10 mph per second, you're braking and decreasing your speed by 10 mph every second.\n",
        "* **In a graph:** The second derivative tells you the *concavity* of the curve. A curve that's bending upwards (like a smile) has a positive second derivative (accelerating). A curve bending downwards (like a frown) has a negative second derivative (decelerating).\n",
        "\n",
        "**Difference in a nutshell:**\n",
        "\n",
        "* **First derivative:**  Measures the *rate of change* of a function (like position).\n",
        "* **Second derivative:** Measures the *rate of change of the rate of change* of a function (like the change in velocity).\n",
        "\n",
        "**Here's a table summarizing the key differences:**\n",
        "\n",
        "| Feature        | First Derivative | Second Derivative |\n",
        "|----------------|--------------------|---------------------|\n",
        "| Measures       | Rate of change     | Rate of change of the rate of change |\n",
        "| In motion      | Velocity         | Acceleration       |\n",
        "| In a graph     | Slope             | Concavity           |\n",
        "\n",
        "**Why are they important?**\n",
        "\n",
        "Derivatives help us understand how things change over time or with respect to other variables. They are fundamental to many areas of math, science, and engineering, including:\n",
        "\n",
        "* **Physics:** Understanding motion, forces, and energy.\n",
        "* **Economics:** Analyzing market trends and optimizing production.\n",
        "* **Machine learning:** Training models and optimizing algorithms.\n"
      ],
      "metadata": {
        "id": "T3KBh64g52hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Differention and Integration\n",
        "\n",
        "Differentiation and integration are two fundamental concepts in calculus that are closely related, yet they represent opposite operations. Here's a comparison to help you understand their key differences:\n",
        "\n",
        "**Differentiation**\n",
        "\n",
        "* **What it does:** Finds the instantaneous rate of change of a function. Think of it as finding the slope of a curve at a specific point.\n",
        "* **Geometric interpretation:**  The derivative at a point represents the slope of the tangent line to the curve at that point.\n",
        "* **Applications:**\n",
        "    * Finding velocities and accelerations in physics.\n",
        "    * Analyzing rates of growth or decay in biology and economics.\n",
        "    * Optimizing functions in machine learning and engineering.\n",
        "* **Example:** If you have a function describing the position of an object over time, the derivative gives you its velocity at any given time.\n",
        "\n",
        "**Integration**\n",
        "\n",
        "* **What it does:** Finds the area under a curve. Think of it as accumulating small changes over an interval.\n",
        "* **Geometric interpretation:** The definite integral represents the signed area between the curve and the x-axis over a given interval.\n",
        "* **Applications:**\n",
        "    * Calculating areas and volumes of irregular shapes.\n",
        "    * Finding the total displacement of an object given its velocity.\n",
        "    * Determining the work done by a force.\n",
        "    * Calculating probabilities in statistics.\n",
        "* **Example:** If you have a function describing the velocity of an object over time, the integral gives you its total displacement over a period.\n",
        "\n",
        "**Relationship:**\n",
        "\n",
        "* **Inverse operations:** Differentiation and integration are inverse operations. This means that if you differentiate a function and then integrate the result, you get back the original function (with a possible constant term added).\n",
        "\n",
        "**Analogy:**\n",
        "\n",
        "Imagine you're building a Lego tower:\n",
        "\n",
        "* **Differentiation:**  Like taking the tower apart brick by brick. You're analyzing the individual components and how they contribute to the overall structure.\n",
        "* **Integration:** Like putting the bricks together to build the tower. You're accumulating the individual components to create the whole.\n",
        "\n",
        "\n",
        "**Here's a table summarizing the key differences:**\n",
        "\n",
        "| Feature        | Differentiation | Integration |\n",
        "|----------------|--------------------|--------------|\n",
        "| Operation       | Finding rate of change | Finding area |\n",
        "| Geometric interpretation | Slope of tangent line | Area under curve |\n",
        "| Application    | Velocity, optimization | Displacement, volume |\n",
        "| Relationship to the other | Inverse operation | Inverse operation |\n",
        "\n",
        "\n",
        "By understanding the differences between differentiation and integration, you can gain a deeper understanding of calculus and its applications in various fields.\n"
      ],
      "metadata": {
        "id": "P7FyCQdw6aNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Maximum Likelihood Estimation (MLE) Review\n",
        "\n",
        "Imagine you're a detective at a crime scene. You find a footprint, but you don't know who it belongs to. You have a list of suspects, and you know the shoe size of each suspect.\n",
        "\n",
        "Maximum likelihood estimation (MLE) is like trying to find the suspect whose shoe size would most likely produce the footprint you found.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "1. **Assume a model:** You assume that the footprint was made by a shoe, and that shoe sizes follow a certain distribution (e.g., normal distribution).\n",
        "\n",
        "2. **Calculate the likelihood:** For each suspect, you calculate how likely it is that their shoe size would produce the observed footprint. This likelihood is based on your assumed model (the distribution of shoe sizes) and the evidence (the footprint size).\n",
        "\n",
        "3. **Find the maximum:** You choose the suspect whose shoe size has the *highest* likelihood of producing the observed footprint. This is your maximum likelihood estimate.\n",
        "\n",
        "**In simpler terms:**\n",
        "\n",
        "MLE is like finding the best guess for an unknown value (like the shoe size of the person who left the footprint) by finding the value that makes the observed data (the footprint) most probable.\n",
        "\n",
        "**Here's an example with coins:**\n",
        "\n",
        "You flip a coin 10 times and get 7 heads. You want to estimate the probability of getting heads (let's call it  'p').\n",
        "\n",
        "* **Assume a model:** You assume the coin flips follow a Bernoulli distribution (each flip has two possible outcomes: heads or tails).\n",
        "* **Calculate the likelihood:** For different values of 'p' (e.g., 0.1, 0.2, ..., 0.9), you calculate how likely it is to get 7 heads out of 10 flips.\n",
        "* **Find the maximum:** The value of 'p' that gives you the highest likelihood is your maximum likelihood estimate. In this case, it would be around 0.7.\n",
        "\n",
        "**What we're doing with MLE:**\n",
        "\n",
        "MLE aims to find the value of 'p' that makes the observed outcome (7 heads in 10 flips) the *most likely* outcome.  It's like saying, \"If the true probability of heads was actually [some value], how likely would it be to get the results I got?\" We try different values of 'p' and see which one gives us the highest probability of observing our data.\n",
        "\n",
        "**Why around 0.7?**\n",
        "\n",
        "Think about it intuitively:\n",
        "\n",
        "* If 'p' was very low (like 0.1), getting 7 heads out of 10 flips would be very unlikely.\n",
        "* If 'p' was very high (like 0.9), getting 7 heads would also be less likely (you'd expect more heads).\n",
        "* A value of 'p' around 0.7 seems to fit the observed data best. It makes sense that a coin with a 70% chance of landing heads would be more likely to produce 7 heads in 10 flips.\n",
        "\n",
        "**How do we actually find the MLE?**\n",
        "\n",
        "1. **Likelihood function:** We use a formula called the likelihood function to calculate the probability of observing our data (7 heads) for different values of 'p'.  For a Bernoulli distribution (like coin flips), the likelihood function looks like this:\n",
        "\n",
        "   ```\n",
        "   Likelihood(p) = p^k * (1-p)^(n-k)\n",
        "   ```\n",
        "\n",
        "   Where:\n",
        "     * `p` is the probability of heads\n",
        "     * `k` is the number of heads (7 in our case)\n",
        "     * `n` is the total number of flips (10 in our case)\n",
        "\n",
        "2. **Maximizing the likelihood:** We can try different values of 'p' (between 0 and 1) and calculate the likelihood for each. The value of 'p' that gives us the *highest* likelihood is our MLE.\n",
        "\n",
        "In practice, we often use calculus (finding the derivative and setting it to zero) or numerical methods to find the exact value of 'p' that maximizes the likelihood function. In this case, the MLE turns out to be 0.7.\n",
        "\n",
        "**Important note:**\n",
        "\n",
        "**The MLE is just an *estimate* based on the observed data. It's not necessarily the *true* value of 'p', but it's the value that best explains the data we have.**\n",
        "\n",
        "**Key takeaways:**\n",
        "\n",
        "* MLE is a method for estimating unknown parameters (like probabilities, averages, etc.) from data.\n",
        "* It finds the parameter value that makes the observed data most likely.\n",
        "* It relies on an assumed model for the data.\n",
        "\n",
        "MLE is a powerful tool used in many areas, from statistics and machine learning to economics and engineering.\n"
      ],
      "metadata": {
        "id": "Vx-54W5EsaBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayes v MLE\n",
        "\n",
        "It's important to understand how Maximum Likelihood Estimation (MLE) and Bayesian approaches differ, especially in how they incorporate prior knowledge. Bayesians believe getting heads is 50-50, a prior held assumption, while a frequentist might use MLE to understand the data at hand, not some theoretical scenario.\n",
        "\n",
        "**MLE (the 0.7 estimate):**\n",
        "\n",
        "* **Focus:** MLE only considers the observed data (7 heads in 10 flips). It tries to find the parameter value (probability of heads, 'p') that maximizes the likelihood of observing that data.\n",
        "* **No prior information:** MLE doesn't incorporate any prior beliefs or information about the coin. It acts as if we know nothing about the coin before the experiment.\n",
        "* **Result:** In this case, MLE suggests that the most likely value of 'p' is 0.7, based purely on the observed data.\n",
        "\n",
        "**Bayesian approach (the 0.5 estimate):**\n",
        "\n",
        "* **Prior knowledge:** Bayesian methods allow us to incorporate prior beliefs or information about the parameter. In our example, we're suggesting a prior belief that the coin is fair (p=0.5). This could be based on our general experience with coins or knowing that the coin is supposed to be standard.\n",
        "* **Updating beliefs:** The Bayesian approach combines the prior information with the observed data to update our belief about 'p'.\n",
        "* **Result:** With a strong prior belief that p=0.5, even after observing 7 heads in 10 flips, the Bayesian estimate might be closer to 0.5 than 0.7. The observed data influences the estimate, but the prior still pulls it towards 0.5.\n",
        "\n",
        "**How they compare:**\n",
        "\n",
        "* **Data vs. prior:** MLE relies solely on the data, while the Bayesian approach balances data with prior knowledge.\n",
        "* **Point estimate vs. distribution:** MLE gives you a single point estimate (0.7), while the Bayesian approach provides a posterior distribution over possible values of 'p'. This distribution reflects the remaining uncertainty after observing the data.\n",
        "\n",
        "**Which is better?**\n",
        "\n",
        "It depends on the situation and how much prior information you have:\n",
        "\n",
        "* **Limited data:** If you have very little data, a Bayesian approach with a reasonable prior can be helpful.\n",
        "* **Strong prior:** If you have strong prior knowledge (e.g., you know the coin is from a reputable mint), a Bayesian approach can incorporate that.\n",
        "* **No prior/objective analysis:** If you want an objective estimate purely based on the data, MLE is a good choice.\n",
        "\n"
      ],
      "metadata": {
        "id": "K0eOoUN77HLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the MLE of 0.7 for the coin flip probability is a frequentist interpretation.\n",
        "\n",
        "Here's why:\n",
        "\n",
        "* **Frequentist interpretation:** Frequentists view probability as the long-run frequency of an event. They would say that if you flip the coin many, many times, the proportion of heads will approach the true probability 'p'. MLE aligns with this view because it seeks the value of 'p' that would make the observed data (7 heads in 10 flips) most likely in the long run.\n",
        "\n",
        "* **No prior beliefs:** As we discussed, MLE doesn't incorporate any prior beliefs about the coin's fairness. It only considers the observed data, which is a key characteristic of frequentist methods.\n",
        "\n",
        "* **Contrast with Bayesian:**  Bayesian methods, on the other hand, allow for the incorporation of prior beliefs. They treat 'p' as a random variable with a prior distribution, which is updated based on the observed data to obtain a posterior distribution.\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "The MLE approach in the coin flip example embodies the frequentist perspective by:\n",
        "\n",
        "1. **Estimating 'p' based on the frequency of heads in the observed data.**\n",
        "2. **Not considering any prior information or beliefs about the coin.**\n",
        "\n",
        "This contrasts with the Bayesian approach, which would incorporate a prior distribution for 'p' (such as assuming a fair coin with p=0.5) and update that distribution based on the observed data.\n"
      ],
      "metadata": {
        "id": "c-FD5VHd7IU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, we often deal with complex loss functions (functions that measure the error of our model). These loss functions can have many \"hills and valleys,\" and our goal is to find the lowest point in this landscape, which represents the best set of parameters for our model.\n",
        "\n",
        "Here's how local and global minima fit into this picture:\n",
        "\n",
        "**Local Minimum**\n",
        "\n",
        "* **What it is:** A point in the \"landscape\" of the loss function where the function has the lowest value in its immediate neighborhood. Think of it as the bottom of a valley.\n",
        "* **Problem:** If your optimization algorithm (like gradient descent) gets stuck in a local minimum, it might think it has found the best solution, even though there might be a much deeper valley (a better solution) somewhere else.\n",
        "\n",
        "**Global Minimum**\n",
        "\n",
        "* **What it is:** The absolute lowest point in the entire landscape of the loss function. This represents the best possible solution (the set of parameters that minimizes the error the most).\n",
        "* **Goal:**  The ultimate goal of optimization in machine learning is to find the global minimum of the loss function.\n",
        "\n",
        "**Analogy:**\n",
        "\n",
        "Imagine you're trying to find the lowest point in a mountain range:\n",
        "\n",
        "* **Local minimum:** You might find yourself at the bottom of a small valley, thinking you've reached the lowest point.\n",
        "* **Global minimum:**  However, there might be a much deeper valley somewhere else in the mountain range that you haven't discovered yet.\n",
        "\n",
        "**Challenges:**\n",
        "\n",
        "* **Finding the global minimum can be very difficult,** especially with complex loss functions that have many local minima.\n",
        "* **Optimization algorithms can get trapped in local minima,** preventing them from finding the best solution.\n",
        "\n",
        "**Techniques to address the issue:**\n",
        "\n",
        "* **Different optimization algorithms:** Some algorithms, like stochastic gradient descent or simulated annealing, are designed to help escape local minima.\n",
        "* **Momentum:** Adding momentum to the optimization process can help \"overshoot\" local minima.\n",
        "* **Multiple starting points:**  Running the optimization from different starting points can increase the chances of finding the global minimum.\n",
        "\n",
        "\n",
        "Understanding the difference between local and global minima is crucial for effectively training machine learning models and avoiding suboptimal solutions.\n"
      ],
      "metadata": {
        "id": "FlS9-fay7rLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalized Linear Models\n",
        "\n",
        "* https://www.youtube.com/watch?v=SqN-qlQOM5A&t=18s"
      ],
      "metadata": {
        "id": "3TSWOr8PW3BC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using GLM to Model Responses\n",
        "\n",
        "#### Quantitative Response\n",
        "\n",
        "* $y = \\theta^T X + \\epsilon_i$ (matrix form)\n",
        "* where $\\epsilon \\overset{iid}{\\sim} \\mathcal{N}(0, \\sigma^2)$\n",
        "* and $\\epsilon$ is model noise and generally unknown\n",
        "\n",
        "This describes a **linear regression model**, which is used when the response variable ($\\mathbf{y}$) is **quantitative** (continuous, numerical, typically non-constrained).\n",
        "\n",
        "* **Equation:** $\\mathbf{y} = \\boldsymbol{\\theta}^T \\mathbf{X} + \\boldsymbol{\\epsilon}_i$ (matrix form)\n",
        "    * $\\mathbf{y}$: The vector of response (dependent) variables.\n",
        "    * $\\mathbf{X}$: The matrix of predictor (independent) variables.\n",
        "    * $\\boldsymbol{\\theta}$: The vector of model coefficients (parameters) to be estimated.\n",
        "    * $\\boldsymbol{\\theta}^T \\mathbf{X}$: The **linear predictor**, which is the core component of all GLMs.\n",
        "    * $\\boldsymbol{\\epsilon}_i$: The error or noise term.\n",
        "* **Assumptions on Error:** $\\boldsymbol{\\epsilon} \\stackrel{\\text{iid}}{\\sim} \\mathbf{N}(0, \\sigma^2)$\n",
        "    * The error terms ($\\boldsymbol{\\epsilon}$) are assumed to be **independent and identically distributed (iid)** following a **Normal (Gaussian) distribution** $\\mathbf{N}$ with a mean of **zero** and a constant variance of $\\sigma^2$. This is a key assumption of standard linear regression.\n",
        "* **Error Nature:** \"and $\\boldsymbol{\\epsilon}$ is model noise and generally unknown\"\n",
        "    * The error represents the part of the response that the predictors $\\mathbf{X}$ cannot explain.\n",
        "\n",
        "#### Binary Response\n",
        "\n",
        "* $p(y=1) = \\Large\\frac{1}{1 + e^{-(\\theta^T X)}}$\n",
        "\n",
        "This describes a **Logistic Regression** model, which is a type of GLM used when the response variable is **binary** (e.g., 0 or 1, success or failure, yes or no).\n",
        "\n",
        "* **Equation:** $p(\\mathbf{y} = 1) = \\frac{1}{1 + e^{-(\\boldsymbol{\\theta}^T \\mathbf{X})}}$\n",
        "    * $p(\\mathbf{y} = 1)$: The probability that the binary response $\\mathbf{y}$ equals 1.\n",
        "    * This formula is the **logistic function** (or sigmoid function). It takes the **linear predictor** ($\\boldsymbol{\\theta}^T \\mathbf{X}$), which can range from $-\\infty$ to $+\\infty$, and transforms it into a probability that ranges from **0 to 1**.\n",
        "    * The link function here is the **logit** function, which relates the linear predictor to the mean of the response distribution.\n",
        "\n",
        "#### Generalized Linear Model\n",
        "\n",
        "* Other responses include non-negative, skewed, etc\n",
        "* Framework for different response types\n",
        "* Useful when models violate assumptions\n",
        "* Assumptions: https://github.com/gitmystuff/DSChunks/blob/main/Assumptions.ipynb\n",
        "\n",
        "This is a summary of the broader **Generalized Linear Model** framework. GLMs unify models like Linear Regression and Logistic Regression.\n",
        "\n",
        "* **Other Responses:** \"Other responses include non-negative, skewed, etc\"\n",
        "    * GLMs can handle response variables that follow many distributions, such as **Poisson** (for count data), **Gamma** or **Inverse Gaussian** (for non-negative or skewed continuous data), and others.\n",
        "* **Framework for different response types:**\n",
        "    * GLMs use three components to connect the response $\\mathbf{y}$ to the predictors $\\mathbf{X}$:\n",
        "        1.  **Random Component:** Specifies the probability distribution of the response variable (e.g., Normal for quantitative, Bernoulli for binary, Poisson for count).\n",
        "        2.  **Systematic Component:** The **linear predictor** ($\\boldsymbol{\\theta}^T \\mathbf{X}$).\n",
        "        3.  **Link Function:** Specifies the function that relates the mean of the response distribution to the linear predictor (e.g., identity for Normal, logit for Bernoulli, log for Poisson).\n",
        "* **Useful when models violate assumptions:**\n",
        "    * Standard linear regression requires the response to be normally distributed and have constant variance. When these **assumptions are violated** (e.g., with count data or probabilities), GLMs provide the appropriate tools to model the data by using a different distribution (random component) and link function.\n",
        "* **See Assumptions Notebook:** https://github.com/gitmystuff/DSChunks/blob/main/Assumptions.ipynb\n",
        "\n"
      ],
      "metadata": {
        "id": "8MO7LaQDYFhE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How the General Linear Model Fits in with T-tests and ANOVA\n",
        "\n",
        "The **General Linear Model** can be written as:\n",
        "\n",
        "$\n",
        "y = \\beta_0 + \\beta_1 X + e\n",
        "$\n",
        "\n",
        "where\n",
        "\n",
        "* ( $y$ ) = outcome (dependent variable)\n",
        "* ( $\\beta_0$ ) = intercept (baseline value or mean)\n",
        "* ( $\\beta_1$ ) = slope (mean difference or effect of predictor)\n",
        "* ( $X$ ) = predictor variable\n",
        "* ( $e$ ) = error term (residuals)\n",
        "\n",
        "This general form, ( $\\text{Outcome} = \\text{Intercept} + \\text{Slope} \\times \\text{Predictor} + e$ ), is flexible enough to represent several common statistical tests.\n",
        "\n",
        "---\n",
        "\n",
        "#### Examples of How Common Tests Fit the GLM\n",
        "\n",
        "1. **One-sample t-test**\n",
        "   * $y = \\beta_0 + e$\n",
        "   * Here, the model estimates the mean of ( $y$ ). The null hypothesis tests whether this mean (( $\\beta_0$ )) equals zero or some other constant.\n",
        "\n",
        "2. **Independent t-test**\n",
        "   * $y = \\beta_0 + \\beta_1 \\text{Group} + e$\n",
        "   * $\\beta_0$ represents the mean of our reference group.\n",
        "   * $\\beta_1$ is the slope, expressing the change (difference) between our reference group (Group 1) and the comparison $Group$.\n",
        "\n",
        "3. **Paired t-test**\n",
        "   * $y_1 - y_2 = \\beta_0 + e$\n",
        "   * The model focuses on the difference between paired observations.\n",
        "   The intercept ( $\\beta_0$ ) represents the mean difference.\n",
        "   * An example of a paired test would be to take the mean before a treatment and then after a treatment and compare the means.\n",
        "\n",
        "4. **ANOVA (Analysis of Variance)**\n",
        "   * $y = \\beta_0 + \\beta_1 \\text{Group}_1 + \\beta_2 \\text{Group}_2 + e$\n",
        "   * For more than two groups, each group gets its own coefficient representing its deviation from the reference group’s mean.\n",
        "\n",
        "---\n",
        "\n",
        "#### Intercepts and Slopes vs. Means and Mean Differences\n",
        "\n",
        "* **Intercept (β₀)** → corresponds to a **mean**\n",
        "* **Slope (β₁, β₂, …)** → correspond to **mean differences** between groups or conditions\n",
        "\n",
        "---\n",
        "\n",
        "#### In summary\n",
        "\n",
        "All t-tests and ANOVA are special cases of the General Linear Model (GLM). They differ only in how the predictor variable(s) are coded (one group, two groups, or multiple groups), but conceptually they all estimate **means** (intercepts) and **mean differences** (slopes).\n"
      ],
      "metadata": {
        "id": "qYNFYGr_8KLz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generalized Linear Model Components\n",
        "\n",
        "#### 1. Random Component - Probability Distribution\n",
        "\n",
        "The **Random Component** specifies the **probability distribution** for the response variable, $Y$. This distribution must be part of the **exponential family** of distributions.\n",
        "\n",
        "* It accounts for the **random variation** or noise in the data.\n",
        "* The choice of distribution depends on the nature of the response variable. For example:\n",
        "    * **Normal distribution** for continuous data (as in standard linear regression).\n",
        "    * **Binomial distribution** for binary data (success/failure) or proportion data (as in logistic regression).\n",
        "    * **Poisson distribution** for count data (as in Poisson regression).\n",
        "\n",
        "***\n",
        "\n",
        "#### 2. Systematic Component - Coefficients + Explanatory Variables\n",
        "\n",
        "The **Systematic Component** defines the linear predictor, $\\eta$ (eta), which is a **linear combination** of the **explanatory variables** (also called predictors or covariates) and their corresponding **coefficients** (parameters).\n",
        "\n",
        "* It captures the **structure** and **relationship** between the explanatory variables ($X_1, X_2, \\dots, X_p$) and the response.\n",
        "* It looks like a standard linear equation:\n",
        "    $$\\eta = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p$$\n",
        "    where the $\\beta$'s are the coefficients to be estimated from the data.\n",
        "\n",
        "***\n",
        "\n",
        "#### 3. Link Function - Transforms (Relates) the Mean to the Explanatory Variables\n",
        "\n",
        "The **Link Function**, $g(\\mu)$, provides the crucial **link** between the **expected value (mean)** of the response variable, $\\mu = E(Y)$, and the **linear predictor** ($\\eta$) from the systematic component.\n",
        "\n",
        "* It relates the mean of the distribution (Random Component) to the linear combination of predictors (Systematic Component).\n",
        "* Specifically, it **transforms** the mean, $\\mu$, so that the transformed value is linearly related to the predictors:\n",
        "    $$g(\\mu) = \\eta$$\n",
        "    $$g(\\mu) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p$$\n",
        "* The link function ensures that the relationship is linear *on the scale of the predictor* and also ensures that the **predicted mean ($\\mu$) remains within the valid range** for the chosen probability distribution (e.g., between 0 and 1 for the Binomial distribution, or non-negative for the Poisson distribution).\n",
        "    * Examples include the **identity link** (for Normal/Gaussian distribution, where $\\mu = \\eta$), the **logit link** (for Binomial/Logistic regression, where $\\text{logit}(\\mu) = \\eta$), and the **log link** (for Poisson regression, where $\\log(\\mu) = \\eta$)."
      ],
      "metadata": {
        "id": "lWuyOC6VHpSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameters of Distributions in the Random Component\n",
        "\n",
        "#### Gaussian (Normal) Distribution\n",
        "\n",
        "The Gaussian distribution, foundational to traditional linear regression, is characterized by two parameters:\n",
        "\n",
        "* **Mean ($\\mu$):** The **location** parameter, representing the center of the distribution. In a GLM, the mean is related to the systematic component via the link function.\n",
        "* **Variance ($\\sigma^2$):** The **scale** parameter, representing the spread or variability of the data around the mean. For standard linear models, this is typically assumed to be constant.\n",
        "\n",
        "---\n",
        "\n",
        "#### Binomial Distribution\n",
        "\n",
        "The Binomial distribution models the number of successes in a fixed number of independent trials. It uses two parameters:\n",
        "\n",
        "* **Independent Trials ($n$):** The total number of fixed, known experiments or observations.\n",
        "* **Success Probability ($p$):** The probability of a successful outcome on any single trial. In a GLM (like logistic regression), the link function relates this probability, $p$ (the mean of the distribution for a single trial), to the linear predictor.\n",
        "\n",
        "---\n",
        "\n",
        "#### Poisson Distribution\n",
        "\n",
        "The Poisson distribution is used for modeling **count data** (e.g., number of events in a time interval). It is defined by a single parameter:\n",
        "\n",
        "* **Rate/Mean ($\\lambda$):** This parameter represents both the **mean** ($\\mu$) and the **variance** ($\\sigma^2$) of the distribution, due to the defining constraint that $\\text{Mean} = \\text{Variance}$ ($\\mu = \\sigma^2$). The link function (usually the log link) connects this mean to the systematic component.\n",
        "\n",
        "---\n",
        "\n",
        "#### Gamma Distribution\n",
        "\n",
        "The Gamma distribution is often employed for modeling continuous, positive, and typically **right-skewed data** (e.g., waiting times). It's characterized by two parameters:\n",
        "\n",
        "* **Shape ($\\alpha$):** Influences the overall form or contour of the distribution.\n",
        "* **Rate ($\\beta$):** Controls the inverse of the scale of the distribution. The mean of the distribution is given by the ratio $\\mu = \\alpha / \\beta$.\n",
        "\n",
        "---\n",
        "\n",
        "#### Negative-Binomial Distribution\n",
        "\n",
        "This distribution is an essential extension of the Poisson, specifically designed for **overdispersed count data** where the variance is greater than the mean ($\\sigma^2 > \\mu$). It has two key parameters:\n",
        "\n",
        "* **Mean ($\\mu$):** The expected number of counts, which is connected to the linear predictor by the link function.\n",
        "* **Dispersion Parameter ($\\alpha$ or $k$):** This **shape parameter** is an extra component that models the excess variance (overdispersion). The variance is $\\text{Variance} = \\mu + \\alpha \\mu^2$, allowing the variance to exceed the mean.\n",
        "\n",
        "---\n",
        "\n",
        "#### Inverse Gaussian Distribution\n",
        "\n",
        "The Inverse Gaussian distribution is used for continuous, positive, and typically skewed data, often relating to processes like **first-passage times**. It is defined by:\n",
        "\n",
        "* **Mean ($\\mu$):** The expected value of the distribution, which is linked to the systematic component.\n",
        "* **Shape ($\\lambda$):** A parameter that controls the overall shape and variability of the distribution."
      ],
      "metadata": {
        "id": "tw5N8PCLB8_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Link Functions\n",
        "\n",
        "Allows the linear model to be related to the response variable by transforming the output of the response to fit the model boundaries.\n",
        "\n",
        "$\\eta$ = link\n",
        "\n",
        "* linear: $\\eta (\\mu) = \\mu$\n",
        "* logistic: $\\eta(\\mu) = log(\\mu/(1-\\mu))$\n",
        "* poisson: $\\eta(\\mu) = log(\\mu)$"
      ],
      "metadata": {
        "id": "HD3zwp2eF8Vz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The link function is a crucial component of a Generalized Linear Model (GLM). It provides the connection between the linear predictor (a linear combination of the predictors) and the expected value of the response variable.  Think of it as a translator that allows you to model different types of response variables within a linear framework.\n",
        "\n",
        "Here's a breakdown of what the link function does and why it's important:\n",
        "\n",
        "**1. Linear Predictor**\n",
        "\n",
        "In a GLM, you have a linear predictor, which is simply a weighted sum of your predictor variables (features). It looks like this:\n",
        "\n",
        "  `linear predictor = b0 + b1*x1 + b2*x2 + ...`\n",
        "\n",
        "where:\n",
        "\n",
        "* `b0`, `b1`, `b2`, etc. are the coefficients estimated by the model.\n",
        "* `x1`, `x2`, etc. are the predictor variables.\n",
        "\n",
        "**2.  Connecting to the Response**\n",
        "\n",
        "The link function takes this linear predictor and transforms it to match the expected value of the response variable. This is necessary because the response variable might not have a linear relationship with the predictors and might have specific constraints.\n",
        "\n",
        "**3.  Examples of Link Functions**\n",
        "\n",
        "* **Identity link:** `g(μ) = μ` (no transformation) - Used for continuous data with a normal distribution (simple linear regression).\n",
        "* **Logit link:** `g(μ) = log(μ / (1 - μ))` - Used for binary data (logistic regression). It maps probabilities (between 0 and 1) to a range from negative to positive infinity.\n",
        "* **Log link:** `g(μ) = log(μ)` - Used for count data (Poisson regression) or continuous positive data. It ensures the predicted values are positive.\n",
        "* **Inverse link:** `g(μ) = 1 / μ` - Used when the response is inversely proportional to the linear predictor.\n",
        "\n",
        "**4.  Why is the Link Function Important?**\n",
        "\n",
        "* **Flexibility:** It allows GLMs to model various types of response variables (continuous, binary, count, etc.) within a unified framework.\n",
        "* **Interpretability:** It provides a meaningful way to interpret the relationship between the predictors and the response variable on a suitable scale.\n",
        "* **Statistical Properties:**  It ensures that the model's predictions satisfy the properties of the chosen response distribution.\n",
        "\n",
        "**5. Choosing the Link Function**\n",
        "\n",
        "The choice of link function depends on the nature of your response variable and the assumptions you're making about its distribution. Here's a general guideline:\n",
        "\n",
        "* **Normally distributed:** Identity link\n",
        "* **Positive skewed data:** Log link\n",
        "* **Binary data:** Logit link\n",
        "* **Count data:** Log link\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "The link function is a key component of a GLM that connects the linear predictor to the expected value of the response variable. It allows you to model different types of response variables and ensures that the model's predictions are meaningful and statistically sound.\n"
      ],
      "metadata": {
        "id": "AhGDFubS3ZaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Review of Functions"
      ],
      "metadata": {
        "id": "xd1IFZ3j-3FK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # link functions\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# fig = plt.figure()\n",
        "# ax1 = fig.add_subplot(321)\n",
        "# ax2 = fig.add_subplot(322)\n",
        "# ax3 = fig.add_subplot(323)\n",
        "# ax4 = fig.add_subplot(324)\n",
        "# ax5 = fig.add_subplot(325)\n",
        "# ax6 = fig.add_subplot(326)\n",
        "\n",
        "# x = np.linspace(-10, 10, 1000)\n",
        "\n",
        "# y = x ** 2\n",
        "# ax1.grid()\n",
        "# ax1.plot(x, y)\n",
        "# ax1.title.set_text(r'$y=x^2$')\n",
        "# # ax1.title.set_text('y=x^2')\n",
        "# ax1.set_ylabel('Quadratic')\n",
        "\n",
        "# y = np.sqrt(x+10)\n",
        "# ax2.grid()\n",
        "# ax2.plot(x, y)\n",
        "# ax2.title.set_text(r'$y=\\sqrt{x+10}$')\n",
        "# # ax2.title.set_text('y=sqrt(x)')\n",
        "# ax2.set_ylabel('Square Root')\n",
        "\n",
        "# y = np.e**(0.1*x)\n",
        "# ax3.grid()\n",
        "# ax3.plot(x, y)\n",
        "# ax3.title.set_text(r'$y=e^{(0.1x)}$')\n",
        "# # ax3.title.set_text('y=e^(0.1x)')\n",
        "# ax3.set_ylabel('Exponential')\n",
        "\n",
        "# y = 1/(x+10)\n",
        "# ax4.grid()\n",
        "# ax4.plot(x, y)\n",
        "# ax4.title.set_text(r'$y=1/(x+10)$')\n",
        "# # ax4.title.set_text('y=1/(x+10)')\n",
        "# ax4.set_ylabel('Inverse')\n",
        "\n",
        "# y = 1/(1 + np.e**-(x))\n",
        "# ax5.grid()\n",
        "# ax5.plot(x, y)\n",
        "# ax5.title.set_text(r'$y=1/(1+e^{-(x)})$')\n",
        "# # ax5.title.set_text('y=1/(1+e^-(x)')\n",
        "# ax5.set_ylabel('Logistic')\n",
        "\n",
        "# y = np.log(x)\n",
        "# ax6.grid()\n",
        "# ax6.plot(x, y)\n",
        "# ax6.title.set_text(r'$y=log(x)$')\n",
        "# # ax6.title.set_text('y=log(x)')\n",
        "# ax6.set_ylabel('Log')\n",
        "\n",
        "# plt.tight_layout();"
      ],
      "metadata": {
        "id": "eesT7JqxBTgS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Family\n",
        "\n",
        "The `family` parameter in a Generalized Linear Model (GLM) is crucial because it specifies the **probability distribution** of your response variable (the variable you're trying to predict). This distribution determines the relationship between the linear predictor (a combination of predictors and coefficients) and the expected value of the response.\n",
        "\n",
        "Here's a breakdown of why the `family` parameter is important and how it works:\n",
        "\n",
        "**1.  Connecting the Linear Predictor to the Response**\n",
        "\n",
        "In a GLM, the link function connects the linear predictor to the response variable. However, the link function alone doesn't fully define the relationship. The `family` parameter specifies the distribution of the response, which, in conjunction with the link function, determines how the linear predictor is related to the expected value of the response.\n",
        "\n",
        "**2.  Common Families and Their Uses**\n",
        "\n",
        "Here are some common families used in GLMs:\n",
        "\n",
        "* **Gaussian (Normal):**  Used for continuous data that is normally distributed. This is the default family for many GLM implementations.\n",
        "* **Binomial:** Used for binary data (e.g., success/failure, presence/absence) or count data where the number of trials is fixed.\n",
        "* **Poisson:** Used for count data where the events occur independently at a constant rate (e.g., the number of customers arriving at a store per hour).\n",
        "* **Gamma:** Used for continuous positive data that is skewed (e.g., income, waiting times).\n",
        "* **Inverse Gaussian:**  Used for continuous positive data with a specific type of skewness.\n",
        "\n",
        "**3.  Choosing the Right Family**\n",
        "\n",
        "The choice of family depends on the nature of your response variable and the assumptions you're willing to make about its distribution. Here's a general guideline:\n",
        "\n",
        "* **Normally distributed:** Gaussian family\n",
        "* **Skewed positive data:** Gamma or Inverse Gaussian family\n",
        "* **Binary data:** Binomial family\n",
        "* **Count data:** Poisson family\n",
        "\n",
        "**4.  Example: `statsmodels` in Python**\n",
        "\n",
        "In `statsmodels`, you specify the family using the `family` argument in the `GLM` class. For example:\n",
        "\n",
        "```python\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Fit a GLM with a Gaussian family and identity link (default)\n",
        "model = sm.GLM(y, X, family=sm.families.Gaussian())\n",
        "```\n",
        "\n",
        "This code specifies a Gaussian family for the response variable `y`. You can change the family to `sm.families.Binomial()`, `sm.families.Poisson()`, etc., depending on your data.\n",
        "\n",
        "**Key Takeaways**\n",
        "\n",
        "* The `family` parameter in a GLM determines the probability distribution of the response variable.\n",
        "* It works in conjunction with the link function to define the relationship between the linear predictor and the response.\n",
        "* Choosing the appropriate family is crucial for building a valid and meaningful GLM.\n",
        "* The choice of family should be based on the nature of your response variable and the assumptions you're making about its distribution.\n"
      ],
      "metadata": {
        "id": "w87hXTW03Lvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Linear Regression\n",
        "\n",
        "The link function for simple linear regression is the **identity function**.\n",
        "\n",
        "Here's why:\n",
        "\n",
        "* **GLMs and Link Functions:** Generalized Linear Models (GLMs) extend traditional linear regression to accommodate various types of response variables (continuous, binary, counts, etc.).  A key component of a GLM is the *link function*, which establishes a connection between the linear predictor (the familiar $mx + b$ in simple linear regression) and the expected value of the response variable.\n",
        "\n",
        "* **Identity Function:** The identity function is simply a function that returns its input unchanged.  Mathematically, it's represented as  $f(x) = x$.\n",
        "\n",
        "* **Simple Linear Regression:** In simple linear regression, we're modeling a continuous response variable with a normal distribution.  The expected value of the response variable is directly equal to the linear predictor.  In other words, we don't need any transformation to map the linear predictor to the expected value.  This \"no transformation\" is precisely what the identity function accomplishes. The identity link function in simple linear regression preserves the linear relationship between the predictor and the response variable. It's the simplest and most direct way to model this type of relationship within the GLM framework.\n",
        "\n",
        "**In summary:** The identity link function in simple linear regression implies that the predicted value from the linear equation ($mx + b$) is directly used as the expected value of the response variable.\n"
      ],
      "metadata": {
        "id": "ZdDs2wSBZ75O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison Between OLS and GLM"
      ],
      "metadata": {
        "id": "ZDNIhbG1oP7g"
      }
    },
    {
      "source": [
        "# import numpy as np\n",
        "# import statsmodels.api as sm\n",
        "# from sklearn.datasets import make_regression\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Generate data with 3 features and 1 target\n",
        "# X, y = make_regression(n_samples=100, n_features=3, noise=10, random_state=42)\n",
        "\n",
        "# # Add a constant term to the predictor matrix for the intercept\n",
        "# X = sm.add_constant(X)\n",
        "\n",
        "# # Fit OLS model\n",
        "# ols_model = sm.OLS(y, X).fit()\n",
        "\n",
        "# # Fit GLM model (with Gaussian family and identity link, equivalent to OLS)\n",
        "# glm_model = sm.GLM(y, X, family=sm.families.Gaussian()).fit()\n",
        "\n",
        "# # Print model summaries\n",
        "# print(\"OLS Model Summary:\")\n",
        "# print(ols_model.summary())\n",
        "# print(\"\\nGLM Model Summary:\")\n",
        "# print(glm_model.summary())\n",
        "\n",
        "# # Compare some key statistics\n",
        "# print(\"\\nComparison:\")\n",
        "# print(f\"OLS R-squared: {ols_model.rsquared:.4f}\")\n",
        "# # Use pseudo-R-squared for GLM (e.g., McFadden's R-squared)\n",
        "# print(f\"GLM Pseudo-R-squared (McFadden): {1 - glm_model.llf / glm_model.llnull:.4f}\")\n",
        "# print(f\"OLS AIC: {ols_model.aic:.4f}\")\n",
        "# print(f\"GLM AIC: {glm_model.aic:.4f}\")\n",
        "\n",
        "# # You can also access other statistics like BIC, log-likelihood, etc.\n",
        "\n",
        "# # Visualize predictions (optional)\n",
        "# y_pred_ols = ols_model.predict(X)\n",
        "# y_pred_glm = glm_model.predict(X)\n",
        "\n",
        "# plt.scatter(y, y_pred_ols, label=\"OLS\", alpha=0.5)\n",
        "# plt.scatter(y, y_pred_glm, label=\"GLM\", alpha=0.2)\n",
        "# plt.xlabel(\"True Values\")\n",
        "# plt.ylabel(\"Predicted Values\")\n",
        "# plt.title(\"OLS vs. GLM Predictions\")\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "aCkB8AWXnHl1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's important to remember that **R-squared is not a suitable measure of goodness-of-fit for GLMs in general**, and especially not for those with non-linear link functions like the log link.\n",
        "\n",
        "R-squared is based on the concept of variance explained, which doesn't directly translate to GLMs with non-identity link functions.\n",
        "\n",
        "Instead of R-squared, you should use other metrics to assess the goodness-of-fit for your GLM, such as:\n",
        "\n",
        "* **AIC (Akaike Information Criterion):**  A measure of relative model quality that penalizes models with more parameters. Lower AIC values indicate a better fit.\n",
        "* **BIC (Bayesian Information Criterion):** Similar to AIC, but with a stronger penalty for model complexity.\n",
        "* **Deviance:** A measure of the difference between the fitted model and the saturated model (a model that perfectly fits the data). Lower deviance indicates a better fit.\n",
        "* **Log-likelihood:** The logarithm of the likelihood function, which measures how well the model explains the observed data. Higher log-likelihood indicates a better fit.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gsr0DurQn5GM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Please explain the difference between the two table outputs above (the OLS and the GLM Regression Results):\n",
        "\n",
        "ANSWER HERE:"
      ],
      "metadata": {
        "id": "i6R1xUuEGDiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quadratic Function"
      ],
      "metadata": {
        "id": "0ehiSdlcf0WC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# fig = plt.figure(figsize=(2,1))\n",
        "# x = np.linspace(-10, 10, 1000)\n",
        "# y = x ** 2\n",
        "# plt.plot(x, y)\n",
        "# plt.title(r'$y=x^2$')\n",
        "# plt.ylabel('Quadratic')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "tQXfhSyUfR46"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import statsmodels.api as sm\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Generate data and set variables\n",
        "# np.random.seed(0)\n",
        "# title = 'Quadratic GLM'\n",
        "# family = sm.families.Gaussian()\n",
        "# link_function = lambda x: x**2\n",
        "# x = np.linspace(-10, 10, 100)\n",
        "# y = link_function(x) + np.random.normal(0, 10, 100)  # Add some noise\n",
        "\n",
        "# # GLM modeling\n",
        "# X = np.column_stack((x, link_function(x)))\n",
        "# X = sm.add_constant(X)  # Add a constant for the intercept\n",
        "# model = sm.GLM(y, X, family=family).fit()\n",
        "# predictions = model.predict(X)\n",
        "# print(model.summary())\n",
        "\n",
        "# # Plot the data and the fitted curve (predictions)\n",
        "# plt.scatter(x, y)\n",
        "# plt.plot(x, predictions, color='red')\n",
        "# plt.xlabel('x')\n",
        "# plt.ylabel('y')\n",
        "# plt.title(f'{title}')\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "nSa8pUvGf2n4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why are we able to use a Gaussian family and still model a quadratic distribution? What is the link function?\n",
        "\n",
        "ANSWER HERE:"
      ],
      "metadata": {
        "id": "zgWxbOTqG9dp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Square Root Function\n",
        "\n",
        "x values must be non-negative for sqrt"
      ],
      "metadata": {
        "id": "dU6S1yjmgglZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# fig = plt.figure(figsize=(2,1))\n",
        "# x = np.linspace(-10, 10, 1000)\n",
        "# y = np.sqrt(x+10)\n",
        "# plt.plot(x, y)\n",
        "# plt.title(r'$y=\\sqrt{x+10}$')\n",
        "# plt.ylabel('Square Root')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "4dT0712lhAYi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import statsmodels.api as sm\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Generate data and set variables\n",
        "# np.random.seed(0)\n",
        "# title = 'Square Root GLM'\n",
        "# family = sm.families.Gaussian()\n",
        "# link_function = lambda x: np.sqrt(x+10)\n",
        "# x = np.linspace(-10, 10, 100)  # x values must be non-negative for sqrt\n",
        "# y = link_function(x) + np.random.normal(0, 0.5, 100)  # Add some noise\n",
        "\n",
        "# # GLM modeling\n",
        "# X = sm.add_constant(x)  # Add a constant for the intercept\n",
        "# # statsmodels doesn't directly support a square root link function for the Gaussian family\n",
        "# y_transformed = y ** 2  # Square the y values\n",
        "# model = sm.GLM(y_transformed, X, family=family).fit()\n",
        "# y_pred_transformed = model.predict(X) # make predictions (on the transformed scale)\n",
        "# predictions = np.sqrt(y_pred_transformed) # Take the square root\n",
        "# print(model.summary())\n",
        "\n",
        "# # Plot the data and the fitted curve (predictions on the original scale)\n",
        "# plt.scatter(x, y)\n",
        "# plt.plot(x, predictions, color='red')\n",
        "# plt.xlabel('x')\n",
        "# plt.ylabel('y')\n",
        "# plt.title(f'{title}')\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "_yXxirN6hv6N"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why y transformation is necessary:\n",
        "\n",
        "* **Desired Relationship:** The code aims to model a square root relationship between `x` and `y`, as indicated by the `link_function = lambda x: np.sqrt(x+10)`. This means that `y` should increase roughly as the square root of `x`.\n",
        "\n",
        "* **Limitations of `statsmodels`:**  While `statsmodels` provides a flexible framework for GLMs, it doesn't directly support a square root link function for the Gaussian family.\n",
        "\n",
        "* **Transformation as a Solution:** To overcome this limitation, the code applies a transformation to the response variable (`y`) *before* fitting the GLM. By squaring `y` (`y_transformed = y ** 2`), the relationship between `x` and `y_transformed` becomes approximately linear.\n",
        "\n",
        "* **Fitting a Linear Model:** Now that the relationship is linear on the transformed scale, a standard GLM with an identity link function can be used to model the data effectively.\n",
        "\n",
        "* **Back-Transformation:** After fitting the model and making predictions on the transformed scale (`y_pred_transformed`), the code applies the inverse transformation (square root) to get predictions on the original scale of `y` (`predictions = np.sqrt(y_pred_transformed)`).\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "Transforming `y` allows you to model a non-linear (square root) relationship using a linear model (GLM with identity link). It's a common technique in statistics when the desired relationship doesn't directly fit into the available model framework.\n",
        "\n",
        "This approach demonstrates the flexibility of GLMs and how transformations can be used to extend their capabilities to model various types of relationships.\n"
      ],
      "metadata": {
        "id": "U3hezH1hoeiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exponential Function\n",
        "\n",
        "family=sm.families.Gaussian(link=sm.families.links.log())"
      ],
      "metadata": {
        "id": "9ssBI2ARiT2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import statsmodels.api as sm\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# link_function = lambda x: np.exp(0.1 * x)\n",
        "\n",
        "# # Generate data with an exponential relationship\n",
        "# np.random.seed(0)\n",
        "# x = np.linspace(-10, 10, 100)\n",
        "# y = link_function(x) + np.random.normal(0, 0.2, 100)  # Add some noise\n",
        "\n",
        "# # Create the design matrix\n",
        "# X = sm.add_constant(x)\n",
        "\n",
        "# # Fit the GLM with a Gaussian family and exponential link function\n",
        "# model = sm.GLM(y, X, family=sm.families.Gaussian(link=sm.families.links.log()))\n",
        "# results = model.fit()\n",
        "\n",
        "# # Print the model summary\n",
        "# print(results.summary())\n",
        "\n",
        "# # Make predictions\n",
        "# predictions = results.predict(X)\n",
        "\n",
        "# # Plot the data and the fitted curve\n",
        "# def plot_GLM(x, y, predictions, title):\n",
        "#   plt.scatter(x, y)\n",
        "#   plt.plot(x, predictions, color='red')\n",
        "#   plt.xlabel('x')\n",
        "#   plt.ylabel('y')\n",
        "#   plt.title('Exponential GLM')\n",
        "#   plt.grid(True)\n",
        "#   plt.show()\n",
        "\n",
        "#   return None\n",
        "\n",
        "# plot_GLM(x, y, predictions, title)"
      ],
      "metadata": {
        "id": "WKJWIZE7sFA_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import statsmodels.api as sm\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Generate data and set variables\n",
        "# np.random.seed(42)\n",
        "# title = 'Exponential GLM'\n",
        "# family = sm.families.Gaussian(link=sm.families.links.Log())\n",
        "# link_function = lambda x: np.exp(0.1 * x)\n",
        "# x = np.linspace(-10, 10, 100)  # x values must be non-negative for sqrt\n",
        "# y = link_function(x) + np.random.normal(0, 0.2, 100)  # Add some noise\n",
        "\n",
        "# # GLM modeling\n",
        "# def model_GLM(x, y, family):\n",
        "#   X = sm.add_constant(x)\n",
        "#   model = sm.GLM(y, X, family=family).fit()\n",
        "#   predictions = model.predict(X)\n",
        "#   return model, predictions\n",
        "\n",
        "# # Plot the data and the fitted curve\n",
        "# def plot_GLM(x, y, predictions, title):\n",
        "#   plt.scatter(x, y)\n",
        "#   plt.plot(x, predictions, color='red')\n",
        "#   plt.xlabel('x')\n",
        "#   plt.ylabel('y')\n",
        "#   plt.title(f'{title}')\n",
        "#   plt.grid(True)\n",
        "#   plt.show()\n",
        "#   return None\n",
        "\n",
        "# plot_GLM(x, y, predictions, title)\n",
        "# model, predictions = model_GLM(x, y, family)\n",
        "# print(model.summary())\n"
      ],
      "metadata": {
        "id": "6xEmyYEMihyU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import statsmodels.api as sm\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # GLM modeling\n",
        "# def model_GLM(x, y, family):\n",
        "#   X = sm.add_constant(x)\n",
        "#   model = sm.GLM(y, X, family=family).fit()\n",
        "#   predictions = model.predict(X)\n",
        "#   return model, predictions\n",
        "\n",
        "# # Plot the data and the fitted curve\n",
        "# def plot_GLM(x, y, predictions, title):\n",
        "#   plt.scatter(x, y)\n",
        "#   plt.plot(x, predictions, color='red')\n",
        "#   plt.xlabel('x')\n",
        "#   plt.ylabel('y')\n",
        "#   plt.title(f'{title}')\n",
        "#   plt.grid(True)\n",
        "#   plt.show()\n",
        "#   return None"
      ],
      "metadata": {
        "id": "QmUg6FCEuxpr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inverse Function\n",
        "\n"
      ],
      "metadata": {
        "id": "duzr-Y-Mj5Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import statsmodels.api as sm\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# link_function = lambda x: 1 / (x + 10)\n",
        "\n",
        "# # Generate data with an inverse relationship\n",
        "# np.random.seed(0)\n",
        "# x = np.linspace(0, 100, 100)\n",
        "# y = link_function(x) + np.random.normal(0, 0.2, 100)  # Add some noise\n",
        "\n",
        "# # Create the design matrix\n",
        "# X = sm.add_constant(x)\n",
        "\n",
        "# # Fit the GLM with a Gaussian family and inverse_power link function\n",
        "# model = sm.GLM(y, X, family=sm.families.Gaussian(link=sm.families.links.inverse_power()))\n",
        "# results = model.fit()\n",
        "\n",
        "# # Print the model summary\n",
        "# print(results.summary())\n",
        "\n",
        "# # Make predictions\n",
        "# y_pred = results.predict(X)\n",
        "\n",
        "# # Plot the data and the fitted curve\n",
        "# plt.scatter(x, y, alpha=0.5)\n",
        "# plt.plot(x, y_pred, color='red')\n",
        "# plt.xlabel('x')\n",
        "# plt.ylabel('y')\n",
        "# plt.title('Inverse Relationship GLM')\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "f0B13EVaj-sg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# np.random.seed(0)\n",
        "# title = 'Inverse GLM'\n",
        "# family = sm.families.Gaussian(link=sm.families.links.InversePower())\n",
        "# link_function = lambda x: 1/(x+10)\n",
        "# x = np.linspace(1, 100, 100)\n",
        "# y = link_function(x) + np.random.normal(0, 0.2, 100)  # Add some noise\n",
        "\n",
        "# plot_GLM(x, y, predictions, title)\n",
        "# model, predictions = model_GLM(x, y, family)\n",
        "# print(model.summary())"
      ],
      "metadata": {
        "id": "D9wIElClu31q"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Function"
      ],
      "metadata": {
        "id": "2d-rIej9lnxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import statsmodels.api as sm\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# link_function = lambda x: 1 / (1 + np.exp(-x))\n",
        "\n",
        "# # Generate data with a logistic relationship\n",
        "# np.random.seed(0)\n",
        "# x = np.linspace(-10, 10, 100)\n",
        "# y = link_function(x) + np.random.normal(0, 0.05, 100)  # Add some noise\n",
        "# # Ensure y values are within (0, 1) for the logistic model\n",
        "# y = np.clip(y, 0.001, 0.999)\n",
        "\n",
        "# # Create the design matrix\n",
        "# X = sm.add_constant(x)\n",
        "\n",
        "# # Fit the GLM with a Binomial family and logit link function\n",
        "# model = sm.GLM(y, X, family=sm.families.Binomial())  # Logit link is default for Binomial\n",
        "# results = model.fit()\n",
        "\n",
        "# # Print the model summary\n",
        "# print(results.summary())\n",
        "\n",
        "# # Make predictions\n",
        "# y_pred = results.predict(X)\n",
        "\n",
        "# # Plot the data and the fitted curve\n",
        "# plt.scatter(x, y, alpha=0.5)\n",
        "# plt.plot(x, y_pred, color='red')\n",
        "# plt.xlabel('x')\n",
        "# plt.ylabel('y')\n",
        "# plt.title('Logistic GLM')\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "560d0n2plp8R"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# np.random.seed(0)\n",
        "# title = 'Logisitic GLM'\n",
        "# family = sm.families.Binomial()\n",
        "# link_function = lambda x: 1 / (1 + np.exp(-x))\n",
        "# x = np.linspace(-10, 10, 100)\n",
        "# y = link_function(x) + np.random.normal(0, 0.05, 100)  # Add some noise\n",
        "# y = np.clip(y, 0.001, 0.999)\n",
        "\n",
        "# plot_GLM(x, y, predictions, title)\n",
        "# model, predictions = model_GLM(x, y, family)\n",
        "# print(model.summary())"
      ],
      "metadata": {
        "id": "jjnzszC1x5ke"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log Function"
      ],
      "metadata": {
        "id": "NAziZJ6ol3l7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import statsmodels.api as sm\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Generate data with a logarithmic relationship\n",
        "# np.random.seed(0)\n",
        "# x = np.linspace(0.1, 10, 100)  # x values must be positive for log\n",
        "# y = np.log(x) + np.random.normal(0, 0.5, 100)  # Add some noise\n",
        "\n",
        "# # Create the design matrix\n",
        "# X = sm.add_constant(x)\n",
        "\n",
        "# # Fit the GLM with a Gaussian family and log link function\n",
        "# model = sm.GLM(y, X, family=sm.families.Gaussian(link=sm.families.links.log()))\n",
        "# results = model.fit()\n",
        "\n",
        "# # Print the model summary\n",
        "# print(results.summary())\n",
        "\n",
        "# # Make predictions\n",
        "# y_pred = results.predict(X)\n",
        "\n",
        "# # Plot the data and the fitted curve\n",
        "# plt.scatter(x, y, alpha=0.5)\n",
        "# plt.plot(x, y_pred, color='red')\n",
        "# plt.xlabel('x')\n",
        "# plt.ylabel('y')\n",
        "# plt.title('Logarithmic GLM')\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "thgdzT_LmAVx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# np.random.seed(0)\n",
        "# title = 'Log GLM'\n",
        "# family = sm.families.Gaussian(link=sm.families.links.Log())\n",
        "# link_function = lambda x: -np.log(x)\n",
        "# x = np.linspace(0.1, 10, 100)\n",
        "# y = link_function(x) + np.random.normal(0, 0.5, 100)  # Add some noise\n",
        "\n",
        "# y_scaled = (y - np.mean(y)) / np.std(y)\n",
        "# model, preds = model_GLM(x, y_scaled, family)\n",
        "# predictions = preds * np.std(y) + np.mean(y)\n",
        "# plot_GLM(x, y, predictions, title)\n",
        "# print(model.summary())"
      ],
      "metadata": {
        "id": "-3NqNozI9NHo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot the data and the fitted curve\n",
        "# def invert_plot_GLM(x, y, predictions, title):\n",
        "#   fig, ax = plt.subplots()\n",
        "#   ax.scatter(x, y)\n",
        "#   ax.plot(x, predictions, color='red')\n",
        "#   ax.invert_yaxis()\n",
        "#   ax.set_xlabel('x')\n",
        "#   ax.set_ylabel('y')\n",
        "#   ax.set_title(f'{title}')\n",
        "#   ax.grid(True)\n",
        "#   plt.show()\n",
        "#   return None\n",
        "\n",
        "# np.random.seed(0)\n",
        "# title = 'Log GLM'\n",
        "# family = sm.families.Gaussian(link=sm.families.links.Log())\n",
        "# link_function = lambda x: -np.log(x)\n",
        "# x = np.linspace(0.1, 10, 100)\n",
        "# y = link_function(x) + np.random.normal(0, 0.5, 100)  # Add some noise\n",
        "\n",
        "# y_scaled = (y - np.mean(y)) / np.std(y)\n",
        "# model, preds = model_GLM(x, y_scaled, family)\n",
        "# predictions = preds * np.std(y) + np.mean(y)\n",
        "# invert_plot_GLM(x, y, predictions, title)\n",
        "# print(model.summary())"
      ],
      "metadata": {
        "id": "6Fw7bQS5zhQU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import statsmodels.api as sm\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Generate data with a flipped logarithmic relationship\n",
        "# np.random.seed(0)\n",
        "# x = np.linspace(0.1, 10, 100)  # x values must be positive for log\n",
        "# y = np.log(x) + np.random.normal(0, 0.5, 100)  # Add noise (NO negation)\n",
        "\n",
        "# # Create the design matrix\n",
        "# X = sm.add_constant(x)\n",
        "\n",
        "# # Fit the GLM with a Gaussian family and log link function\n",
        "# # Note: We'll negate y to effectively use a \"negative log link\"\n",
        "# model = sm.GLM(y, X, family=sm.families.Gaussian(link=sm.families.links.Log()))\n",
        "# results = model.fit()\n",
        "\n",
        "# # Print the model summary\n",
        "# print(results.summary())\n",
        "\n",
        "# # Make predictions (on the transformed scale)\n",
        "# y_pred_transformed = results.predict(X)\n",
        "\n",
        "# # Transform predictions back to the original scale and flip\n",
        "# y_pred = y_pred_transformed\n",
        "\n",
        "# # Plot the data and the fitted curve\n",
        "# plt.scatter(x, y, alpha=0.5)\n",
        "# plt.plot(x, y_pred, color='red')\n",
        "# plt.xlabel('x')\n",
        "# plt.ylabel('y')\n",
        "# plt.title('Flipped Logarithmic GLM')\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "eu37x0re7Sa9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non Linear Models\n",
        "\n",
        "The truth is never linear!"
      ],
      "metadata": {
        "id": "suiIUngVBzNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Polynomial Regression\n"
      ],
      "metadata": {
        "id": "dbF7hlSbYJm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# X = 6 * np.random.rand(200, 1) - 3\n",
        "# noise = np.random.normal(0, 1, X.shape)\n",
        "# y = 0.8*X**2 + 0.9*X + 2 + noise\n",
        "\n",
        "# plt.scatter(X, y, color='blue')\n",
        "# plt.xlabel(\"X\")\n",
        "# plt.ylabel(\"y\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "YcPaEywXziib"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step Functions / Piecewise Linear Fit\n",
        "\n",
        "Cutting a variable into distinct regions\n",
        "\n",
        "Both **step functions** and **piecewise linear fits** are methods used in statistical modeling to account for non-linear relationships by breaking a predictor variable's range into segments, or \"pieces,\" and fitting a simpler function to each piece.\n",
        "\n",
        "***\n",
        "\n",
        "#### Step Functions (Piecewise Constant)\n",
        "\n",
        "A step function is a regression technique that models a relationship using a **piecewise constant** function. The fitted value remains constant over a specified interval of the predictor variable and then abruptly \"jumps\" to a new constant value when the predictor crosses a defined boundary. The graph looks like a series of steps on a staircase.\n",
        "\n",
        "#### How It Works\n",
        "1.  **Define Cut-Points:** A set of **cut-points** ($c_1, c_2, \\dots$) are chosen to divide the range of the predictor $X$ into non-overlapping bins or intervals.\n",
        "2.  **Use Indicator Variables:** For each bin, an **indicator variable** (a dummy variable) is created. This variable takes a value of **1** if the predictor $X$ falls within that bin, and **0** otherwise.\n",
        "3.  **Model Fitting:** A linear regression model is fit using these indicator variables as predictors. The coefficient for each indicator variable represents the constant predicted response within that segment, typically relative to the first segment.\n",
        "\n",
        "#### Application\n",
        "This method is appropriate when the effect of a predictor is believed to be **discrete and abrupt**. For example, modeling an outcome based on age groups (e.g., 0-18, 19-35, 36-55), where the response is assumed to be the same for everyone within a group but changes sharply when moving to the next group.\n",
        "\n",
        "***\n",
        "\n",
        "#### Piecewise Linear Fit (Segmented Regression)\n",
        "\n",
        "A piecewise linear fit, also known as **segmented regression**, models a relationship using multiple connected or unconnected linear segments. This technique allows the **slope** of the relationship to change at specific points, making it more flexible than a single straight line.\n",
        "\n",
        "#### How It Works\n",
        "1.  **Define Breakpoints (Knots):** Points where the slope is allowed to change are defined. These are called **breakpoints** or **knots** ($k_1, k_2, \\dots$).\n",
        "2.  **Model for Continuity:** To ensure the fitted function is **continuous** (meaning the segments meet without a gap), the model is typically constructed using the main linear term and a series of transformed terms based on the knots, often using the **truncated power basis**: $\\max(0, X - k_j)$.\n",
        "3.  **Slope Interpretation:** In a continuous piecewise linear model, the coefficients associated with the transformed terms ($\\max(0, X - k_j)$) represent the **change in slope** that occurs after the corresponding knot $k_j$.\n",
        "    * If no continuity is required, a separate linear regression can simply be run for the data points within each segment.\n",
        "\n",
        "#### Application\n",
        "Piecewise linear fits are used when the relationship between $X$ and $Y$ follows different linear trends over different ranges, but the transition between these trends is expected to be gradual or merely a change in rate. For example, a learning curve that increases slowly at first, then rapidly, with the change occurring at a particular milestone (the knot).\n",
        "\n",
        "***\n",
        "\n",
        "#### Core Distinction\n",
        "\n",
        "The primary difference lies in how they model the relationship change:\n",
        "\n",
        "* **Step Functions** model a change in the **level** of the response (a constant shift) at the boundary.\n",
        "* **Piecewise Linear Fits** model a change in the **slope** or **rate** of the response (a change in the line's angle) at the boundary.\n"
      ],
      "metadata": {
        "id": "O7qe_hcUYStB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # polynomial regression and step functions\n",
        "# # https://stackoverflow.com/questions/29382903/how-to-apply-piecewise-linear-fit-in-python\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from scipy import optimize\n",
        "\n",
        "# X = 6 * np.random.rand(200, 1) - 3\n",
        "# noise = np.random.normal(0, 1, X.shape)\n",
        "# y = 0.8*X**2 + 0.9*X + 2 + noise\n",
        "\n",
        "# plt.scatter(X, y, color='blue')\n",
        "\n",
        "\n",
        "# def piecewise_linear(x, x0, y0, k1, k2):\n",
        "#   return np.piecewise(x, [x < x0], [lambda x:k1*x + y0-k1*x0, lambda x:k2*x + y0-k2*x0])\n",
        "\n",
        "# sX = [-3, -2, -1, 0, 0, 1, 2, 3]\n",
        "# sy = [i**2 for i in sX]\n",
        "# plt.step(sX, sy, where='pre', color='red')\n",
        "\n",
        "# # Fit the piecewise linear function using shifted sX values\n",
        "# sX_shifted = np.array(sX) - 1\n",
        "# p, e = optimize.curve_fit(piecewise_linear, sX_shifted, sy)\n",
        "# plt.plot(sX_shifted, piecewise_linear(sX_shifted, *p), color='green')\n",
        "\n",
        "# # p , e = optimize.curve_fit(piecewise_linear, sX, sy)\n",
        "# # plt.plot(sX, piecewise_linear(sX, *p), color='green')\n",
        "\n",
        "# plt.xlabel(\"X\")\n",
        "# plt.xlim(-4, 4)\n",
        "# plt.ylabel(\"y\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "1z9g7mfNUCa0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # https://www.science.smith.edu/~jcrouser/SDS293/labs/lab12-py.html\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import matplotlib as mpl\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# from sklearn.preprocessing import PolynomialFeatures\n",
        "# import statsmodels.api as sm\n",
        "\n",
        "# X = 6 * np.random.rand(200, 1) - 3\n",
        "# noise = np.random.normal(0, 1, X.shape)\n",
        "# y = 0.8*X**2 + 0.9*X + 2 + noise\n",
        "\n",
        "# d = {'X': X.flatten(), 'y': y.flatten()}\n",
        "# df = pd.DataFrame(d)\n",
        "\n",
        "# X1 = PolynomialFeatures(1).fit_transform(df.X.values.reshape(-1,1))\n",
        "# X2 = PolynomialFeatures(2).fit_transform(df.X.values.reshape(-1,1))\n",
        "# X3 = PolynomialFeatures(3).fit_transform(df.X.values.reshape(-1,1))\n",
        "# X4 = PolynomialFeatures(4).fit_transform(df.X.values.reshape(-1,1))\n",
        "# X5 = PolynomialFeatures(5).fit_transform(df.X.values.reshape(-1,1))\n",
        "# X6 = PolynomialFeatures(6).fit_transform(df.X.values.reshape(-1,1))\n",
        "# X7 = PolynomialFeatures(7).fit_transform(df.X.values.reshape(-1,1))\n",
        "\n",
        "# fit1 = sm.GLS(y, X4).fit() # generalized least squares\n",
        "# print(fit1.summary())\n",
        "\n",
        "# for fitx in [X1, X2, X3, X4, X5, X6, X7]:\n",
        "#   model = sm.GLS(y, fitx).fit()\n",
        "#   print(model.summary().tables[0].data[1][3])\n",
        "#   print(model.summary().tables[0].data[5][3])\n",
        "#   print(model.summary().tables[0].data[6][3])\n",
        "#   print(model.summary().tables[0].data)\n"
      ],
      "metadata": {
        "id": "sMiCFzHLuAg-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create response matrix\n",
        "# import pandas as pd\n",
        "\n",
        "# resp_mx = (df.y > 3).map({False:0, True:1})\n",
        "\n",
        "# # GLM families comprise a link function as well as a mean-variance relationship\n",
        "# clf = sm.GLM(resp_mx, X4, family=sm.families.Binomial(sm.families.links.Logit()))\n",
        "# res = clf.fit()\n",
        "\n",
        "# # Generate a sequence of X values spanning the range\n",
        "# X_grid = np.arange(df.X.min(), df.X.max()).reshape(-1, 1)\n",
        "\n",
        "# # Generate test data\n",
        "# X_test = PolynomialFeatures(4).fit_transform(X_grid)\n",
        "\n",
        "# # Predict the value of the generated X\n",
        "# pred1 = fit1.predict(X_test)\n",
        "\n",
        "# # Create plots\n",
        "# plt.title('Degree-4 Polynomial')\n",
        "\n",
        "# # Scatter plot with polynomial regression line\n",
        "# plt.scatter(df.X, df.y, facecolor='None', edgecolor='b', alpha=0.3)\n",
        "# plt.plot(X_grid, pred1, color = 'g')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "5djaKfCNvmse"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generalized Least Squares (Not Ordinary)\n",
        "\n",
        "**Generalized Least Squares (GLS)** is a regression estimation method used when the assumptions about the errors (residuals) of the standard Ordinary Least Squares (OLS) model are violated, specifically when the errors are **heteroscedastic** (unequal variance) or **autocorrelated** (correlated with one another).\n",
        "\n",
        "***\n",
        "\n",
        "#### OLS Inefficiency and the Need for GLS\n",
        "\n",
        "In a standard linear regression, OLS assumes that the errors are independent and identically distributed (i.i.d.). This ensures that OLS produces the **Best Linear Unbiased Estimator (BLUE)**.\n",
        "\n",
        "When these assumptions are violated:\n",
        "\n",
        "* **Heteroscedasticity** means the variance of the errors is not constant across all observations.\n",
        "* **Autocorrelation** means the errors from one observation are correlated with the errors from other observations (common in time-series or panel data).\n",
        "\n",
        "In these cases, OLS estimates remain unbiased, but they become **inefficient** (they don't have the smallest possible variance), and their standard errors are incorrect, which invalidates statistical hypothesis testing. GLS corrects this by introducing a weighting mechanism.\n",
        "\n",
        "***\n",
        "\n",
        "#### How Generalized Least Squares Works\n",
        "\n",
        "The fundamental mechanism of GLS is to transform the original data so that the errors in the transformed model satisfy the OLS assumptions.\n",
        "\n",
        "1.  **Modeling the Error Structure:** GLS requires knowledge of the **variance-covariance matrix of the errors** ($\\mathbf{\\Omega}$). This matrix contains the variance of each error term (on the diagonal) and the correlation between all pairs of error terms (on the off-diagonals).\n",
        "2.  **Transformation:** GLS uses the inverse of the error covariance matrix, $\\mathbf{\\Omega}^{-1}$, as a weighting tool. This matrix is used to transform the observed data ($\\mathbf{Y}$) and the predictor matrix ($\\mathbf{X}$).\n",
        "3.  **Weighted Estimation:** Applying OLS to the transformed data yields the GLS estimator. This process effectively **down-weights** observations that have high variance (heteroscedasticity) or are strongly correlated with other errors (autocorrelation), leading to more precise and efficient coefficient estimates ($\\hat{\\mathbf{\\beta}}_{\\text{GLS}}$).\n",
        "\n",
        "The result is a set of coefficient estimates that is once again the Best Linear Unbiased Estimator (BLUE) for the corrected model.\n",
        "\n",
        "***\n",
        "\n",
        "#### Feasible Generalized Least Squares (FGLS)\n",
        "\n",
        "In practice, the true error covariance matrix ($\\mathbf{\\Omega}$) is almost never known. Therefore, the approach typically used is **Feasible Generalized Least Squares (FGLS)**, which involves an iterative process:\n",
        "\n",
        "1.  Run an initial OLS model to obtain the residuals.\n",
        "2.  Use these OLS residuals to estimate the covariance matrix, $\\hat{\\mathbf{\\Omega}}$, based on an assumed structure (e.g., a specific type of autocorrelation).\n",
        "3.  Use the estimated $\\hat{\\mathbf{\\Omega}}$ in the GLS formula to compute the final, efficient regression coefficients.\n",
        "\n",
        "#### Special Case: Weighted Least Squares (WLS)\n",
        "\n",
        "**Weighted Least Squares (WLS)** is a special case of GLS used specifically when only **heteroscedasticity** is the issue (no autocorrelation). In WLS, the covariance matrix $\\mathbf{\\Omega}$ is purely diagonal, and WLS simply weights each observation by the inverse of its individual error variance."
      ],
      "metadata": {
        "id": "Y8gEcs7gI-q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # generalized least squares\n",
        "# fit_1 = fit = sm.GLS(df.X, X1).fit()\n",
        "# fit_2 = fit = sm.GLS(df.X, X2).fit()\n",
        "# fit_3 = fit = sm.GLS(df.X, X3).fit()\n",
        "# fit_4 = fit = sm.GLS(df.X, X4).fit()\n",
        "# fit_5 = fit = sm.GLS(df.X, X5).fit()\n",
        "\n",
        "# print(sm.stats.anova_lm(fit_1, fit_2, fit_3, fit_4, fit_5, typ=1))"
      ],
      "metadata": {
        "id": "Lmg_rVP6z7p4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # step functions\n",
        "# df_cut, bins = pd.cut(df.X, 4, retbins = True, right = True)\n",
        "# df_cut.value_counts(sort = False)\n",
        "\n",
        "# df_steps = pd.concat([df.X, df_cut, df.y], keys = ['X','X_cuts','y'], axis = 1)\n",
        "\n",
        "# # Create dummy variables for the age groups\n",
        "# df_steps_dummies = pd.get_dummies(df_steps['X_cuts'])\n",
        "\n",
        "# # Statsmodels requires explicit adding of a constant (intercept)\n",
        "# df_steps_dummies = sm.add_constant(df_steps_dummies)\n",
        "\n",
        "# # Drop the (17.938, 33.5] category\n",
        "# df_steps_dummies = df_steps_dummies.drop(df_steps_dummies.columns[1], axis = 1)\n",
        "\n",
        "# df_steps_dummies.head(5)"
      ],
      "metadata": {
        "id": "NBkbs7pC0o0e"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit2 = sm.GLM(df_steps.y, df_steps_dummies).fit()\n",
        "# print(fit2.summary())"
      ],
      "metadata": {
        "id": "JKvMWbsQ2ns5"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Put the test data in the same bins as the training data.\n",
        "# bin_mapping = np.digitize(X_grid.ravel(), bins)\n",
        "\n",
        "# # Get dummies, drop first dummy category, add constant\n",
        "# X_test2 = sm.add_constant(pd.get_dummies(bin_mapping).drop(1, axis = 1))\n",
        "\n",
        "# # Predict the value of the generated ages using the linear model\n",
        "# predictions = fit2.predict(X_test2)\n",
        "\n",
        "# # # And the logistic model\n",
        "# # clf2 = sm.GLM(y, df_steps_dummies, family=sm.families.Binomial(sm.families.links.logit()))\n",
        "# # res2 = clf2.fit()\n",
        "# # pred3 = res2.predict(X_test2)\n",
        "\n",
        "# # Plot\n",
        "# plt.title('Piecewise Constant')\n",
        "\n",
        "# # Scatter plot with polynomial regression line\n",
        "# plt.scatter(df.X, df.y, facecolor = 'None', edgecolor = 'k', alpha = 0.3)\n",
        "# plt.plot(X_grid, predictions, c = 'b')\n",
        "\n",
        "# plt.xlabel('X')\n",
        "# plt.ylabel('y')\n",
        "# plt.ylim(ymin = 0)\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "9Y8_XLmY2r_D"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basis Functions\n",
        "\n",
        "* Family of functions or transformations that can be applied to X\n",
        "* Examples include polynomials, piece-wise constant functions, wavelets for Fourier series, and regression splines"
      ],
      "metadata": {
        "id": "RKMpjQogYXI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basis Functions**\n",
        "\n",
        "* **Purpose:** To model the relationship between the predictors (independent variables) and the response variable. They determine the *shape* of the relationship.\n",
        "* **How they work:**\n",
        "    * They are building blocks for creating flexible functions.\n",
        "    * They are combined linearly with coefficients to create complex curves.\n",
        "    * Examples: Polynomials, splines, radial basis functions.\n",
        "* **Where they operate:** They work on the *predictor side* of the equation, transforming the input variables before they are used to predict the response.\n",
        "\n",
        "**Link Functions**\n",
        "\n",
        "* **Purpose:** To connect the linear predictor (a combination of predictors and coefficients) to the expected value of the response variable. They determine how the *scale* of the response is related to the linear predictor.\n",
        "* **How they work:**\n",
        "    * They are typically non-linear functions.\n",
        "    * They map the linear predictor to a suitable range for the response variable (e.g., probabilities for logistic regression, positive values for Poisson regression).\n",
        "    * Examples: Logit, probit, log, inverse.\n",
        "* **Where they operate:** They work on the *response side* of the equation, transforming the output of the linear predictor to match the characteristics of the response variable.\n",
        "\n",
        "**Analogy**\n",
        "\n",
        "Imagine you're building a house:\n",
        "\n",
        "* **Basis functions** are like the different materials you use (wood, bricks, concrete). They determine the overall shape and structure of the house.\n",
        "* **Link functions** are like the electrical wiring that connects the power source to the different rooms and appliances. They ensure that the electricity is delivered in a way that's compatible with each device.\n",
        "\n",
        "**In the code `LogisticGAM(s(0) + s(1) + s(2))`**\n",
        "\n",
        "* `s(0)`, `s(1)`, `s(2)`: These are basis functions (splines) used to model the shape of the relationship between the first three features and the response.\n",
        "* `LogisticGAM`: This indicates that a logistic link function is used to connect the linear predictor (the sum of the spline terms) to the probability of the binary outcome.\n",
        "\n",
        "By combining basis functions and link functions, you can create flexible and powerful models that can capture complex relationships in your data and make accurate predictions.\n"
      ],
      "metadata": {
        "id": "P5CZpaTLz4v1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splines\n",
        "\n",
        "Splines create smooth curves out of irregular data points\n",
        "\n",
        "Source\n",
        "\n",
        "* https://towardsdatascience.com/data-science-deciphered-what-is-a-spline-18632bf96646"
      ],
      "metadata": {
        "id": "ut22YYXYYhQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Piecewise Polynomials, Knots, Splines, and Interpolation\n",
        "\n",
        "* An alternative to fit all data points with a single polynomial curve, is to fit segments to different parts of the data, with breakpoints (knots) at pre-determined places\n",
        "* Connect the knots with lines (polynomial lines if needed)\n",
        "* Smooth out the fitted curve\n",
        "* In essence, splines are piecewise polynomials, joined at points called knots\n",
        "\n",
        "Source\n",
        "\n",
        "* https://bookdown.org/tpinto_home/Beyond-Linearity/piecewise-regression-and-splines.html"
      ],
      "metadata": {
        "id": "2PZwPZbKYi7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # https://docs.scipy.org/doc/scipy/tutorial/interpolate/splines_and_polynomials.html\n",
        "# import matplotlib.pyplot as plt\n",
        "# from scipy.interpolate import PchipInterpolator\n",
        "# from scipy.special import ellipk\n",
        "\n",
        "# m = np.linspace(0, 0.9, 11)\n",
        "# x = np.linspace(0, np.pi/2, 70)\n",
        "# y = 1 / np.sqrt(1 - m[:, None]*np.sin(x)**2)\n",
        "\n",
        "# spl = PchipInterpolator(x, y, axis=1)  # the default is axis=0\n",
        "\n",
        "# plt.plot(m, spl.integrate(0, np.pi/2), '--')\n",
        "# plt.plot(m, ellipk(m), 'o')\n",
        "# plt.legend(['`ellipk`', 'integrated piecewise polynomial'])\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "e13ON9o7eYGt"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from scipy.interpolate import PchipInterpolator\n",
        "\n",
        "# m = np.linspace(-3, 3, 200)\n",
        "# x = 6 * np.random.rand(200, 1) - 3\n",
        "# noise = np.random.normal(0, 1, x.shape)\n",
        "# x = np.sort(x.flatten())\n",
        "# y = [0.8*i**2 + 0.9*i + 2 + noise for i in x]\n",
        "\n",
        "# X1 = 6 * np.random.rand(200, 1) - 3\n",
        "# y1 = 0.8*X1**2 + 0.9*X1 + 2 + noise\n",
        "\n",
        "# spl = PchipInterpolator(x, y, axis=1)  # the default is axis=0\n",
        "\n",
        "# plt.scatter(X1, y1, facecolor = 'None', edgecolor = 'k', alpha = 0.3)\n",
        "# plt.plot(m, spl.integrate(0, np.pi/3))\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "KZVq5PV7Ak-W"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpolation is a method of estimating unknown data points in a known range to make the curve smoother. Univariate interpolation is a type of curve fitting that seeks the curve that best fits a set of two-dimensional data points. Since the data points are sampled from a single variable function, it is called univariate interpolation.\n",
        "\n",
        "SciPy API provides several functions to implement the interpolation method for a given data. In this tutorial, you'll learn how to apply interpolation for a given data by using interp1d, CubicSpline, PchipInterpolator, and Akima1DInterplator methods in Python. The tutorial covers;\n",
        "\n",
        "* Preparing test data\n",
        "* interp1d  method\n",
        "* CubicSpline method\n",
        "* PchipInterpolator  method\n",
        "* Akima1DInterpolator method\n",
        "* Source code listing\n",
        "\n",
        "Source\n",
        "\n",
        "* https://www.datatechnotes.com/2022/12/univariate-interpolation-examples-in.html"
      ],
      "metadata": {
        "id": "jVZxawOZP8SN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Smoothing Splines\n",
        "\n",
        "* For the interpolation problem, the task is to construct a curve which passes through a given set of data points. This may be not appropriate if the data is noisy: we then want to construct a smooth curve, g(x), which approximates the input data without passing through each point exactly. To this end, scipy.interpolate allows constructing smoothing splines, based on the Fortran library FITPACK by P. Dierckx.\n",
        "* splrep: Spline interpolation requires two essential steps: (1) a spline representation of the curve is computed, and (2) the spline is evaluated at the desired points.\n",
        "* BSpline: A basis spline is a nonlinear function constructed of flexible bands that pass through control points to create a smooth curve.\n",
        "\n",
        "Sources\n",
        "\n",
        "* https://docs.scipy.org/doc/scipy/tutorial/interpolate/smoothing_splines.html\n",
        "* https://apmonitor.com/wiki/index.php/Main/ObjectBspline"
      ],
      "metadata": {
        "id": "F3qU6ymNYdra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # https://docs.scipy.org/doc/scipy/tutorial/interpolate/smoothing_splines.html\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from scipy.interpolate import splrep, BSpline\n",
        "\n",
        "# x = np.arange(0, 2*np.pi+np.pi/4, 2*np.pi/16)\n",
        "# rng = np.random.default_rng()\n",
        "# y =  np.sin(x) + 0.4*rng.standard_normal(size=len(x))"
      ],
      "metadata": {
        "id": "cXrMxoqvetNO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tck = splrep(x, y, s=0) # s is used to specify the amount of smoothing to perform; 0 is no smoothing\n",
        "# tck_s = splrep(x, y, s=len(x))\n",
        "\n",
        "# xnew = np.arange(0, 9/4, 1/50) * np.pi\n",
        "# plt.plot(x, y, 'o', label = 'x, y plot', color='r') # 1\n",
        "# plt.plot(xnew, np.sin(xnew), '-.', label='sin(x)', color='g') # 2\n",
        "# # plt.plot(xnew, BSpline(*tck)(xnew), '-', label='s=0', color='b') # 3\n",
        "# # plt.plot(xnew, BSpline(*tck_s)(xnew), '-', label=f's={len(x)}', color='k') # 4\n",
        "\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "bTDARIuOH1Ki"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(x, y, 'o', label = 'x, y plot', color='r') # 1\n",
        "# plt.plot(xnew, np.sin(xnew), '-.', label='sin(x)', color='g') # 2\n",
        "# plt.plot(xnew, BSpline(*tck)(xnew), '-', label='s=0', color='b') # 3\n",
        "# # plt.plot(xnew, BSpline(*tck_s)(xnew), '-', label=f's={len(x)}', color='k') # 4\n",
        "\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "Vjk68blLI0iR"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(x, y, 'o', label = 'x, y plot', color='r') # 1\n",
        "# plt.plot(xnew, np.sin(xnew), '-.', label='sin(x)', color='g') # 2\n",
        "# plt.plot(xnew, BSpline(*tck)(xnew), '-', label='s=0', color='b') # 3\n",
        "# plt.plot(xnew, BSpline(*tck_s)(xnew), '-', label=f's={len(x)}', color='k') # 4\n",
        "\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "D3TcpcOkI8Vv"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Review\n",
        "\n",
        "This series of plots feature two main components:\n",
        "\n",
        "1.  **The Data ($\\mathbf{x, y}$ plot):** The **red dots** represent a set of observed data points. These points show an underlying pattern but also contain a noticeable amount of **noise** or **random error**. For instance, while most points hover around the green curve, some points (like the one near $x=5$ at $y \\approx -1.3$) deviate significantly.\n",
        "2.  **The True Function ($\\mathbf{sin(x)}$):** The **green dashed-dot line** represents the assumed or true underlying function generating the data, which is $\\sin(x)$. This curve is perfectly smooth and captures the **systematic structure** in the data.\n",
        "\n",
        "***\n",
        "\n",
        "### Relevance to Smoothing Splines\n",
        "\n",
        "Our plots demonstrate the environment where a smoothing spline is used: fitting a smooth curve to noisy data.\n",
        "\n",
        "### The Problem\n",
        "The goal in the scenario is to estimate the true, smooth function (the green curve) given only the noisy data points (the red dots). If you tried to fit a simple, low-degree polynomial, it would miss the non-linear, cyclical pattern. If you tried to connect all the dots, the resulting curve would be erratic and bumpy, which is known as **overfitting** or a **low-bias, high-variance** fit.\n",
        "\n",
        "### The Smoothing Spline Solution\n",
        "A **smoothing spline** is a non-parametric method specifically designed to solve this trade-off between **fit** and **smoothness**.\n",
        "\n",
        "A smoothing spline finds a function, $\\hat{f}(x)$, that minimizes a penalized sum of squares:\n",
        "\n",
        "$$\\sum_{i=1}^{n} (y_i - \\hat{f}(x_i))^2 + \\lambda \\int (\\hat{f}''(x))^2 dx$$\n",
        "\n",
        "* The **first term** ($\\sum$) measures the **lack of fit** or fidelity to the data (i.e., how close the red dots are to the fitted curve). Minimizing this term alone leads to a curve that perfectly connects all the dots (like interpolation).\n",
        "* The **second term** ($\\lambda \\int$) is the **penalty** on roughness, measured by the integrated squared second derivative of the function ($\\hat{f}''(x)$). This forces the fitted curve to be smooth.\n",
        "* **$\\lambda$ (Lambda)** is the **smoothing parameter**.\n",
        "    * If $\\lambda \\rightarrow 0$, the penalty is ignored, and the spline becomes an interpolating curve (perfect fit, rough).\n",
        "    * If $\\lambda \\rightarrow \\infty$, the penalty dominates, forcing the curve to be a straight line (extremely smooth, poor fit).\n",
        "\n",
        "In this context, a well-chosen $\\lambda$ would generate a fitted curve that closely tracks the smooth green $\\sin(x)$ curve without being overly pulled toward the individual noisy red dots. The plot perfectly illustrates the need for a technique like a smoothing spline to balance the signal ($\\sin(x)$) and the noise (the vertical scatter of the red dots).\n",
        "\n",
        "#### 1. The True Function and Data Noise\n",
        "\n",
        "* **Red Dots ($\\mathbf{x, y}$ plot):** The noisy observed data points.\n",
        "* **Green Line ($\\mathbf{\\sin(x)}$):** The true, smooth underlying relationship we are trying to estimate.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Low-Smoothness Fit ($\\mathbf{s=0}$) - Overfitting\n",
        "\n",
        "The **blue solid line** represents a spline fit where the **smoothing parameter** ($s$, equivalent to $1/\\lambda$) is set to a very low value, in this case, $s=0$.\n",
        "\n",
        "* **Fit/Bias:** This curve achieves an almost **perfect fit** to every individual red data point, minimizing the penalty for roughness. This means it has **low bias** relative to the *observed data*.\n",
        "* **Smoothness/Variance:** Because it follows the noise in the data so closely, the curve is **very rough** (wiggly). This model is prone to **overfitting** and has **high variance**, meaning it would perform poorly on new data not included in the original set. It does a poor job of recovering the true underlying $\\sin(x)$ curve.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. Optimal Smoothness Fit ($\\mathbf{s=18}$) - Balanced\n",
        "\n",
        "The **black solid line** represents a spline fit with an **intermediate smoothing parameter**, $s=18$.\n",
        "\n",
        "* **Fit/Bias:** This curve is close to the data points, but not perfectly through them. It accepts a small amount of **bias** (distance from the red dots).\n",
        "* **Smoothness/Variance:** By accepting this bias, the curve is significantly **smoother** than the blue line. It does a much better job of capturing the general trend and shape of the green $\\sin(x)$ curve, effectively **filtering out the random noise**. This fit achieves a better balance in the **bias-variance trade-off**, resulting in a more robust model for prediction.\n",
        "\n",
        "The last plot beautifully demonstrates how a smoothing spline works by using the smoothing parameter $s$ (or $\\lambda$) to control the flexibility of the fit, allowing us to find a balance between following the noisy data and producing a smooth, generalizable curve."
      ],
      "metadata": {
        "id": "-ijxN4GJMDPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cubic Spline\n",
        "\n",
        "* Cubic spline interpolation is a way of finding a curve that connects data points with a degree of three or less\n",
        "\n",
        "Source\n",
        "\n",
        "* https://www.geeksforgeeks.org/cubic-spline-interpolation/"
      ],
      "metadata": {
        "id": "VqEpvfR0gKcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # https://docs.scipy.org/doc/scipy/tutorial/interpolate/smoothing_splines.html\n",
        "# import matplotlib.pyplot as plt\n",
        "# from scipy import interpolate\n",
        "\n",
        "# x = np.arange(0, 2*np.pi+np.pi/4, 2*np.pi/8)\n",
        "# y = np.sin(x)\n",
        "# tck = interpolate.splrep(x, y, s=0)\n",
        "# xnew = np.arange(0, 2*np.pi, np.pi/50)\n",
        "# ynew = interpolate.splev(xnew, tck, der=0)\n",
        "\n",
        "# plt.figure()\n",
        "# plt.plot(x, y, 'x', xnew, ynew, xnew, np.sin(xnew), x, y)\n",
        "# plt.legend(['Linear', 'Cubic Spline', 'True'])\n",
        "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
        "# plt.title('Cubic-spline interpolation')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "knFGlkJCgOP1"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Regression\n",
        "\n",
        "* Local regression is a diferent approach for ftting fexible non-linear func- local regression tions, which involves computing the ft at a target point x0 using only the nearby training observations\n",
        "* Check out Python LOWESS (Locally Weighted Scatterplot Smoothing)\n",
        "* https://www.google.com/imgres?imgurl=https%3A%2F%2Fmiro.medium.com%2Fv2%2Fresize%3Afit%3A928%2F1*H3QS05Q1GJtY-tiBL00iug.png&tbnid=z0DKwyhWlDC45M&vet=12ahUKEwiOna6PwJSCAxUH3MkDHWOIA8oQMygAegQIARBW..i&imgrefurl=https%3A%2F%2Ftowardsdatascience.com%2Flocally-weighted-linear-regression-in-python-3d324108efbf&docid=nFD3cy7xVIYQVM&w=928&h=704&q=local%20regression%20python%20example&ved=2ahUKEwiOna6PwJSCAxUH3MkDHWOIA8oQMygAegQIARBW"
      ],
      "metadata": {
        "id": "Bw5DSNjZYeYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# X = 6 * np.random.rand(200) - 3\n",
        "# noise = np.random.normal(0, 1, X.shape)\n",
        "# y = 0.8*X**2 + 0.9*X + 2 + noise\n",
        "\n",
        "# plt.scatter(X, y, color='blue', alpha=0.3);"
      ],
      "metadata": {
        "id": "Z-wHlthcyw0s"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # https://james-brennan.github.io/posts/lowess_conf/\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "# import scipy.stats\n",
        "\n",
        "# x = 6 * np.random.rand(200) - 3\n",
        "# noise = np.random.normal(0, 1, x.shape)\n",
        "# y = 0.8*x**2 + 0.9*x + 2 + noise\n",
        "# y_vector = np.hstack(y)\n",
        "\n",
        "# plt.figure(figsize=(10,6))\n",
        "# plt.scatter(x, y, edgecolors='black', color=None, alpha=0.1);\n",
        "\n",
        "# order = np.argsort(x)\n",
        "# b = 10\n",
        "# colors = ['red', 'green', 'blue', 'purple']\n",
        "# for i, k in enumerate([40, 50, 60, 70]):\n",
        "#   xx = x[order][k-b:k+b]\n",
        "#   yy = y_vector[order][k-b:k+b]\n",
        "#   plt.scatter(xx, yy, marker='o', color=colors[i], alpha=0.2)\n",
        "#   res = scipy.stats.linregress(xx, yy)\n",
        "#   plt.plot(xx, res.intercept + res.slope *xx, marker='_', color=colors[i])"
      ],
      "metadata": {
        "id": "rvy_7F-vlSZZ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOESS\n",
        "\n",
        "LOESS (locally estimated scatterplot smoothing), sometimes called LOWESS (locally weighted scatterplot smoothing), is a method used to create a smooth curve that fits a set of data points without assuming a specific functional form like a line or a parabola. It's particularly helpful when you have data with a complex relationship that doesn't follow a simple pattern.\n",
        "\n",
        "Here's a breakdown of how LOESS works:\n",
        "\n",
        "**1. Local Regression:**\n",
        "\n",
        "- Imagine you have a set of data points scattered on a plot. LOESS works by fitting a separate regression model to localized subsets of the data.\n",
        "- It's like sliding a small window across your data points. For each window, LOESS fits a simple model (usually a low-degree polynomial like a line or a quadratic curve) to the points within that window.\n",
        "\n",
        "**2. Weighted Regression:**\n",
        "\n",
        "- Not all points within the window are treated equally. LOESS assigns weights to the points based on their distance from the center of the window.\n",
        "- Points closer to the center have higher weights, meaning they have more influence on the local regression. Points farther away have lower weights and less influence.\n",
        "- This weighting ensures that the local regression is most influenced by the points closest to the point being estimated.\n",
        "\n",
        "**3. Combining Local Models:**\n",
        "\n",
        "- After fitting local regressions to each window, LOESS combines the results to create a smooth curve that passes through the entire dataset.\n",
        "- The final curve is a weighted average of the predictions from all the local models.\n",
        "\n",
        "**Key Features and Benefits of LOESS:**\n",
        "\n",
        "* **Non-parametric:** LOESS doesn't assume any specific functional form for the relationship between the variables. It's flexible and can capture a wide range of patterns.\n",
        "* **Robust to outliers:** The weighting scheme reduces the influence of outliers, making LOESS more robust compared to methods like ordinary least squares regression.\n",
        "* **Control over smoothness:** You can control how smooth the curve is by adjusting the size of the window (bandwidth) and the degree of the polynomial used in the local regressions.\n",
        "\n",
        "**When is LOESS Useful?**\n",
        "\n",
        "* **Complex relationships:** When the relationship between variables is non-linear and difficult to model with a simple function.\n",
        "* **Noisy data:** When your data has a lot of noise or variability, LOESS can help smooth out the noise and reveal the underlying trend.\n",
        "* **Data exploration:** LOESS is a great tool for exploring data and visualizing patterns without making strong assumptions about the data.\n",
        "\n",
        "**In simpler terms:**\n",
        "\n",
        "Imagine you're trying to draw a smooth line through a set of scattered points. LOESS is like using a small, flexible ruler that you move along the points, fitting it to small sections of the data at a time. The final curve is a combination of all these local \"fits,\" creating a smooth and flexible representation of the overall trend.\n"
      ],
      "metadata": {
        "id": "hkHnQI9WYs73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from statsmodels.nonparametric.smoothers_lowess import lowess\n",
        "\n",
        "# # Generate data\n",
        "# np.random.seed(123)\n",
        "# x = np.arange(0, 10, 0.1)\n",
        "# y = np.sin(x) + np.random.normal(0, 0.5, len(x))\n",
        "\n",
        "# # Apply LOESS smoothing\n",
        "# smoothed = lowess(y, x, frac=0.2)  # frac controls the smoothness\n",
        "\n",
        "# # Plot the results\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.scatter(x, y, label='Data', s=10)\n",
        "# plt.plot(smoothed[:, 0], smoothed[:, 1], label='LOESS', color='red', linewidth=2)\n",
        "# plt.xlabel('x')\n",
        "# plt.ylabel('y')\n",
        "# plt.title('LOESS Smoothing')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "AVtSza2w-QrJ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's a great question! LOESS and KDE are both smoothing techniques, but they approach the problem from different perspectives and have distinct characteristics. Here's how they relate and differ:\n",
        "\n",
        "**Similarities:**\n",
        "\n",
        "* **Non-parametric:** Both LOESS and KDE are non-parametric methods, meaning they don't assume a specific functional form (like a linear or polynomial equation) for the relationship between variables. They let the data \"speak for itself\" to determine the shape of the smooth curve.\n",
        "* **Local smoothing:** Both methods focus on local neighborhoods of data points to create the smooth curve. LOESS fits local regressions, while KDE uses kernels to estimate local density.\n",
        "* **Smoothing parameter:** Both have a parameter that controls the degree of smoothing. In LOESS, it's the bandwidth or window size, while in KDE, it's the bandwidth of the kernel.\n",
        "\n",
        "**Differences:**\n",
        "\n",
        "* **Purpose:**\n",
        "    * **LOESS:** Primarily used for smoothing and visualizing the relationship between two variables. It focuses on estimating the conditional mean of the response variable (y) given the predictor variable (x).\n",
        "    * **KDE:**  Primarily used for estimating the probability density function of a single variable. It focuses on visualizing the distribution of the data and identifying areas of high and low density.\n",
        "\n",
        "* **Method:**\n",
        "    * **LOESS:**  Uses locally weighted polynomial regression. It fits a low-degree polynomial to a subset of data points within a local window, with weights decreasing as you move away from the center of the window.\n",
        "    * **KDE:**  Uses kernel smoothing. It places a kernel function (a small, smooth function like a Gaussian) at each data point and sums them up to create a smooth density estimate.\n",
        "\n",
        "* **Output:**\n",
        "    * **LOESS:** Produces a smooth curve that represents the trend in the data.\n",
        "    * **KDE:** Produces a density estimate that shows the probability of observing a value within a given range.\n",
        "\n",
        "\n",
        "**Analogy:**\n",
        "\n",
        "Imagine you're trying to map a mountain range:\n",
        "\n",
        "* **LOESS:** Like drawing a smooth road that follows the general terrain of the mountains, focusing on the elevation (y) at different locations (x).\n",
        "* **KDE:** Like creating a topographic map that shows the density of elevation changes, highlighting peaks and valleys.\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "LOESS and KDE are both valuable tools for smoothing data, but they serve different purposes. LOESS is more focused on capturing the relationship between variables, while KDE is more focused on visualizing the distribution of a single variable. The choice of which method to use depends on the specific goals of your analysis.\n"
      ],
      "metadata": {
        "id": "vm-nQjti_YjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from scipy.stats import gaussian_kde\n",
        "\n",
        "# # Generate data\n",
        "# np.random.seed(123)\n",
        "# x = np.arange(0, 10, 0.1)\n",
        "# y = np.sin(x) + np.random.normal(0, 0.5, len(x))\n",
        "\n",
        "# # Sort the data by x-values\n",
        "# sorted_indices = np.argsort(x)\n",
        "# x_sorted = x[sorted_indices]\n",
        "# y_sorted = y[sorted_indices]\n",
        "\n",
        "# # Create the KDE object for the joint distribution of (x, y)\n",
        "# data = np.vstack([x_sorted, y_sorted])\n",
        "# kde = gaussian_kde(data)\n",
        "\n",
        "# # Function to estimate conditional mean E(y|x) using KDE\n",
        "# def conditional_mean(x_val):\n",
        "#     # Generate a grid of y values around the given x_val\n",
        "#     y_grid = np.linspace(min(y_sorted), max(y_sorted), 100)\n",
        "#     # Evaluate the joint density on the grid\n",
        "#     positions = np.vstack([np.full_like(y_grid, x_val), y_grid])\n",
        "#     density = kde(positions)\n",
        "#     # Calculate the weighted average of y values\n",
        "#     return np.sum(y_grid * density) / np.sum(density)\n",
        "\n",
        "# # Calculate conditional mean for each x value\n",
        "# y_smooth = np.array([conditional_mean(x_val) for x_val in x_sorted])\n",
        "\n",
        "# # Plot the results\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.scatter(x, y, label='Data', s=10)\n",
        "# plt.plot(x_sorted, y_smooth, label='KDE (Conditional Mean)', color='red', linewidth=2)\n",
        "# plt.xlabel('x')\n",
        "# plt.ylabel('y')\n",
        "# plt.title('KDE-based Smoothing (Conditional Mean)')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "3uyQLqC1_vK-"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KDE?\n",
        "\n",
        "**Kernel Density Estimation (KDE)** is a non-parametric method used to estimate the **probability density function (PDF)** of a random variable, essentially creating a smooth curve that represents the distribution of a set of data points.\n",
        "\n",
        "It is often used as a smoother alternative to a histogram.\n",
        "\n",
        "***\n",
        "\n",
        "## How KDE Works\n",
        "\n",
        "For every single data point observed, KDE places a smooth function, called the **kernel**, over that point. These individual kernel functions are then summed up and normalized to create the final estimated density curve.\n",
        "\n",
        "### 1. The Kernel\n",
        "The **kernel** function determines the shape of the bumps added at each data point. The most common choice is the **Gaussian (Normal) kernel** (a bell curve), but others like uniform or triweight kernels can also be used.\n",
        "\n",
        "### 2. The Bandwidth\n",
        "The **bandwidth ($h$)** is the most crucial parameter in KDE. It controls the width of the kernel bumps and thus dictates the degree of smoothness in the final density estimate:\n",
        "\n",
        "* **Small Bandwidth:** Creates narrow bumps, resulting in a density estimate that is very wiggly and may show spurious structure (undersmoothing, high variance).\n",
        "* **Large Bandwidth:** Creates wide, overlapping bumps, resulting in an overly smooth estimate that might mask true features of the underlying distribution (oversmoothing, high bias).\n",
        "\n",
        "### 3. The Formula\n",
        "The general formula for the KDE estimate $\\hat{f}(x)$ at any point $x$ is:\n",
        "$$\\hat{f}(x) = \\frac{1}{n \\cdot h} \\sum_{i=1}^{n} K\\left(\\frac{x - x_i}{h}\\right)$$\n",
        "where $n$ is the number of data points, $h$ is the bandwidth, $x_i$ is an individual data point, and $K$ is the kernel function.\n",
        "\n",
        "In summary, KDE works by blurring the discrete data points according to the bandwidth and kernel shape to reveal a continuous density shape."
      ],
      "metadata": {
        "id": "lgNr9ujONyrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While KDE fundamentally estimates density, we can adapt it to mimic the behavior of LOESS by focusing on how the *conditional mean* of `y` changes along the `x-axis`.\n",
        "\n",
        "Here's an approach that uses KDE to estimate the conditional mean, producing a smoother line that better captures the trend in your data:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "# Generate data\n",
        "np.random.seed(123)\n",
        "x = np.arange(0, 10, 0.1)\n",
        "y = np.sin(x) + np.random.normal(0, 0.5, len(x))\n",
        "\n",
        "# Sort the data by x-values\n",
        "sorted_indices = np.argsort(x)\n",
        "x_sorted = x[sorted_indices]\n",
        "y_sorted = y[sorted_indices]\n",
        "\n",
        "# Create the KDE object for the joint distribution of (x, y)\n",
        "data = np.vstack([x_sorted, y_sorted])\n",
        "kde = gaussian_kde(data)\n",
        "\n",
        "# Function to estimate conditional mean E(y|x) using KDE\n",
        "def conditional_mean(x_val):\n",
        "    # Generate a grid of y values around the given x_val\n",
        "    y_grid = np.linspace(min(y_sorted), max(y_sorted), 100)\n",
        "    # Evaluate the joint density on the grid\n",
        "    positions = np.vstack([np.full_like(y_grid, x_val), y_grid])\n",
        "    density = kde(positions)\n",
        "    # Calculate the weighted average of y values\n",
        "    return np.sum(y_grid * density) / np.sum(density)\n",
        "\n",
        "# Calculate conditional mean for each x value\n",
        "y_smooth = np.array([conditional_mean(x_val) for x_val in x_sorted])\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(x, y, label='Data', s=10)\n",
        "plt.plot(x_sorted, y_smooth, label='KDE (Conditional Mean)', color='red', linewidth=2)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('KDE-based Smoothing (Conditional Mean)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "1. **Joint KDE:** We create a 2D KDE object for the joint distribution of `(x, y)`.\n",
        "\n",
        "2. **`conditional_mean(x_val)` function:**\n",
        "   - For a given `x_val`, it generates a grid of `y` values.\n",
        "   - It evaluates the joint density on the grid (`x_val`, `y_grid`).\n",
        "   - It calculates the weighted average of `y` values using the density as weights. This effectively estimates the conditional mean E(y|x).\n",
        "\n",
        "3. **Calculate smoothed y:**  We apply `conditional_mean` to each `x` value to get the smoothed `y_smooth`.\n",
        "\n",
        "4. **Plot:** The original data and the smoothed KDE line are plotted.\n",
        "\n",
        "This approach leverages KDE to estimate the conditional mean of `y` given `x`, providing a smoother curve that better captures the trend in the data, similar to LOESS. The resulting plot should resemble the LOESS output more closely than the previous 1D KDE.\n"
      ],
      "metadata": {
        "id": "ysqcn5o1_3k8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalized Additive Models\n",
        "\n",
        "* Allows for flexible nonlinearities in several variables, but retains the additive structure of linear models\n",
        "* Can fit a GAM using splines, smoothing splines, local regression\n",
        "* Coefficients not interesting; fitted functions are\n",
        "* Can mix terms, linear, nonlinear\n",
        "* $g(E(y)) = \\beta_0 + f_1(X_1) + f_2(x_2) + ... + f_m(x_m)$"
      ],
      "metadata": {
        "id": "BC-JxoddYs9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pygam"
      ],
      "metadata": {
        "id": "JXm190O_S1DL"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please run the following simple Python demonstration of a **Generalized Additive Model (GAM)**. This example uses the `pygam` library to model a non-linear relationship.\n",
        "\n",
        "-----\n",
        "\n",
        "### Generalized Additive Model (GAM) Demonstration (Python)\n",
        "\n",
        "This script will generate synthetic data with a non-linear component, fit a GAM to it using a spline smoother, and then plot the fitted smooth function.\n",
        "\n",
        "### Colab Setup\n",
        "\n",
        "Run the following cell first to install the necessary library:\n",
        "\n",
        "```python\n",
        "!pip install pygam\n",
        "```\n",
        "\n",
        "### Python Code for GAM\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pygam import LinearGAM, s as spline\n",
        "\n",
        "# --- 1. Generate Synthetic Data ---\n",
        "\n",
        "# Create a predictor variable (x)\n",
        "np.random.seed(42)\n",
        "N = 100\n",
        "X = np.linspace(0, 10, N).reshape(-1, 1) # Reshape for pygam\n",
        "\n",
        "# Define the true, non-linear function: f(x) = 2 * sin(X) + 0.5 * X\n",
        "true_function = lambda x: 2 * np.sin(x) + 0.5 * x\n",
        "Y_true = true_function(X)\n",
        "\n",
        "# Add random noise to simulate real-world data\n",
        "Y_noise = Y_true + np.random.normal(0, 1.5, N).reshape(-1, 1)\n",
        "Y = Y_noise.flatten() # Flatten Y for pygam input\n",
        "\n",
        "# --- 2. Fit the Generalized Additive Model (GAM) ---\n",
        "\n",
        "# We use LinearGAM because the response variable Y is continuous (Gaussian-like).\n",
        "# The 's' function (aliased as 'spline') indicates a smooth term.\n",
        "# The `n_splines` argument controls the flexibility of the smooth term.\n",
        "gam = LinearGAM(spline(0, n_splines=20)).fit(X, Y)\n",
        "\n",
        "# --- 3. Evaluate and Plot Results ---\n",
        "\n",
        "# Create a grid for prediction to get a smooth line\n",
        "XX = gam.generate_X_grid(term=0, n=500)\n",
        "\n",
        "# Get confidence intervals for the prediction\n",
        "predictions = gam.predict(XX)\n",
        "confidence_intervals = gam.confidence_intervals(XX)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot the noisy data points\n",
        "plt.scatter(X, Y, color='red', alpha=0.5, label='Noisy Data Points')\n",
        "\n",
        "# Plot the true underlying function (for comparison)\n",
        "plt.plot(X, Y_true, 'g--', label=r'True Function $2\\sin(x) + 0.5x$')\n",
        "\n",
        "# Plot the GAM fit (the smooth line)\n",
        "plt.plot(XX, predictions, color='black', linewidth=3, label='GAM Fit')\n",
        "\n",
        "# Plot the 95% confidence intervals\n",
        "plt.plot(XX, confidence_intervals[:, 0], color='blue', linestyle='--', alpha=0.6, label='95% C.I.')\n",
        "plt.plot(XX, confidence_intervals[:, 1], color='blue', linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.title('Generalized Additive Model (GAM) for Non-linear Smoothing')\n",
        "plt.xlabel('Predictor X')\n",
        "plt.ylabel('Response Y')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=':', alpha=0.6)\n",
        "plt.show()\n",
        "\n",
        "# --- 4. Interpretation (Printed Output) ---\n",
        "\n",
        "print(\"\\n--- GAM Summary ---\")\n",
        "# The summary shows the smoothness penalty (Lambda) and deviance explained\n",
        "print(gam.summary())\n",
        "```\n",
        "\n",
        "-----\n",
        "\n",
        "#### Brief Interpretation\n",
        "\n",
        "The output plot will show the following:\n",
        "\n",
        "  * **Red Dots:** Your noisy data.\n",
        "  * **Green Dashed Line:** The true, underlying non-linear relationship.\n",
        "  * **Black Solid Line:** The fitted GAM curve. You'll see this curve closely tracks the green line, demonstrating the GAM's ability to model the **non-linear structure** using a smooth function (the spline) while ignoring much of the random noise.\n",
        "  * **Blue Dashed Lines:** The 95% confidence intervals around the GAM fit, indicating the uncertainty in the model's prediction.\n",
        "\n",
        "A GAM is successful here because it replaces the simple linear term ($\\beta X$) with a smooth, additive function ($s(X)$) which can bend and adapt to the underlying non-linear pattern."
      ],
      "metadata": {
        "id": "DrNTzOhNPyYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain the output from the Python Code for GAM you just ran.\n",
        "\n",
        "ANSWER HERE:"
      ],
      "metadata": {
        "id": "X5yPhPc0RA1L"
      }
    }
  ]
}